# -*- coding: utf-8 -*-
import re
import os
from datetime import datetime
from abc import ABC, abstractmethod
from types import TracebackType
from typing import Dict, Generator, List, Optional, Tuple, Type

from typing_extensions import Self

from rich import box  # type: ignore
from rich.live import Live  # type: ignore
from rich.panel import Panel  # type: ignore
from rich.status import Status  # type: ignore
from rich.text import Text  # type: ignore

from jarvis.jarvis_utils.config import (
    get_max_input_token_count,
    get_pretty_output,
    is_print_prompt,
    is_immediate_abort,
    is_save_session_history,
    get_data_dir,
)
from jarvis.jarvis_utils.embedding import split_text_into_chunks
from jarvis.jarvis_utils.globals import set_in_chat, get_interrupt, console
import jarvis.jarvis_utils.globals as G
from jarvis.jarvis_utils.output import OutputType, PrettyOutput  # ä¿ç•™ç”¨äºè¯­æ³•é«˜äº®
from jarvis.jarvis_utils.tag import ct, ot
from jarvis.jarvis_utils.utils import get_context_token_count, while_success, while_true


class BasePlatform(ABC):
    """å¤§è¯­è¨€æ¨¡å‹åŸºç±»"""

    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        self.suppress_output = True  # æ·»åŠ è¾“å‡ºæ§åˆ¶æ ‡å¿—
        self.web = False  # æ·»åŠ webå±æ€§ï¼Œé»˜è®¤false
        self._saved = False
        self.model_group: Optional[str] = None
        self._session_history_file: Optional[str] = None

    def __enter__(self) -> Self:
        """è¿›å…¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        return self

    def __exit__(
        self,
        exc_type: Optional[Type[BaseException]],
        exc_val: Optional[BaseException],
        exc_tb: Optional[TracebackType],
    ) -> None:
        """é€€å‡ºä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if not self._saved:
            self.delete_chat()

    @abstractmethod
    def set_model_name(self, model_name: str):
        """è®¾ç½®æ¨¡å‹åç§°"""
        raise NotImplementedError("set_model_name is not implemented")

    def reset(self):
        """é‡ç½®æ¨¡å‹"""
        self.delete_chat()
        self._session_history_file = None

    @abstractmethod
    def chat(self, message: str) -> Generator[str, None, None]:
        """æ‰§è¡Œå¯¹è¯"""
        raise NotImplementedError("chat is not implemented")

    @abstractmethod
    def upload_files(self, file_list: List[str]) -> bool:
        raise NotImplementedError("upload_files is not implemented")

    @abstractmethod
    def support_upload_files(self) -> bool:
        """æ£€æŸ¥å¹³å°æ˜¯å¦æ”¯æŒæ–‡ä»¶ä¸Šä¼ """
        return False

    def _submit_part_with_split(self, part_content: str, threshold_factor: float = 1.0) -> str:
        """æäº¤å•ä¸ªéƒ¨åˆ†ï¼Œå¦‚æœåå¤å¤±è´¥åˆ™å°†å…¶æ‹†åˆ†ã€‚
        
        å‚æ•°:
            part_content: è¦æäº¤çš„å†…å®¹ã€‚
            threshold_factor: è°ƒæ•´tokené˜ˆå€¼çš„å› ç´ ã€‚
            
        è¿”å›:
            æäº¤éƒ¨åˆ†åçš„å“åº”ã€‚
        """
        try:
            response = ""
            for trunk in while_true(
                lambda: while_success(
                    lambda: self._chat(
                        f"<part_content>{part_content}</part_content>\n\nè¯·è¿”å›<å·²æ”¶åˆ°>ï¼Œä¸éœ€è¦è¿”å›å…¶ä»–ä»»ä½•å†…å®¹"
                    )
                )
            ):
                response += trunk
            return response
        except Exception as e:
            # å¦‚æœå•ä¸ªpartåå¤å¤±è´¥ï¼Œå°è¯•å°†å…¶æ‹†åˆ†æˆä¸¤ä»½
            part_token_count = get_context_token_count(part_content)
            base_max_token = get_max_input_token_count(self.model_group)
            adjusted_max_token = int(base_max_token * threshold_factor)
            min_chunk_size = adjusted_max_token - 2048
            
            # å¦‚æœpartå·²ç»å¾ˆå°ï¼ˆå°äºæœ€å°chunk sizeï¼‰ï¼Œæˆ–è€…tokenæ•°å·²ç»å¾ˆå°ï¼Œä¸å†æ‹†åˆ†
            if part_token_count <= min_chunk_size or len(part_content) < 100:
                print(f"âš ï¸ Partæäº¤å¤±è´¥ä¸”å·²æ— æ³•è¿›ä¸€æ­¥æ‹†åˆ†ï¼Œé‡æ–°æŠ›å‡ºå¼‚å¸¸: {e}")
                raise
            
            print(f"âš ï¸ Partæäº¤å¤±è´¥ï¼Œå°è¯•æ‹†åˆ†æˆä¸¤ä»½: {e}")
            # å°†partæ‹†åˆ†æˆä¸¤ä»½ï¼Œä½¿ç”¨æ›´å°çš„max_lengthä»¥ç¡®ä¿æ‹†åˆ†æˆåŠŸ
            # ä½¿ç”¨æ›´ä¿å®ˆçš„é˜ˆå€¼å› å­ï¼ˆè¿›ä¸€æ­¥é™ä½20%ï¼‰æ¥æ‹†åˆ†
            split_threshold_factor = threshold_factor * 0.8
            split_max_token = int(base_max_token * split_threshold_factor)
            split_max_chunk_size = split_max_token - 1024
            chunks = split_text_into_chunks(part_content, split_max_chunk_size, split_max_chunk_size // 2)
            if len(chunks) < 2:
                # å¦‚æœæ— æ³•æ‹†åˆ†ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
                print(f"âš ï¸ æ— æ³•æ‹†åˆ†partï¼Œé‡æ–°æŠ›å‡ºå¼‚å¸¸: {e}")
                raise
            
            # é€’å½’å¤„ç†ä¸¤ä¸ªæ›´å°çš„éƒ¨åˆ†ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„é˜ˆå€¼å› å­
            response = ""
            for i, chunk in enumerate(chunks, 1):
                print(f"â„¹ï¸ å¤„ç†æ‹†åˆ†åçš„ç¬¬{i}/{len(chunks)}éƒ¨åˆ†...")
                chunk_response = self._submit_part_with_split(chunk, split_threshold_factor)
                response += "\n" + chunk_response
            return response

    def _handle_long_context(self, message: str, threshold_factor: float = 1.0) -> str:
        """é€šè¿‡æ‹†åˆ†å’Œåˆ†å—æäº¤æ¥å¤„ç†é•¿ä¸Šä¸‹æ–‡ã€‚
        
        å‚æ•°:
            message: è¦æ‹†åˆ†å’Œæäº¤çš„è¾ƒé•¿æ¶ˆæ¯ã€‚
            threshold_factor: è°ƒæ•´tokené˜ˆå€¼çš„å› ç´ ï¼ˆé»˜è®¤ä¸º1.0ï¼‰ã€‚
                             ä½¿ç”¨å°äº1.0çš„å€¼ï¼ˆä¾‹å¦‚0.8ï¼‰åœ¨é‡è¯•æ—¶é™ä½é˜ˆå€¼ã€‚
            
        è¿”å›:
            æ‰€æœ‰å—æäº¤çš„ç´¯ç§¯å“åº”ã€‚
        """
        base_max_token = get_max_input_token_count(self.model_group)
        adjusted_max_token = int(base_max_token * threshold_factor)
        max_chunk_size = adjusted_max_token - 1024  # ç•™å‡ºä¸€äº›ä½™é‡
        min_chunk_size = adjusted_max_token - 2048
        inputs = split_text_into_chunks(message, max_chunk_size, min_chunk_size)
        print(f"â„¹ï¸ é•¿ä¸Šä¸‹æ–‡ï¼Œåˆ†æ‰¹æäº¤ï¼Œå…±{len(inputs)}éƒ¨åˆ†...")
        prefix_prompt = """
        æˆ‘å°†åˆ†å¤šæ¬¡æä¾›å¤§é‡å†…å®¹ï¼Œåœ¨æˆ‘æ˜ç¡®å‘Šè¯‰ä½ å†…å®¹å·²ç»å…¨éƒ¨æä¾›å®Œæ¯•ä¹‹å‰ï¼Œæ¯æ¬¡ä»…éœ€è¦è¾“å‡º"å·²æ”¶åˆ°"ï¼Œæ˜ç™½è¯·è¾“å‡º"å¼€å§‹æ¥æ”¶è¾“å…¥"ã€‚
        """
        while_true(lambda: while_success(lambda: self._chat(prefix_prompt)))
        submit_count = 0
        length = 0
        response = ""
        for input in inputs:
            submit_count += 1
            length += len(input)

            response += "\n"
            try:
                part_response = self._submit_part_with_split(input, threshold_factor)
                response += part_response
            except Exception as e:
                print(f"âš ï¸ ç¬¬{submit_count}éƒ¨åˆ†æäº¤æœ€ç»ˆå¤±è´¥: {e}")
                raise

        print("âœ… æäº¤å®Œæˆ")
        response += "\n" + while_true(
            lambda: while_success(
                lambda: self._chat("å†…å®¹å·²ç»å…¨éƒ¨æä¾›å®Œæ¯•ï¼Œè¯·æ ¹æ®å†…å®¹ç»§ç»­")
            )
        )
        return response

    def _chat(self, message: str):
        import time

        start_time = time.time()

        # å½“è¾“å…¥ä¸ºç©ºç™½å­—ç¬¦ä¸²æ—¶ï¼Œæ‰“å°è­¦å‘Šå¹¶ç›´æ¥è¿”å›ç©ºå­—ç¬¦ä¸²
        if message.strip() == "":
            print("âš ï¸ è¾“å…¥ä¸ºç©ºç™½å­—ç¬¦ä¸²ï¼Œå·²å¿½ç•¥æœ¬æ¬¡è¯·æ±‚")
            return ""

        input_token_count = get_context_token_count(message)

        if input_token_count > get_max_input_token_count(self.model_group):
            response = self._handle_long_context(message)
        else:
            response = ""

            if not self.suppress_output:
                if get_pretty_output():
                    chat_iterator = self.chat(message)
                    first_chunk = None

                    with Status(
                        f"ğŸ¤” {(G.current_agent_name + ' Â· ') if G.current_agent_name else ''}{self.name()} æ­£åœ¨æ€è€ƒä¸­...",
                        spinner="dots",
                        console=console,
                    ):
                        try:
                            while True:
                                first_chunk = next(chat_iterator)
                                if first_chunk:
                                    break
                        except StopIteration:
                            self._append_session_history(message, "")
                            return ""

                    text_content = Text(overflow="fold")
                    panel = Panel(
                        text_content,
                        title=f"[bold cyan]{(G.current_agent_name + ' Â· ') if G.current_agent_name else ''}{self.name()}[/bold cyan]",
                        subtitle="[yellow]æ­£åœ¨å›ç­”... (æŒ‰ Ctrl+C ä¸­æ–­)[/yellow]",
                        border_style="bright_blue",
                        box=box.ROUNDED,
                        expand=True,  # å…è®¸é¢æ¿è‡ªåŠ¨è°ƒæ•´å¤§å°
                    )

                    with Live(panel, refresh_per_second=4, transient=False) as live:

                        def _update_panel_content(content: str):
                            text_content.append(content, style="bright_white")
                            # --- Scrolling Logic ---
                            # Calculate available height in the panel
                            max_text_height = (
                                console.height - 5
                            )  # Leave space for borders/titles
                            if max_text_height <= 0:
                                max_text_height = 1

                            # Get the actual number of lines the text will wrap to
                            lines = text_content.wrap(
                                console,
                                console.width - 4 if console.width > 4 else 1,
                            )

                            # If content overflows, truncate to show only the last few lines
                            if len(lines) > max_text_height:
                                # Rebuild the text from the wrapped lines to ensure visual consistency
                                # This correctly handles both wrapped long lines and explicit newlines
                                text_content.plain = "\n".join(
                                    [line.plain for line in lines[-max_text_height:]]
                                )

                            panel.subtitle = (
                                "[yellow]æ­£åœ¨å›ç­”... (æŒ‰ Ctrl+C ä¸­æ–­)[/yellow]"
                            )
                            live.update(panel)

                        # Process first chunk
                        response += first_chunk
                        if first_chunk:
                            _update_panel_content(first_chunk)

                        # ç¼“å­˜æœºåˆ¶ï¼šé™ä½æ›´æ–°é¢‘ç‡ï¼Œå‡å°‘ç•Œé¢é—ªçƒ
                        buffer = ""  # å†…å®¹ç¼“å­˜
                        last_update_time = time.time()  # ä¸Šæ¬¡æ›´æ–°æ—¶é—´
                        update_interval = 0.5  # æœ€å°æ›´æ–°é—´éš”ï¼ˆç§’ï¼‰
                        min_buffer_size = 5  # æœ€å°ç¼“å­˜å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰

                        def _flush_buffer():
                            """åˆ·æ–°ç¼“å­˜å†…å®¹åˆ°é¢æ¿"""
                            nonlocal buffer, last_update_time
                            if buffer:
                                _update_panel_content(buffer)
                                buffer = ""
                                last_update_time = time.time()

                        # Process rest of the chunks
                        for s in chat_iterator:
                            if not s:
                                continue
                            response += s  # Accumulate the full response string
                            buffer += s  # ç´¯ç§¯åˆ°ç¼“å­˜

                            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼šç¼“å­˜è¾¾åˆ°é˜ˆå€¼æˆ–è¶…è¿‡æ—¶é—´é—´éš”
                            current_time = time.time()
                            should_update = (
                                len(buffer) >= min_buffer_size
                                or (current_time - last_update_time) >= update_interval
                            )

                            if should_update:
                                _flush_buffer()

                            if is_immediate_abort() and get_interrupt():
                                # ä¸­æ–­æ—¶ä¹Ÿè¦åˆ·æ–°å‰©ä½™ç¼“å­˜
                                _flush_buffer()
                                self._append_session_history(message, response)
                                return response  # Return the partial response immediately

                        # å¾ªç¯ç»“æŸæ—¶ï¼Œåˆ·æ–°æ‰€æœ‰å‰©ä½™ç¼“å­˜å†…å®¹
                        _flush_buffer()

                        # At the end, display the entire response
                        text_content.plain = response

                        end_time = time.time()
                        duration = end_time - start_time
                        panel.subtitle = f"[bold green]âœ“ å¯¹è¯å®Œæˆè€—æ—¶: {duration:.2f}ç§’[/bold green]"
                        live.update(panel)
                    console.print()
                else:
                    # Print a clear prefix line before streaming model output (non-pretty mode)
                    console.print(
                        f"ğŸ¤– æ¨¡å‹è¾“å‡º - {(G.current_agent_name + ' Â· ') if G.current_agent_name else ''}{self.name()}  (æŒ‰ Ctrl+C ä¸­æ–­)",
                        soft_wrap=False,
                    )
                    for s in self.chat(message):
                        console.print(s, end="")
                        response += s
                        if is_immediate_abort() and get_interrupt():
                            self._append_session_history(message, response)
                            return response
                    console.print()
                    end_time = time.time()
                    duration = end_time - start_time
                    console.print(f"âœ“ å¯¹è¯å®Œæˆè€—æ—¶: {duration:.2f}ç§’")
            else:
                for s in self.chat(message):
                    response += s
                    if is_immediate_abort() and get_interrupt():
                        self._append_session_history(message, response)
                        return response
        # Keep original think tag handling
        response = re.sub(
            ot("think") + r".*?" + ct("think"), "", response, flags=re.DOTALL
        )
        response = re.sub(
            ot("thinking") + r".*?" + ct("thinking"), "", response, flags=re.DOTALL
        )
        # Save session history (input and full response)
        self._append_session_history(message, response)
        return response

    def chat_until_success(self, message: str) -> str:
        """ä¸æ¨¡å‹å¯¹è¯ç›´åˆ°æˆåŠŸå“åº”ã€‚
        
        å¦‚æœåˆå§‹å°è¯•å¤±è´¥ï¼ˆå¯èƒ½æ˜¯ç”±äºtokenä¼°ç®—ä¸å‡†ç¡®ï¼‰ï¼Œ
        è‡ªåŠ¨ä½¿ç”¨é•¿ä¸Šä¸‹æ–‡å¤„ç†é‡è¯•ã€‚
        """
        try:
            set_in_chat(True)
            if not self.suppress_output and is_print_prompt():
                PrettyOutput.print(f"{message}", OutputType.USER)  # ä¿ç•™ç”¨äºè¯­æ³•é«˜äº®
            
            # Check if we should use long context handling based on token count
            input_token_count = get_context_token_count(message)
            max_token_count = get_max_input_token_count(self.model_group)
            use_long_context = input_token_count > max_token_count
            
            result: str = ""
            threshold_factor = 1.0  # åˆå§‹é˜ˆå€¼å› å­
            try:
                if use_long_context:
                    # Use long context handling directly
                    result = while_true(
                        lambda: while_success(lambda: self._handle_long_context(message, threshold_factor))
                    )
                else:
                    # Try normal chat first
                    result = while_true(
                        lambda: while_success(lambda: self._chat(message))
                    )
                
                # Check if result is empty or False (retry exhausted)
                # Convert False to empty string for type safety
                if result is False or result == "":
                    raise ValueError("è¿”å›ç»“æœä¸ºç©º")
            except Exception as e:
                # If normal chat failed and we haven't tried long context yet,
                # retry with long context handling (token estimation might be inaccurate)
                if not use_long_context:
                    print(f"âš ï¸ é¦–æ¬¡å°è¯•å¤±è´¥ï¼Œå¯èƒ½æ˜¯tokenä¼°ç®—ä¸å‡†ç¡®ï¼Œå°è¯•ä½¿ç”¨é•¿ä¸Šä¸‹æ–‡å¤„ç†: {e}")
                    # é‡è¯•æ—¶é™ä½é˜ˆå€¼ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„åˆ¤æ–­ï¼Œé¿å…å†æ¬¡è¶…å‡º
                    # é™ä½20%çš„é˜ˆå€¼ï¼Œæˆ–è€…è‡³å°‘é™ä½1024ä¸ªtoken
                    adjusted_max_token = max(
                        int(max_token_count * 0.8),
                        max_token_count - 1024
                    )
                    if input_token_count > adjusted_max_token:
                        # å¦‚æœé™ä½é˜ˆå€¼åä»ç„¶è¶…å‡ºï¼Œç›´æ¥ä½¿ç”¨é•¿ä¸Šä¸‹æ–‡å¤„ç†ï¼Œå¹¶é™ä½é˜ˆå€¼å› å­
                        threshold_factor = 0.8
                        result = while_true(
                            lambda: while_success(lambda: self._handle_long_context(message, threshold_factor))
                        )
                    else:
                        # å¦‚æœé™ä½é˜ˆå€¼åä¸è¶…å‡ºï¼Œå†æ¬¡å°è¯•æ­£å¸¸chat
                        result = while_true(
                            lambda: while_success(lambda: self._chat(message))
                        )
                    if result is False or result == "":
                        raise ValueError("é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¹Ÿå¤±è´¥ï¼Œè¿”å›ç»“æœä¸ºç©º")
                else:
                    # Already tried long context, retry with lowered threshold
                    print(f"âš ï¸ é•¿ä¸Šä¸‹æ–‡å¤„ç†å¤±è´¥ï¼Œé™ä½é˜ˆå€¼åé‡è¯•: {e}")
                    threshold_factor = 0.8  # é™ä½20%çš„é˜ˆå€¼
                    result = while_true(
                        lambda: while_success(lambda: self._handle_long_context(message, threshold_factor))
                    )
                    if result is False or result == "":
                        raise ValueError("é™ä½é˜ˆå€¼åé•¿ä¸Šä¸‹æ–‡å¤„ç†ä»ç„¶å¤±è´¥ï¼Œè¿”å›ç»“æœä¸ºç©º")
            
            from jarvis.jarvis_utils.globals import set_last_message

            set_last_message(result)
            return result
        finally:
            set_in_chat(False)

    @abstractmethod
    def name(self) -> str:
        """æ¨¡å‹åç§°"""
        raise NotImplementedError("name is not implemented")

    @classmethod
    @abstractmethod
    def platform_name(cls) -> str:
        """å¹³å°åç§°"""
        raise NotImplementedError("platform_name is not implemented")

    @abstractmethod
    def delete_chat(self) -> bool:
        """åˆ é™¤å¯¹è¯"""
        raise NotImplementedError("delete_chat is not implemented")

    @abstractmethod
    def save(self, file_path: str) -> bool:
        """ä¿å­˜å¯¹è¯ä¼šè¯åˆ°æ–‡ä»¶ã€‚

        æ³¨æ„:
            æ­¤æ–¹æ³•çš„å®ç°åº”åœ¨æˆåŠŸä¿å­˜åå°†`self._saved`è®¾ç½®ä¸ºTrueï¼Œ
            ä»¥é˜²æ­¢åœ¨å¯¹è±¡é”€æ¯æ—¶åˆ é™¤ä¼šè¯ã€‚

        å‚æ•°:
            file_path: ä¿å­˜ä¼šè¯æ–‡ä»¶çš„è·¯å¾„ã€‚

        è¿”å›:
            å¦‚æœä¿å­˜æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›Falseã€‚
        """
        raise NotImplementedError("save is not implemented")

    @abstractmethod
    def restore(self, file_path: str) -> bool:
        """ä»æ–‡ä»¶æ¢å¤å¯¹è¯ä¼šè¯ã€‚

        å‚æ•°:
            file_path: è¦æ¢å¤ä¼šè¯æ–‡ä»¶çš„è·¯å¾„ã€‚

        è¿”å›:
            å¦‚æœæ¢å¤æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›Falseã€‚
        """
        raise NotImplementedError("restore is not implemented")

    @abstractmethod
    def set_system_prompt(self, message: str):
        """è®¾ç½®ç³»ç»Ÿæ¶ˆæ¯"""
        raise NotImplementedError("set_system_prompt is not implemented")

    @abstractmethod
    def get_model_list(self) -> List[Tuple[str, str]]:
        """è·å–æ¨¡å‹åˆ—è¡¨"""
        raise NotImplementedError("get_model_list is not implemented")

    @classmethod
    @abstractmethod
    def get_required_env_keys(cls) -> List[str]:
        """è·å–å¿…éœ€çš„ç¯å¢ƒå˜é‡é”®"""
        raise NotImplementedError("get_required_env_keys is not implemented")

    @classmethod
    def get_env_defaults(cls) -> Dict[str, str]:
        """è·å–ç¯å¢ƒå˜é‡é»˜è®¤å€¼"""
        return {}

    @classmethod
    def get_env_config_guide(cls) -> Dict[str, str]:
        """è·å–ç¯å¢ƒå˜é‡é…ç½®æŒ‡å—

        è¿”å›:
            Dict[str, str]: å°†ç¯å¢ƒå˜é‡é”®åæ˜ å°„åˆ°å…¶é…ç½®è¯´æ˜çš„å­—å…¸
        """
        return {}

    def set_suppress_output(self, suppress: bool):
        """è®¾ç½®æ˜¯å¦æŠ‘åˆ¶è¾“å‡º"""
        self.suppress_output = suppress

    def set_model_group(self, model_group: Optional[str]):
        """è®¾ç½®æ¨¡å‹ç»„"""
        self.model_group = model_group

    def set_web(self, web: bool):
        """è®¾ç½®ç½‘é¡µæ ‡å¿—"""
        self.web = web

    def _append_session_history(self, user_input: str, model_output: str) -> None:
        """
        Append the user input and model output to a session history file if enabled.
        The file name is generated on first save and reused until reset.
        """
        try:
            if not is_save_session_history():
                return

            if self._session_history_file is None:
                # Ensure session history directory exists under data directory
                data_dir = get_data_dir()
                session_dir = os.path.join(data_dir, "session_history")
                os.makedirs(session_dir, exist_ok=True)

                # Build a safe filename including platform, model and timestamp
                try:
                    platform_name = type(self).platform_name()
                except Exception:
                    platform_name = "unknown_platform"

                try:
                    model_name = self.name()
                except Exception:
                    model_name = "unknown_model"

                safe_platform = re.sub(r"[^\w\-\.]+", "_", str(platform_name))
                safe_model = re.sub(r"[^\w\-\.]+", "_", str(model_name))
                ts = datetime.now().strftime("%Y%m%d_%H%M%S")

                self._session_history_file = os.path.join(
                    session_dir, f"session_history_{safe_platform}_{safe_model}_{ts}.log"
                )

            # Append record
            with open(self._session_history_file, "a", encoding="utf-8", errors="ignore") as f:
                ts_line = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                f.write(f"===== {ts_line} =====\n")
                f.write("USER:\n")
                f.write(f"{user_input}\n")
                f.write("\nASSISTANT:\n")
                f.write(f"{model_output}\n\n")
        except Exception:
            # Do not break chat flow if writing history fails
            pass

    @abstractmethod
    def support_web(self) -> bool:
        """æ£€æŸ¥å¹³å°æ˜¯å¦æ”¯æŒç½‘é¡µåŠŸèƒ½"""
        return False

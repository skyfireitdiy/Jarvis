"""éœ€æ±‚æ¨ç†å™¨æ¨¡å—

ä»æ˜¾å¼éœ€æ±‚æ¨ç†éšå¼éœ€æ±‚ï¼Œæ”¯æŒè§„åˆ™å¼•æ“å’ŒçŸ¥è¯†å›¾è°±æ¨ç†ã€‚
"""

from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, List, Optional, Protocol

from jarvis.jarvis_utils.output import PrettyOutput

from jarvis.jarvis_digital_twin.prediction import (
    InferenceResult,
    PredictionContext,
    PredictionType,
)


class InferenceStrategy(Enum):
    """æ¨ç†ç­–ç•¥æšä¸¾"""

    RULE_BASED = "rule_based"  # çº¯è§„åˆ™æ¨ç†
    PATTERN_BASED = "pattern_based"  # æ¨¡å¼åŒ¹é…æ¨ç†
    HYBRID = "hybrid"  # æ··åˆç­–ç•¥ï¼ˆé»˜è®¤ï¼‰


class KnowledgeGraphProvider(Protocol):
    """çŸ¥è¯†å›¾è°±æä¾›è€…åè®®

    å®šä¹‰çŸ¥è¯†å›¾è°±æŸ¥è¯¢çš„æ ‡å‡†æ¥å£ï¼Œæ”¯æŒä¾èµ–æ³¨å…¥ã€‚
    """

    def query_related(self, concept: str, relation_type: str) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢ç›¸å…³æ¦‚å¿µ"""
        ...

    def get_concept_hierarchy(self, concept: str) -> List[str]:
        """è·å–æ¦‚å¿µå±‚çº§"""
        ...


@dataclass
class InferenceChainStep:
    """æ¨ç†é“¾æ­¥éª¤"""

    step_number: int
    description: str
    input_data: str
    output_data: str
    confidence: float = 1.0


@dataclass
class EvidenceItem:
    """è¯æ®é¡¹"""

    content: str
    source: str
    weight: float = 1.0
    is_supporting: bool = True


class RuleBasedInferrer:
    """è§„åˆ™æ¨ç†å¼•æ“

    åŸºäºé¢„å®šä¹‰è§„åˆ™è¿›è¡Œéœ€æ±‚æ¨ç†ã€‚
    """

    # éšå¼éœ€æ±‚æ˜ å°„ï¼šæ˜¾å¼éœ€æ±‚ -> [(éšå¼éœ€æ±‚, ç½®ä¿¡åº¦, æ¨ç†ä¾æ®)]
    IMPLICIT_NEED_RULES: Dict[str, List[tuple[str, float, str]]] = {
        "implement_feature": [
            ("write_tests", 0.9, "åŠŸèƒ½å®ç°é€šå¸¸éœ€è¦é…å¥—æµ‹è¯•"),
            ("update_documentation", 0.7, "æ–°åŠŸèƒ½éœ€è¦æ–‡æ¡£è¯´æ˜"),
            ("code_review", 0.6, "ä»£ç éœ€è¦å®¡æŸ¥ç¡®ä¿è´¨é‡"),
        ],
        "fix_bug": [
            ("add_regression_test", 0.85, "ä¿®å¤bugåº”æ·»åŠ å›å½’æµ‹è¯•"),
            ("root_cause_analysis", 0.7, "éœ€è¦åˆ†ææ ¹æœ¬åŸå› é˜²æ­¢å¤å‘"),
            ("update_changelog", 0.5, "é‡è¦ä¿®å¤åº”è®°å½•åœ¨å˜æ›´æ—¥å¿—"),
        ],
        "refactor_code": [
            ("run_existing_tests", 0.95, "é‡æ„åå¿…é¡»éªŒè¯ç°æœ‰åŠŸèƒ½"),
            ("update_documentation", 0.6, "é‡æ„å¯èƒ½éœ€è¦æ›´æ–°æ–‡æ¡£"),
            ("performance_check", 0.5, "é‡æ„ååº”æ£€æŸ¥æ€§èƒ½å½±å“"),
        ],
        "write_tests": [
            ("setup_test_environment", 0.8, "æµ‹è¯•éœ€è¦é…ç½®ç¯å¢ƒ"),
            ("mock_dependencies", 0.7, "æµ‹è¯•é€šå¸¸éœ€è¦æ¨¡æ‹Ÿä¾èµ–"),
            ("coverage_analysis", 0.6, "åº”åˆ†ææµ‹è¯•è¦†ç›–ç‡"),
        ],
        "debug_issue": [
            ("reproduce_issue", 0.9, "è°ƒè¯•å‰éœ€è¦å¤ç°é—®é¢˜"),
            ("add_logging", 0.7, "è°ƒè¯•é€šå¸¸éœ€è¦æ·»åŠ æ—¥å¿—"),
            ("isolate_problem", 0.8, "éœ€è¦éš”ç¦»é—®é¢˜èŒƒå›´"),
        ],
        "deploy_application": [
            ("backup_data", 0.85, "éƒ¨ç½²å‰åº”å¤‡ä»½æ•°æ®"),
            ("rollback_plan", 0.9, "éœ€è¦å‡†å¤‡å›æ»šæ–¹æ¡ˆ"),
            ("health_check", 0.8, "éƒ¨ç½²åéœ€è¦å¥åº·æ£€æŸ¥"),
        ],
        "optimize_performance": [
            ("benchmark_baseline", 0.9, "ä¼˜åŒ–å‰éœ€è¦åŸºå‡†æµ‹è¯•"),
            ("profile_code", 0.85, "éœ€è¦æ€§èƒ½åˆ†æå®šä½ç“¶é¢ˆ"),
            ("monitor_metrics", 0.7, "ä¼˜åŒ–åéœ€è¦ç›‘æ§æŒ‡æ ‡"),
        ],
    }

    # åç»­ä»»åŠ¡æ˜ å°„
    FOLLOW_UP_TASK_RULES: Dict[str, List[tuple[str, float, str]]] = {
        "create_file": [
            ("implement_logic", 0.9, "åˆ›å»ºæ–‡ä»¶åéœ€è¦å®ç°é€»è¾‘"),
            ("add_imports", 0.8, "éœ€è¦æ·»åŠ å¿…è¦çš„å¯¼å…¥"),
            ("create_test_file", 0.7, "åº”åˆ›å»ºå¯¹åº”çš„æµ‹è¯•æ–‡ä»¶"),
        ],
        "implement_function": [
            ("write_unit_test", 0.85, "å‡½æ•°å®ç°ååº”ç¼–å†™å•å…ƒæµ‹è¯•"),
            ("add_docstring", 0.7, "åº”æ·»åŠ æ–‡æ¡£å­—ç¬¦ä¸²"),
            ("type_annotations", 0.6, "åº”æ·»åŠ ç±»å‹æ³¨è§£"),
        ],
        "write_test": [
            ("run_test", 0.95, "ç¼–å†™æµ‹è¯•ååº”è¿è¡ŒéªŒè¯"),
            ("check_coverage", 0.7, "åº”æ£€æŸ¥æµ‹è¯•è¦†ç›–ç‡"),
            ("add_edge_cases", 0.6, "åº”æ·»åŠ è¾¹ç•Œæƒ…å†µæµ‹è¯•"),
        ],
        "fix_error": [
            ("verify_fix", 0.95, "ä¿®å¤åéœ€è¦éªŒè¯"),
            ("add_test_case", 0.8, "åº”æ·»åŠ æµ‹è¯•ç”¨ä¾‹é˜²æ­¢å›å½’"),
            ("check_related_code", 0.6, "åº”æ£€æŸ¥ç›¸å…³ä»£ç æ˜¯å¦æœ‰ç±»ä¼¼é—®é¢˜"),
        ],
        "code_review": [
            ("address_comments", 0.9, "éœ€è¦å¤„ç†å®¡æŸ¥æ„è§"),
            ("update_code", 0.8, "å¯èƒ½éœ€è¦æ›´æ–°ä»£ç "),
            ("re_review", 0.6, "ä¿®æ”¹åå¯èƒ½éœ€è¦å†æ¬¡å®¡æŸ¥"),
        ],
        "merge_branch": [
            ("delete_branch", 0.7, "åˆå¹¶åå¯æ¸…ç†åˆ†æ”¯"),
            ("deploy_staging", 0.6, "å¯éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ"),
            ("notify_team", 0.5, "å¯é€šçŸ¥å›¢é˜Ÿæˆå‘˜"),
        ],
    }

    # æ ¹æœ¬åŸå› æ˜ å°„
    ROOT_CAUSE_RULES: Dict[str, List[tuple[str, float, str]]] = {
        "import_error": [
            ("missing_dependency", 0.8, "å¯èƒ½ç¼ºå°‘ä¾èµ–åŒ…"),
            ("wrong_module_path", 0.7, "æ¨¡å—è·¯å¾„å¯èƒ½é”™è¯¯"),
            ("circular_import", 0.5, "å¯èƒ½å­˜åœ¨å¾ªç¯å¯¼å…¥"),
        ],
        "type_error": [
            ("wrong_argument_type", 0.8, "å‚æ•°ç±»å‹å¯èƒ½é”™è¯¯"),
            ("none_value", 0.7, "å¯èƒ½ä¼ å…¥äº†Noneå€¼"),
            ("incompatible_types", 0.6, "ç±»å‹å¯èƒ½ä¸å…¼å®¹"),
        ],
        "attribute_error": [
            ("typo_in_attribute", 0.7, "å±æ€§åå¯èƒ½æ‹¼å†™é”™è¯¯"),
            ("object_not_initialized", 0.6, "å¯¹è±¡å¯èƒ½æœªæ­£ç¡®åˆå§‹åŒ–"),
            ("wrong_object_type", 0.5, "å¯¹è±¡ç±»å‹å¯èƒ½é”™è¯¯"),
        ],
        "test_failure": [
            ("logic_error", 0.7, "ä»£ç é€»è¾‘å¯èƒ½æœ‰é”™è¯¯"),
            ("test_data_issue", 0.6, "æµ‹è¯•æ•°æ®å¯èƒ½æœ‰é—®é¢˜"),
            ("environment_difference", 0.5, "ç¯å¢ƒå·®å¼‚å¯èƒ½å¯¼è‡´å¤±è´¥"),
        ],
        "performance_issue": [
            ("inefficient_algorithm", 0.7, "ç®—æ³•æ•ˆç‡å¯èƒ½è¾ƒä½"),
            ("database_query", 0.6, "æ•°æ®åº“æŸ¥è¯¢å¯èƒ½éœ€è¦ä¼˜åŒ–"),
            ("memory_leak", 0.5, "å¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼"),
        ],
        "connection_error": [
            ("network_issue", 0.7, "ç½‘ç»œå¯èƒ½æœ‰é—®é¢˜"),
            ("service_down", 0.6, "æœåŠ¡å¯èƒ½å·²åœæ­¢"),
            ("wrong_endpoint", 0.5, "ç«¯ç‚¹åœ°å€å¯èƒ½é”™è¯¯"),
        ],
    }

    # å…³é”®è¯æ˜ å°„
    KEYWORD_MAPPINGS: Dict[str, Dict[str, str]] = {
        "need": {
            "implement": "implement_feature",
            "create": "implement_feature",
            "build": "implement_feature",
            "fix": "fix_bug",
            "bug": "fix_bug",
            "error": "fix_bug",
            "refactor": "refactor_code",
            "clean": "refactor_code",
            "test": "write_tests",
            "unittest": "write_tests",
            "debug": "debug_issue",
            "deploy": "deploy_application",
            "release": "deploy_application",
            "optimize": "optimize_performance",
            "performance": "optimize_performance",
        },
        "task": {
            "create": "create_file",
            "new": "create_file",
            "implement": "implement_function",
            "function": "implement_function",
            "method": "implement_function",
            "test": "write_test",
            "fix": "fix_error",
            "review": "code_review",
            "merge": "merge_branch",
        },
        "problem": {
            "import": "import_error",
            "module": "import_error",
            "type": "type_error",
            "typeerror": "type_error",
            "attribute": "attribute_error",
            "attributeerror": "attribute_error",
            "test": "test_failure",
            "fail": "test_failure",
            "slow": "performance_issue",
            "performance": "performance_issue",
            "connection": "connection_error",
            "timeout": "connection_error",
        },
    }

    def infer_implicit_needs(
        self, explicit_need: str
    ) -> List[tuple[str, float, str, List[str]]]:
        """æ¨ç†éšå¼éœ€æ±‚"""
        need_type = self._identify_type(explicit_need, self.KEYWORD_MAPPINGS["need"])
        if need_type not in self.IMPLICIT_NEED_RULES:
            return []
        results = []
        for implicit_need, confidence, reasoning in self.IMPLICIT_NEED_RULES[need_type]:
            chain = [
                f"è¯†åˆ«æ˜¾å¼éœ€æ±‚ç±»å‹: {need_type}",
                f"åº”ç”¨è§„åˆ™: {need_type} -> {implicit_need}",
                f"æ¨ç†ä¾æ®: {reasoning}",
            ]
            results.append((implicit_need, confidence, reasoning, chain))
        return results

    def infer_follow_up_tasks(
        self, current_task: str
    ) -> List[tuple[str, float, str, List[str]]]:
        """æ¨ç†åç»­ä»»åŠ¡"""
        task_type = self._identify_type(current_task, self.KEYWORD_MAPPINGS["task"])
        if task_type not in self.FOLLOW_UP_TASK_RULES:
            return []
        results = []
        for follow_up, confidence, reasoning in self.FOLLOW_UP_TASK_RULES[task_type]:
            chain = [
                f"è¯†åˆ«å½“å‰ä»»åŠ¡ç±»å‹: {task_type}",
                f"åº”ç”¨è§„åˆ™: {task_type} -> {follow_up}",
                f"æ¨ç†ä¾æ®: {reasoning}",
            ]
            results.append((follow_up, confidence, reasoning, chain))
        return results

    def infer_root_cause(self, problem: str) -> List[tuple[str, float, str, List[str]]]:
        """æ¨ç†æ ¹æœ¬åŸå› """
        problem_type = self._identify_type(problem, self.KEYWORD_MAPPINGS["problem"])
        if problem_type not in self.ROOT_CAUSE_RULES:
            return []
        results = []
        for cause, confidence, reasoning in self.ROOT_CAUSE_RULES[problem_type]:
            chain = [
                f"è¯†åˆ«é—®é¢˜ç±»å‹: {problem_type}",
                f"åº”ç”¨è§„åˆ™: {problem_type} -> {cause}",
                f"æ¨ç†ä¾æ®: {reasoning}",
            ]
            results.append((cause, confidence, reasoning, chain))
        return results

    def _identify_type(self, text: str, mapping: Dict[str, str]) -> str:
        """è¯†åˆ«ç±»å‹"""
        if not text:
            return "unknown"
        text_lower = text.lower()
        for keyword, type_name in mapping.items():
            if keyword in text_lower:
                return type_name
        return "unknown"


class PatternBasedInferrer:
    """æ¨¡å¼æ¨ç†å¼•æ“

    åŸºäºç”¨æˆ·ç”»åƒå’Œå†å²æ¨¡å¼è¿›è¡Œæ¨ç†ã€‚
    """

    def __init__(self) -> None:
        """åˆå§‹åŒ–æ¨¡å¼æ¨ç†å¼•æ“"""
        self._pattern_cache: Dict[str, List[tuple[str, float]]] = {}

    def infer_from_profile(
        self,
        user_profile: Dict[str, Any],
        inference_type: str,
    ) -> List[tuple[str, float, str, List[str]]]:
        """åŸºäºç”¨æˆ·ç”»åƒæ¨ç†"""
        if not user_profile:
            return []
        results: List[tuple[str, float, str, List[str]]] = []

        # ä»äº¤äº’æ¨¡å¼ä¸­æå–æ¨ç†
        interaction_pattern = user_profile.get("interaction_pattern", {})
        if interaction_pattern:
            pattern_results = self._infer_from_interaction_pattern(
                interaction_pattern, inference_type
            )
            results.extend(pattern_results)

        # ä»åå¥½ä¸­æå–æ¨ç†
        preferences = user_profile.get("preferences", {})
        if preferences:
            pref_results = self._infer_from_preferences(preferences, inference_type)
            results.extend(pref_results)

        # ä»ç›®æ ‡ä¸­æå–æ¨ç†
        goals = user_profile.get("goals", [])
        if goals:
            goal_results = self._infer_from_goals(goals, inference_type)
            results.extend(goal_results)

        return results

    def _infer_from_interaction_pattern(
        self,
        pattern: Dict[str, Any],
        inference_type: str,
    ) -> List[tuple[str, float, str, List[str]]]:
        """ä»äº¤äº’æ¨¡å¼æ¨ç†"""
        results: List[tuple[str, float, str, List[str]]] = []

        question_pattern = pattern.get("question_pattern", {})
        question_types = question_pattern.get("question_types", {})

        if question_types and inference_type == "implicit_need":
            sorted_types = sorted(
                question_types.items(), key=lambda x: x[1], reverse=True
            )
            for qtype, count in sorted_types[:3]:
                confidence = min(0.8, count / 10)
                chain = [
                    "åˆ†æç”¨æˆ·é—®é¢˜æ¨¡å¼",
                    f"å‘ç°å¸¸è§é—®é¢˜ç±»å‹: {qtype} (å‡ºç°{count}æ¬¡)",
                    "æ¨æ–­å¯èƒ½çš„éšå¼éœ€æ±‚",
                ]
                results.append(
                    (
                        f"help_with_{qtype}",
                        confidence,
                        f"ç”¨æˆ·ç»å¸¸è¯¢é—®{qtype}ç›¸å…³é—®é¢˜",
                        chain,
                    )
                )

        command_pattern = pattern.get("command_pattern", {})
        command_categories = command_pattern.get("command_categories", {})

        if command_categories and inference_type == "follow_up":
            sorted_cats = sorted(
                command_categories.items(), key=lambda x: x[1], reverse=True
            )
            for cat, count in sorted_cats[:3]:
                confidence = min(0.7, count / 10)
                chain = [
                    "åˆ†æç”¨æˆ·å‘½ä»¤æ¨¡å¼",
                    f"å‘ç°å¸¸ç”¨å‘½ä»¤ç±»åˆ«: {cat} (ä½¿ç”¨{count}æ¬¡)",
                    "æ¨æ–­å¯èƒ½çš„åç»­ä»»åŠ¡",
                ]
                results.append(
                    (f"use_{cat}_tools", confidence, f"ç”¨æˆ·ç»å¸¸ä½¿ç”¨{cat}ç±»å·¥å…·", chain)
                )

        return results

    def _infer_from_preferences(
        self,
        preferences: Dict[str, Any],
        inference_type: str,
    ) -> List[tuple[str, float, str, List[str]]]:
        """ä»åå¥½æ¨ç†"""
        results: List[tuple[str, float, str, List[str]]] = []

        tech_stack = preferences.get("tech_stack", {})
        preferred_languages = tech_stack.get("preferred_languages", [])

        if preferred_languages and inference_type == "implicit_need":
            for lang in preferred_languages[:2]:
                chain = [
                    "åˆ†æç”¨æˆ·æŠ€æœ¯æ ˆåå¥½",
                    f"å‘ç°åå¥½è¯­è¨€: {lang}",
                    "æ¨æ–­ç›¸å…³éšå¼éœ€æ±‚",
                ]
                results.append(
                    (f"{lang}_best_practices", 0.6, f"ç”¨æˆ·åå¥½ä½¿ç”¨{lang}", chain)
                )

        code_style = preferences.get("code_style", {})
        if code_style and inference_type == "implicit_need":
            style_prefs = []
            if code_style.get("prefers_type_hints"):
                style_prefs.append("type_annotations")
            if code_style.get("prefers_docstrings"):
                style_prefs.append("documentation")

            for pref in style_prefs:
                chain = [
                    "åˆ†æç”¨æˆ·ä»£ç é£æ ¼åå¥½",
                    f"å‘ç°åå¥½: {pref}",
                    "æ¨æ–­éšå¼éœ€æ±‚",
                ]
                results.append((f"add_{pref}", 0.65, f"ç”¨æˆ·åå¥½{pref}", chain))

        return results

    def _infer_from_goals(
        self,
        goals: List[Dict[str, Any]],
        inference_type: str,
    ) -> List[tuple[str, float, str, List[str]]]:
        """ä»ç›®æ ‡æ¨ç†"""
        results: List[tuple[str, float, str, List[str]]] = []

        for goal in goals[:3]:
            goal_type = goal.get("type", "")
            goal_desc = goal.get("description", "")
            progress = goal.get("progress", 0)

            if inference_type == "follow_up" and progress < 100:
                confidence = 0.5 + (progress / 200)
                chain = [
                    "åˆ†æç”¨æˆ·ç›®æ ‡",
                    f"å‘ç°æœªå®Œæˆç›®æ ‡: {goal_desc}",
                    f"å½“å‰è¿›åº¦: {progress}%",
                    "æ¨æ–­åç»­ä»»åŠ¡",
                ]
                results.append(
                    (
                        f"continue_{goal_type}",
                        confidence,
                        f"ç”¨æˆ·æœ‰æœªå®Œæˆçš„{goal_type}ç›®æ ‡",
                        chain,
                    )
                )

        return results


class NeedInferrer:
    """éœ€æ±‚æ¨ç†å™¨

    ä»æ˜¾å¼éœ€æ±‚æ¨ç†éšå¼éœ€æ±‚ï¼Œæ”¯æŒè§„åˆ™å¼•æ“å’Œæ¨¡å¼åŒ¹é…æ··åˆç­–ç•¥ã€‚
    """

    def __init__(
        self,
        strategy: InferenceStrategy = InferenceStrategy.HYBRID,
        knowledge_graph: Optional[KnowledgeGraphProvider] = None,
        llm_client: Optional[Any] = None,
    ) -> None:
        """åˆå§‹åŒ–éœ€æ±‚æ¨ç†å™¨"""
        self._strategy = strategy
        self._rule_inferrer = RuleBasedInferrer()
        self._pattern_inferrer = PatternBasedInferrer()
        self._knowledge_graph = knowledge_graph
        self._llm_client = llm_client

    @property
    def strategy(self) -> InferenceStrategy:
        """è·å–å½“å‰æ¨ç†ç­–ç•¥"""
        return self._strategy

    @strategy.setter
    def strategy(self, value: InferenceStrategy) -> None:
        """è®¾ç½®æ¨ç†ç­–ç•¥"""
        self._strategy = value

    def _llm_infer_implicit_needs(
        self, explicit_need: str
    ) -> List[tuple[str, float, str, List[str]]]:
        """ä½¿ç”¨LLMæ¨ç†éšå¼éœ€æ±‚"""
        if not self._llm_client:
            return []

        try:
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªéœ€æ±‚åˆ†æä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·çš„æ˜¾å¼éœ€æ±‚ï¼Œæ¨ç†å‡ºå¯èƒ½çš„éšå¼éœ€æ±‚ã€‚

æ˜¾å¼éœ€æ±‚ï¼š{explicit_need}

è¯·è¿”å›JSONæ ¼å¼çš„æ¨ç†ç»“æœï¼Œæ¯ä¸ªç»“æœåŒ…å«ï¼š
- implicit_need: éšå¼éœ€æ±‚æè¿°
- confidence: ç½®ä¿¡åº¦ï¼ˆ0-1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰
- reasoning: æ¨ç†ä¾æ®
- inference_chain: æ¨ç†æ­¥éª¤åˆ—è¡¨

è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
[
  {{
    "implicit_need": "ç¼–å†™å•å…ƒæµ‹è¯•",
    "confidence": 0.9,
    "reasoning": "åŠŸèƒ½å®ç°é€šå¸¸éœ€è¦é…å¥—æµ‹è¯•",
    "inference_chain": ["è¯†åˆ«åˆ°åŠŸèƒ½å¼€å‘éœ€æ±‚", "æ ¹æ®æœ€ä½³å®è·µæ¨æ–­éœ€è¦æµ‹è¯•", "å¾—å‡ºç»“è®º"]
  }}
]

è¯·ä»…è¿”å›JSONæ•°ç»„ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šæ–‡å­—ã€‚"""

            response = self._llm_client.complete(prompt)

            # è§£æJSONå“åº”
            import json
            import re

            # æå–JSONæ•°ç»„éƒ¨åˆ†
            json_match = re.search(r"\[.*\]", response, re.DOTALL)
            if not json_match:
                return []

            json_str = json_match.group(0)
            data = json.loads(json_str)

            results = []
            for item in data:
                implicit_need = item.get("implicit_need", "")
                confidence = float(item.get("confidence", 0.5))
                reasoning = item.get("reasoning", "LLMæ¨ç†")
                inference_chain = item.get("inference_chain", ["LLMæ™ºèƒ½æ¨ç†"])

                if implicit_need:
                    results.append(
                        (implicit_need, confidence, reasoning, inference_chain)
                    )

            return results

        except Exception:
            # LLMæ¨ç†å¤±è´¥æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œå°†é™çº§åˆ°è§„åˆ™æ¨ç†
            return []

    def set_knowledge_graph(self, provider: KnowledgeGraphProvider) -> None:
        """è®¾ç½®çŸ¥è¯†å›¾è°±æä¾›è€…"""
        self._knowledge_graph = provider

    def infer_implicit_needs(
        self,
        context: PredictionContext,
        explicit_need: str,
    ) -> List[InferenceResult]:
        """ä»æ˜¾å¼éœ€æ±‚æ¨ç†éšå¼éœ€æ±‚"""
        if not explicit_need:
            return []

        all_inferences: List[tuple[str, float, str, List[str]]] = []
        inference_mode = "è§„åˆ™"

        # LLMæ¨ç†ï¼ˆä¼˜å…ˆï¼‰
        if self._llm_client is not None:
            llm_results = self._llm_infer_implicit_needs(explicit_need)
            if llm_results:
                all_inferences.extend(llm_results)
                inference_mode = "LLM"
            else:
                # LLMæ¨ç†å¤±è´¥ï¼Œé™çº§åˆ°è§„åˆ™æ¨ç†
                inference_mode = "è§„åˆ™(é™çº§)"

        # è§„åˆ™æ¨ç†ï¼ˆLLMå¤±è´¥æ—¶æˆ–ä½œä¸ºè¡¥å……ï¼‰
        if self._strategy in (InferenceStrategy.RULE_BASED, InferenceStrategy.HYBRID):
            if not all_inferences or self._strategy == InferenceStrategy.HYBRID:
                rule_results = self._rule_inferrer.infer_implicit_needs(explicit_need)
                all_inferences.extend(rule_results)

        # æ¨¡å¼æ¨ç†
        if self._strategy in (
            InferenceStrategy.PATTERN_BASED,
            InferenceStrategy.HYBRID,
        ):
            pattern_results = self._pattern_inferrer.infer_from_profile(
                context.user_profile, "implicit_need"
            )
            adjusted_results = [
                (content, confidence * 0.9, reasoning, chain)
                for content, confidence, reasoning, chain in pattern_results
            ]
            all_inferences.extend(adjusted_results)

        # åº”ç”¨ç”¨æˆ·ç”»åƒè°ƒæ•´
        all_inferences = self._apply_profile_adjustment(
            all_inferences, context.user_profile
        )

        # æ”¶é›†è¯æ®
        supporting_evidence = self._collect_supporting_evidence(
            context, explicit_need, "implicit_need"
        )
        opposing_evidence = self._collect_opposing_evidence(
            context, explicit_need, "implicit_need"
        )

        # æ„å»ºç»“æœ
        results = self._build_inference_results(
            all_inferences,
            PredictionType.IMPLICIT_NEED,
            supporting_evidence,
            opposing_evidence,
        )

        results.sort(key=lambda x: x.confidence_score, reverse=True)

        # è¿‡ç¨‹æ‰“å°
        PrettyOutput.auto_print(
            f"ğŸ“š éœ€æ±‚æ¨ç†: {len(results)}ä¸ªç»“æœ (æ¨¡å¼: {inference_mode})"
        )

        return results

    def infer_follow_up_tasks(
        self,
        context: PredictionContext,
        current_task: str,
    ) -> List[InferenceResult]:
        """ä»å½“å‰ä»»åŠ¡æ¨ç†åç»­ä»»åŠ¡"""
        if not current_task:
            return []

        all_inferences: List[tuple[str, float, str, List[str]]] = []

        # è§„åˆ™æ¨ç†
        if self._strategy in (InferenceStrategy.RULE_BASED, InferenceStrategy.HYBRID):
            rule_results = self._rule_inferrer.infer_follow_up_tasks(current_task)
            all_inferences.extend(rule_results)

        # æ¨¡å¼æ¨ç†
        if self._strategy in (
            InferenceStrategy.PATTERN_BASED,
            InferenceStrategy.HYBRID,
        ):
            pattern_results = self._pattern_inferrer.infer_from_profile(
                context.user_profile, "follow_up"
            )
            adjusted_results = [
                (content, confidence * 0.85, reasoning, chain)
                for content, confidence, reasoning, chain in pattern_results
            ]
            all_inferences.extend(adjusted_results)

        all_inferences = self._apply_profile_adjustment(
            all_inferences, context.user_profile
        )

        supporting_evidence = self._collect_supporting_evidence(
            context, current_task, "follow_up"
        )
        opposing_evidence = self._collect_opposing_evidence(
            context, current_task, "follow_up"
        )

        results = self._build_inference_results(
            all_inferences,
            PredictionType.FOLLOW_UP_TASK,
            supporting_evidence,
            opposing_evidence,
        )

        results.sort(key=lambda x: x.confidence_score, reverse=True)
        return results

    def infer_root_cause(
        self,
        context: PredictionContext,
        problem: str,
    ) -> InferenceResult:
        """ä»é—®é¢˜æè¿°æ¨ç†æ ¹æœ¬åŸå› """
        if not problem:
            return InferenceResult(
                inference_type=PredictionType.ROOT_CAUSE,
                content="æ— æ³•æ¨ç†æ ¹æœ¬åŸå› ",
                confidence_score=0.0,
                reasoning_chain=["é—®é¢˜æè¿°ä¸ºç©º"],
            )

        all_inferences: List[tuple[str, float, str, List[str]]] = []

        # è§„åˆ™æ¨ç†
        if self._strategy in (InferenceStrategy.RULE_BASED, InferenceStrategy.HYBRID):
            rule_results = self._rule_inferrer.infer_root_cause(problem)
            all_inferences.extend(rule_results)

        # æ¨¡å¼æ¨ç†ï¼ˆä»å†å²é—®é¢˜ä¸­å­¦ä¹ ï¼‰
        if self._strategy in (
            InferenceStrategy.PATTERN_BASED,
            InferenceStrategy.HYBRID,
        ):
            pattern_results = self._infer_root_cause_from_history(context, problem)
            all_inferences.extend(pattern_results)

        if not all_inferences:
            return InferenceResult(
                inference_type=PredictionType.ROOT_CAUSE,
                content="æ— æ³•ç¡®å®šæ ¹æœ¬åŸå› ",
                confidence_score=0.1,
                reasoning_chain=[
                    f"åˆ†æé—®é¢˜: {problem[:100]}",
                    "æœªæ‰¾åˆ°åŒ¹é…çš„é—®é¢˜æ¨¡å¼",
                    "å»ºè®®è¿›ä¸€æ­¥è°ƒæŸ¥",
                ],
            )

        all_inferences = self._apply_profile_adjustment(
            all_inferences, context.user_profile
        )

        supporting_evidence = self._collect_supporting_evidence(
            context, problem, "root_cause"
        )
        opposing_evidence = self._collect_opposing_evidence(
            context, problem, "root_cause"
        )

        best = max(all_inferences, key=lambda x: x[1])
        content, confidence, reasoning, chain = best

        related = self._build_inference_results(
            all_inferences[1:4],
            PredictionType.ROOT_CAUSE,
            supporting_evidence,
            opposing_evidence,
        )

        return InferenceResult(
            inference_type=PredictionType.ROOT_CAUSE,
            content=content,
            confidence_score=confidence,
            reasoning_chain=chain,
            supporting_evidence=supporting_evidence,
            opposing_evidence=opposing_evidence,
            related_inferences=related,
        )

    def _infer_root_cause_from_history(
        self,
        context: PredictionContext,
        problem: str,
    ) -> List[tuple[str, float, str, List[str]]]:
        """ä»å†å²è®°å½•æ¨ç†æ ¹æœ¬åŸå› """
        results: List[tuple[str, float, str, List[str]]] = []
        project_state = context.project_state
        if project_state:
            error_history = project_state.get("error_history", [])
            for error in error_history[-5:]:
                error_type = error.get("type", "")
                error_solution = error.get("solution", "")
                if error_type and self._is_similar_problem(problem, error_type):
                    chain = [
                        "åˆ†æå†å²é”™è¯¯è®°å½•",
                        f"å‘ç°ç±»ä¼¼é—®é¢˜: {error_type}",
                        f"å†å²è§£å†³æ–¹æ¡ˆ: {error_solution}",
                    ]
                    results.append(
                        (error_solution, 0.75, "å†å²ä¸Šç±»ä¼¼é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ", chain)
                    )
        return results

    def _is_similar_problem(self, problem1: str, problem2: str) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªé—®é¢˜æ˜¯å¦ç›¸ä¼¼"""
        p1 = problem1.lower()
        p2 = problem2.lower()
        keywords1 = set(p1.split())
        keywords2 = set(p2.split())
        common = keywords1 & keywords2
        if len(common) >= 2:
            return True
        return p1 in p2 or p2 in p1

    def _apply_profile_adjustment(
        self,
        inferences: List[tuple[str, float, str, List[str]]],
        user_profile: Dict[str, Any],
    ) -> List[tuple[str, float, str, List[str]]]:
        """åº”ç”¨ç”¨æˆ·ç”»åƒè°ƒæ•´ç½®ä¿¡åº¦"""
        if not user_profile or not inferences:
            return inferences

        adjusted: List[tuple[str, float, str, List[str]]] = []
        preferences = user_profile.get("preferences", {})
        tech_stack = preferences.get("tech_stack", {})
        preferred_langs = tech_stack.get("preferred_languages", [])

        for content, confidence, reasoning, chain in inferences:
            boost = 0.0
            content_lower = content.lower()
            for lang in preferred_langs:
                if lang.lower() in content_lower:
                    boost = max(boost, 0.05)
            new_confidence = min(1.0, confidence + boost)
            adjusted.append((content, new_confidence, reasoning, chain))

        return adjusted

    def _collect_supporting_evidence(
        self,
        context: PredictionContext,
        input_text: str,
        inference_type: str,
    ) -> List[str]:
        """æ”¶é›†æ”¯æŒè¯æ®"""
        evidence: List[str] = []

        if input_text:
            evidence.append(f"è¾“å…¥: {input_text[:100]}")

        if context.conversation_history:
            evidence.append(f"å¯¹è¯å†å²é•¿åº¦: {len(context.conversation_history)}")

        if inference_type == "implicit_need" and context.code_context:
            modified = context.code_context.get("modified_files", [])
            if modified:
                evidence.append(f"ä¿®æ”¹çš„æ–‡ä»¶: {', '.join(modified[:3])}")

        if inference_type == "root_cause" and context.project_state:
            if context.project_state.get("has_errors"):
                evidence.append("æ£€æµ‹åˆ°é”™è¯¯çŠ¶æ€")

        return evidence

    def _collect_opposing_evidence(
        self,
        context: PredictionContext,
        input_text: str,
        inference_type: str,
    ) -> List[str]:
        """æ”¶é›†åå¯¹è¯æ®"""
        evidence: List[str] = []

        if not context.user_profile:
            evidence.append("ç¼ºå°‘ç”¨æˆ·ç”»åƒæ•°æ®")

        if not context.conversation_history:
            evidence.append("ç¼ºå°‘å¯¹è¯å†å²")

        if inference_type == "follow_up" and not context.code_context:
            evidence.append("ç¼ºå°‘ä»£ç ä¸Šä¸‹æ–‡")

        return evidence

    def _build_inference_results(
        self,
        inferences: List[tuple[str, float, str, List[str]]],
        inference_type: PredictionType,
        supporting_evidence: List[str],
        opposing_evidence: List[str],
    ) -> List[InferenceResult]:
        """æ„å»ºæ¨ç†ç»“æœåˆ—è¡¨"""
        results: List[InferenceResult] = []
        for content, confidence, reasoning, chain in inferences:
            results.append(
                InferenceResult(
                    inference_type=inference_type,
                    content=content,
                    confidence_score=confidence,
                    reasoning_chain=chain,
                    supporting_evidence=supporting_evidence.copy(),
                    opposing_evidence=opposing_evidence.copy(),
                )
            )
        return results

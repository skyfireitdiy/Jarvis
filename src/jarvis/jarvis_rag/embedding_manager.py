import torch
from typing import List, cast
from langchain_huggingface import HuggingFaceEmbeddings

from .cache import EmbeddingCache


class EmbeddingManager:
    """
    ç®¡ç†æœ¬åœ°åµŒå…¥æ¨¡å‹çš„åŠ è½½å’Œä½¿ç”¨ï¼Œå¹¶å¸¦æœ‰ç¼“å­˜åŠŸèƒ½ã€‚

    è¯¥ç±»è´Ÿè´£ä»Hugging FaceåŠ è½½æŒ‡å®šçš„æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨åŸºäºç£ç›˜çš„ç¼“å­˜
    æ¥é¿å…ä¸ºç›¸åŒæ–‡æœ¬é‡æ–°è®¡ç®—åµŒå…¥ã€‚
    """

    def __init__(self, model_name: str, cache_dir: str):
        """
        åˆå§‹åŒ–EmbeddingManagerã€‚

        å‚æ•°:
            model_name: è¦åŠ è½½çš„Hugging Faceæ¨¡å‹çš„åç§°ã€‚
            cache_dir: ç”¨äºå­˜å‚¨åµŒå…¥ç¼“å­˜çš„ç›®å½•ã€‚
        """
        self.model_name = model_name

        print(f"ğŸš€ åˆå§‹åŒ–åµŒå…¥ç®¡ç†å™¨, æ¨¡å‹: '{self.model_name}'...")

        # ç¼“å­˜çš„saltæ˜¯æ¨¡å‹åç§°ï¼Œä»¥é˜²æ­¢å†²çª
        self.cache = EmbeddingCache(cache_dir=cache_dir, salt=self.model_name)
        self.model = self._load_model()

    def _load_model(self) -> HuggingFaceEmbeddings:
        """æ ¹æ®é…ç½®åŠ è½½Hugging FaceåµŒå…¥æ¨¡å‹ã€‚"""
        model_kwargs = {"device": "cuda" if torch.cuda.is_available() else "cpu"}
        encode_kwargs = {"normalize_embeddings": True}

        try:
            return HuggingFaceEmbeddings(
                model_name=self.model_name,
                model_kwargs=model_kwargs,
                encode_kwargs=encode_kwargs,
                show_progress=True,
            )
        except Exception as e:
            print(f"âŒ åŠ è½½åµŒå…¥æ¨¡å‹ '{self.model_name}' æ—¶å‡ºé”™: {e}")
            print("è¯·ç¡®ä¿æ‚¨å·²å®‰è£… 'sentence_transformers' å’Œ 'torch'ã€‚")
            raise

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        ä½¿ç”¨ç¼“å­˜ä¸ºæ–‡æ¡£åˆ—è¡¨è®¡ç®—åµŒå…¥ã€‚

        å‚æ•°:
            texts: è¦åµŒå…¥çš„æ–‡æ¡£ï¼ˆå­—ç¬¦ä¸²ï¼‰åˆ—è¡¨ã€‚

        è¿”å›:
            ä¸€ä¸ªåµŒå…¥åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£å¯¹åº”ä¸€ä¸ªåµŒå…¥ã€‚
        """
        if not texts:
            return []

        # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å·²å­˜åœ¨åµŒå…¥
        cached_embeddings = self.cache.get_batch(texts)

        texts_to_embed = []
        indices_to_embed = []
        for i, (text, cached) in enumerate(zip(texts, cached_embeddings)):
            if cached is None:
                texts_to_embed.append(text)
                indices_to_embed.append(i)

        # ä¸ºä¸åœ¨ç¼“å­˜ä¸­çš„æ–‡æœ¬è®¡ç®—åµŒå…¥
        if texts_to_embed:
            print(
                f"ğŸ” ç¼“å­˜æœªå‘½ä¸­ã€‚æ­£åœ¨ä¸º {len(texts_to_embed)}/{len(texts)} ä¸ªæ–‡æ¡£è®¡ç®—åµŒå…¥ã€‚"
            )
            new_embeddings = self.model.embed_documents(texts_to_embed)

            # å°†æ–°çš„åµŒå…¥å­˜å‚¨åœ¨ç¼“å­˜ä¸­
            self.cache.set_batch(texts_to_embed, new_embeddings)

            # å°†æ–°çš„åµŒå…¥æ”¾å›ç»“æœåˆ—è¡¨ä¸­
            for i, embedding in zip(indices_to_embed, new_embeddings):
                cached_embeddings[i] = embedding
        else:
            print(f"âœ… ç¼“å­˜å‘½ä¸­ã€‚æ‰€æœ‰ {len(texts)} ä¸ªæ–‡æ¡£çš„åµŒå…¥å‡ä»ç¼“å­˜ä¸­æ£€ç´¢ã€‚")

        return cast(List[List[float]], cached_embeddings)

    def embed_query(self, text: str) -> List[float]:
        """
        ä¸ºå•ä¸ªæŸ¥è¯¢è®¡ç®—åµŒå…¥ã€‚
        æŸ¥è¯¢é€šå¸¸ä¸è¢«ç¼“å­˜ï¼Œä½†å¦‚æœéœ€è¦å¯ä»¥æ·»åŠ ã€‚
        """
        return self.model.embed_query(text)

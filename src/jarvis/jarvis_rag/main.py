import os
import numpy as np
import faiss
from typing import List, Tuple, Optional, Dict
import pickle
from dataclasses import dataclass
from tqdm import tqdm
import fitz  # PyMuPDF for PDF files
from docx import Document as DocxDocument  # python-docx for DOCX files
from pathlib import Path

from yaspin import yaspin
from jarvis.jarvis_platform.registry import PlatformRegistry
import lzma  # æ·»åŠ  lzma å¯¼å…¥
from threading import Lock
import hashlib

from jarvis.jarvis_utils.config import get_max_paragraph_length, get_max_token_count, get_min_paragraph_length, get_thread_count, get_rag_ignored_paths
from jarvis.jarvis_utils.embedding import get_context_token_count, get_embedding, get_embedding_batch, load_embedding_model
from jarvis.jarvis_utils.output import OutputType, PrettyOutput
from jarvis.jarvis_utils.utils import  get_file_md5, init_env, init_gpu_config

@dataclass
class Document:
    """Document class, for storing document content and metadata"""
    content: str  # Document content
    metadata: Dict  # Metadata (file path, position, etc.)
    md5: str = ""  # File MD5 value, for incremental update detection

class FileProcessor:
    """Base class for file processor"""
    @staticmethod
    def can_handle(file_path: str) -> bool:
        """Determine if the file can be processed"""
        raise NotImplementedError
        
    @staticmethod
    def extract_text(file_path: str) -> str:
        """Extract file text content"""
        raise NotImplementedError

class TextFileProcessor(FileProcessor):
    """Text file processor"""
    ENCODINGS = ['utf-8', 'gbk', 'gb2312', 'latin1']
    SAMPLE_SIZE = 8192  # Read the first 8KB to detect encoding
    
    @staticmethod
    def can_handle(file_path: str) -> bool:
        """Determine if the file is a text file by trying to decode it"""
        try:
            # Read the first part of the file to detect encoding
            with open(file_path, 'rb') as f:
                sample = f.read(TextFileProcessor.SAMPLE_SIZE)
                
            # Check if it contains null bytes (usually represents a binary file)
            if b'\x00' in sample:
                return False
                
            # Check if it contains too many non-printable characters (usually represents a binary file)
            non_printable = sum(1 for byte in sample if byte < 32 and byte not in (9, 10, 13))  # tab, newline, carriage return
            if non_printable / len(sample) > 0.3:  # If non-printable characters exceed 30%, it is considered a binary file
                return False
                
            # Try to decode with different encodings
            for encoding in TextFileProcessor.ENCODINGS:
                try:
                    sample.decode(encoding)
                    return True
                except UnicodeDecodeError:
                    continue
                    
            return False
            
        except Exception:
            return False
    
    @staticmethod
    def extract_text(file_path: str) -> str:
        """Extract text content, using the detected correct encoding"""
        detected_encoding = None
        try:
            # First try to detect encoding
            with open(file_path, 'rb') as f:
                raw_data = f.read()
                
            # Try different encodings
            for encoding in TextFileProcessor.ENCODINGS:
                try:
                    raw_data.decode(encoding)
                    detected_encoding = encoding
                    break
                except UnicodeDecodeError:
                    continue
                    
            if not detected_encoding:
                raise UnicodeDecodeError(f"Failed to decode file with supported encodings: {file_path}") # type: ignore
                
            # Use the detected encoding to read the file
            with open(file_path, 'r', encoding=detected_encoding, errors='ignore') as f:
                content = f.read()
                
            # Normalize Unicode characters
            import unicodedata
            content = unicodedata.normalize('NFKC', content)
            
            return content
            
        except Exception as e:
            raise Exception(f"Failed to read file: {str(e)}")

class PDFProcessor(FileProcessor):
    """PDF file processor"""
    @staticmethod
    def can_handle(file_path: str) -> bool:
        return Path(file_path).suffix.lower() == '.pdf'
    
    @staticmethod
    def extract_text(file_path: str) -> str:
        text_parts = []
        with fitz.open(file_path) as doc: # type: ignore
            for page in doc:
                text_parts.append(page.get_text()) # type: ignore
        return "\n".join(text_parts)

class DocxProcessor(FileProcessor):
    """DOCX file processor"""
    @staticmethod
    def can_handle(file_path: str) -> bool:
        return Path(file_path).suffix.lower() == '.docx'
    
    @staticmethod
    def extract_text(file_path: str) -> str:
        doc = DocxDocument(file_path)
        return "\n".join([paragraph.text for paragraph in doc.paragraphs])

class RAGTool:
    def __init__(self, root_dir: str):
        """Initialize RAG tool
        
        Args:
            root_dir: Project root directory
        """
        with yaspin(text="åˆå§‹åŒ–ç¯å¢ƒ...", color="cyan") as spinner:
            init_env()
            self.root_dir = root_dir
            os.chdir(self.root_dir)
            spinner.text = "ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ"
            spinner.ok("âœ…")
        
        # Initialize configuration
        with yaspin(text="åˆå§‹åŒ–é…ç½®...", color="cyan") as spinner:
            self.min_paragraph_length = get_min_paragraph_length()  # Minimum paragraph length
            self.max_paragraph_length = get_max_paragraph_length()  # Maximum paragraph length
            self.context_window = 5  # Fixed context window size
            self.max_token_count = int(get_max_token_count() * 0.8)
            spinner.text = "é…ç½®åˆå§‹åŒ–å®Œæˆ"
            spinner.ok("âœ…")
        
        # Initialize data directory
        with yaspin(text="åˆå§‹åŒ–æ•°æ®ç›®å½•...", color="cyan") as spinner:
            self.data_dir = os.path.join(self.root_dir, ".jarvis/rag")
            if not os.path.exists(self.data_dir):
                os.makedirs(self.data_dir)
            spinner.text = "æ•°æ®ç›®å½•åˆå§‹åŒ–å®Œæˆ"
            spinner.ok("âœ…")
            
        # Initialize embedding model
        with yaspin(text="åˆå§‹åŒ–æ¨¡å‹...", color="cyan") as spinner:
            try:
                self.embedding_model = load_embedding_model()
                self.vector_dim = self.embedding_model.get_sentence_embedding_dimension()
                spinner.text = "æ¨¡å‹åŠ è½½å®Œæˆ"
                spinner.ok("âœ…")
            except Exception as e:
                spinner.text = "æ¨¡å‹åŠ è½½å¤±è´¥"
                spinner.fail("âŒ")
                raise

        with yaspin(text="åˆå§‹åŒ–ç¼“å­˜ç›®å½•...", color="cyan") as spinner:
            self.cache_dir = os.path.join(self.data_dir, "cache")
            if not os.path.exists(self.cache_dir):
                os.makedirs(self.cache_dir)
                
            self.documents: List[Document] = []
            self.index = None
            self.flat_index = None
            self.file_md5_cache = {}
            spinner.text = "ç¼“å­˜ç›®å½•åˆå§‹åŒ–å®Œæˆ"
            spinner.ok("âœ…")
        
        # åŠ è½½ç¼“å­˜ç´¢å¼•
        self._load_cache_index()

        # Register file processors
        with yaspin(text="åˆå§‹åŒ–æ–‡ä»¶å¤„ç†å™¨...", color="cyan") as spinner:
            self.file_processors = [
                TextFileProcessor(),
                PDFProcessor(),
                DocxProcessor()
            ]
            spinner.text = "æ–‡ä»¶å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ"
            spinner.ok("âœ…")


        # Add thread related configuration
        with yaspin(text="åˆå§‹åŒ–çº¿ç¨‹é…ç½®...", color="cyan") as spinner:
            self.thread_count = get_thread_count()
            self.vector_lock = Lock()  # Protect vector list concurrency
            spinner.text = "çº¿ç¨‹é…ç½®åˆå§‹åŒ–å®Œæˆ"
            spinner.ok("âœ…")

        # åˆå§‹åŒ– GPU å†…å­˜é…ç½®
        with yaspin(text="åˆå§‹åŒ– GPU å†…å­˜é…ç½®...", color="cyan") as spinner:
            with spinner.hidden():
                self.gpu_config = init_gpu_config()
            spinner.text = "GPU å†…å­˜é…ç½®åˆå§‹åŒ–å®Œæˆ"
            spinner.ok("âœ…")


    def _get_cache_path(self, file_path: str, cache_type: str = "doc") -> str:
        """Get cache file path for a document
        
        Args:
            file_path: Original file path
            cache_type: Type of cache ("doc" for documents, "vec" for vectors)
            
        Returns:
            str: Cache file path
        """
        # ä½¿ç”¨æ–‡ä»¶è·¯å¾„çš„å“ˆå¸Œä½œä¸ºç¼“å­˜æ–‡ä»¶å
        file_hash = hashlib.md5(file_path.encode()).hexdigest()
        
        # ç¡®ä¿ä¸åŒç±»å‹çš„ç¼“å­˜æœ‰ä¸åŒçš„ç›®å½•
        if cache_type == "doc":
            cache_subdir = os.path.join(self.cache_dir, "documents")
        elif cache_type == "vec":
            cache_subdir = os.path.join(self.cache_dir, "vectors")
        else:
            cache_subdir = self.cache_dir
            
        # ç¡®ä¿å­ç›®å½•å­˜åœ¨
        if not os.path.exists(cache_subdir):
            os.makedirs(cache_subdir)
            
        return os.path.join(cache_subdir, f"{file_hash}.cache")

    def _load_cache_index(self):
        """Load cache index"""
        index_path = os.path.join(self.data_dir, "index.pkl")
        if os.path.exists(index_path):
            try:
                with yaspin(text="åŠ è½½ç¼“å­˜ç´¢å¼•...", color="cyan") as spinner:
                    with lzma.open(index_path, 'rb') as f:
                        cache_data = pickle.load(f)
                        self.file_md5_cache = cache_data.get("file_md5_cache", {})
                    spinner.text = "ç¼“å­˜ç´¢å¼•åŠ è½½å®Œæˆ"
                    spinner.ok("âœ…")
                        
                # ä»å„ä¸ªç¼“å­˜æ–‡ä»¶åŠ è½½æ–‡æ¡£
                with yaspin(text="åŠ è½½ç¼“å­˜æ–‡ä»¶...", color="cyan") as spinner:
                    for file_path in self.file_md5_cache:
                        doc_cache_path = self._get_cache_path(file_path, "doc")
                        if os.path.exists(doc_cache_path):
                            try:
                                with lzma.open(doc_cache_path, 'rb') as f:
                                    doc_cache_data = pickle.load(f)
                                    self.documents.extend(doc_cache_data["documents"])
                                spinner.text = f"åŠ è½½æ–‡æ¡£ç¼“å­˜: {file_path}"
                            except Exception as e:
                                spinner.write(f"âŒ åŠ è½½æ–‡æ¡£ç¼“å­˜å¤±è´¥: {file_path}: {str(e)}")
                    spinner.text = "æ–‡æ¡£ç¼“å­˜åŠ è½½å®Œæˆ"
                    spinner.ok("âœ…")
                
                # é‡å»ºå‘é‡ç´¢å¼•
                if self.documents:
                    with yaspin(text="é‡å»ºå‘é‡ç´¢å¼•...", color="cyan") as spinner:
                        vectors = []
                        
                        # æŒ‰ç…§æ–‡æ¡£åˆ—è¡¨é¡ºåºåŠ è½½å‘é‡
                        processed_files = set()
                        for doc in self.documents:
                            file_path = doc.metadata['file_path']
                            
                            # é¿å…é‡å¤å¤„ç†åŒä¸€ä¸ªæ–‡ä»¶
                            if file_path in processed_files:
                                continue
                                
                            processed_files.add(file_path)
                            vec_cache_path = self._get_cache_path(file_path, "vec")
                            
                            if os.path.exists(vec_cache_path):
                                try:
                                    # åŠ è½½è¯¥æ–‡ä»¶çš„å‘é‡ç¼“å­˜
                                    with lzma.open(vec_cache_path, 'rb') as f:
                                        vec_cache_data = pickle.load(f)
                                        file_vectors = vec_cache_data["vectors"]
                                    
                                    # æŒ‰ç…§æ–‡æ¡£çš„chunk_indexæ£€ç´¢å¯¹åº”å‘é‡
                                    doc_indices = [d.metadata['chunk_index'] for d in self.documents 
                                                if d.metadata['file_path'] == file_path]
                                    
                                    # æ£€æŸ¥å‘é‡æ•°é‡ä¸æ–‡æ¡£å—æ•°é‡æ˜¯å¦åŒ¹é…
                                    if len(doc_indices) <= file_vectors.shape[0]:
                                        for idx in doc_indices:
                                            if idx < file_vectors.shape[0]:
                                                vectors.append(file_vectors[idx].reshape(1, -1))
                                    else:
                                        spinner.write(f"âš ï¸ å‘é‡ç¼“å­˜ä¸åŒ¹é…: {file_path}")
                                        
                                    spinner.text = f"åŠ è½½å‘é‡ç¼“å­˜: {file_path}"
                                except Exception as e:
                                    spinner.write(f"âŒ åŠ è½½å‘é‡ç¼“å­˜å¤±è´¥: {file_path}: {str(e)}")
                            else:
                                spinner.write(f"âš ï¸ ç¼ºå°‘å‘é‡ç¼“å­˜: {file_path}")
                        
                        if vectors:
                            vectors = np.vstack(vectors)
                            self._build_index(vectors, spinner)
                        spinner.text = f"å‘é‡ç´¢å¼•é‡å»ºå®Œæˆï¼ŒåŠ è½½ {len(self.documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ"
                        spinner.ok("âœ…")
                                
            except Exception as e:
                PrettyOutput.print(f"åŠ è½½ç¼“å­˜ç´¢å¼•å¤±è´¥: {str(e)}", 
                                output_type=OutputType.WARNING)
                self.documents = []
                self.index = None
                self.flat_index = None
                self.file_md5_cache = {}

    def _save_cache(self, file_path: str, documents: List[Document], vectors: np.ndarray, spinner=None):
        """Save cache for a single file
        
        Args:
            file_path: File path
            documents: List of documents
            vectors: Document vectors
            spinner: Optional spinner for progress display
        """
        try:
            # ä¿å­˜æ–‡æ¡£ç¼“å­˜
            if spinner:
                spinner.text = f"ä¿å­˜ {file_path} çš„æ–‡æ¡£ç¼“å­˜..."
            doc_cache_path = self._get_cache_path(file_path, "doc")
            doc_cache_data = {
                "documents": documents
            }
            with lzma.open(doc_cache_path, 'wb') as f:
                pickle.dump(doc_cache_data, f)
                
            # ä¿å­˜å‘é‡ç¼“å­˜
            if spinner:
                spinner.text = f"ä¿å­˜ {file_path} çš„å‘é‡ç¼“å­˜..."
            vec_cache_path = self._get_cache_path(file_path, "vec")
            vec_cache_data = {
                "vectors": vectors
            }
            with lzma.open(vec_cache_path, 'wb') as f:
                pickle.dump(vec_cache_data, f)
                
            # æ›´æ–°å¹¶ä¿å­˜ç´¢å¼•
            if spinner:
                spinner.text = f"æ›´æ–° {file_path} çš„ç´¢å¼•ç¼“å­˜..."
            index_path = os.path.join(self.data_dir, "index.pkl")
            index_data = {
                "file_md5_cache": self.file_md5_cache
            }
            with lzma.open(index_path, 'wb') as f:
                pickle.dump(index_data, f)
            
            if spinner:
                spinner.text = f"{file_path} çš„ç¼“å­˜ä¿å­˜å®Œæˆ"
                            
        except Exception as e:
            if spinner:
                spinner.text = f"ä¿å­˜ {file_path} çš„ç¼“å­˜å¤±è´¥: {str(e)}"
            PrettyOutput.print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {str(e)}", output_type=OutputType.ERROR)

    def _build_index(self, vectors: np.ndarray, spinner=None):
        """Build FAISS index"""
        if vectors.shape[0] == 0:
            if spinner:
                spinner.text = "å‘é‡ä¸ºç©ºï¼Œè·³è¿‡ç´¢å¼•æ„å»º"
            self.index = None
            self.flat_index = None
            return
            
        # Create a flat index to store original vectors, for reconstruction
        if spinner:
            spinner.text = "åˆ›å»ºå¹³é¢ç´¢å¼•ç”¨äºå‘é‡é‡å»º..."
        self.flat_index = faiss.IndexFlatIP(self.vector_dim)
        self.flat_index.add(vectors) # type: ignore
        
        # Create an IVF index for fast search
        if spinner:
            spinner.text = "åˆ›å»ºIVFç´¢å¼•ç”¨äºå¿«é€Ÿæœç´¢..."
        nlist = max(4, int(vectors.shape[0] / 1000))  # æ¯1000ä¸ªå‘é‡ä¸€ä¸ªèšç±»ä¸­å¿ƒ
        quantizer = faiss.IndexFlatIP(self.vector_dim)
        self.index = faiss.IndexIVFFlat(quantizer, self.vector_dim, nlist, faiss.METRIC_INNER_PRODUCT)
        
        # Train and add vectors
        if spinner:
            spinner.text = f"è®­ç»ƒç´¢å¼•ï¼ˆ{vectors.shape[0]}ä¸ªå‘é‡ï¼Œ{nlist}ä¸ªèšç±»ä¸­å¿ƒï¼‰..."
        self.index.train(vectors) # type: ignore
        
        if spinner:
            spinner.text = "æ·»åŠ å‘é‡åˆ°ç´¢å¼•..."
        self.index.add(vectors) # type: ignore
        
        # Set the number of clusters to probe during search
        if spinner:
            spinner.text = "è®¾ç½®æœç´¢å‚æ•°..."
        self.index.nprobe = min(nlist, 10)
        
        if spinner:
            spinner.text = f"ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {vectors.shape[0]} ä¸ªå‘é‡"

    def _split_text(self, text: str) -> List[str]:
        """Use a more intelligent splitting strategy"""
        # Add overlapping blocks to maintain context consistency
        overlap_size = min(200, self.max_paragraph_length // 4)
        
        paragraphs = []
        current_chunk = []
        current_length = 0
        
        # First split by sentence
        sentences = []
        current_sentence = []
        sentence_ends = {'ã€‚', 'ï¼', 'ï¼Ÿ', 'â€¦', '.', '!', '?'}
        
        for char in text:
            current_sentence.append(char)
            if char in sentence_ends:
                sentence = ''.join(current_sentence)
                if sentence.strip():
                    sentences.append(sentence)
                current_sentence = []
        
        if current_sentence:
            sentence = ''.join(current_sentence)
            if sentence.strip():
                sentences.append(sentence)
        
        # Build overlapping blocks based on sentences
        for sentence in sentences:
            if current_length + len(sentence) > self.max_paragraph_length:
                if current_chunk:
                    chunk_text = ' '.join(current_chunk)
                    if len(chunk_text) >= self.min_paragraph_length:
                        paragraphs.append(chunk_text)
                        
                    # Keep some content as overlap
                    overlap_text = ' '.join(current_chunk[-2:])  # Keep the last two sentences
                    current_chunk = []
                    if overlap_text:
                        current_chunk.append(overlap_text)
                        current_length = len(overlap_text)
                    else:
                        current_length = 0
                        
            current_chunk.append(sentence)
            current_length += len(sentence)
        
        # Process the last chunk
        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            if len(chunk_text) >= self.min_paragraph_length:
                paragraphs.append(chunk_text)
        
        return paragraphs


    def _process_file(self, file_path: str, spinner=None) -> List[Document]:
        """Process a single file"""
        try:
            # Calculate file MD5
            if spinner:
                spinner.text = f"è®¡ç®—æ–‡ä»¶ {file_path} çš„MD5..."
            current_md5 = get_file_md5(file_path)
            if not current_md5:
                if spinner:
                    spinner.text = f"æ–‡ä»¶ {file_path} è®¡ç®—MD5å¤±è´¥"
                return []

            # Check if the file needs to be reprocessed
            if file_path in self.file_md5_cache and self.file_md5_cache[file_path] == current_md5:
                if spinner:
                    spinner.text = f"æ–‡ä»¶ {file_path} æœªå‘ç”Ÿå˜åŒ–ï¼Œè·³è¿‡å¤„ç†"
                return []

            # Find the appropriate processor
            if spinner:
                spinner.text = f"æŸ¥æ‰¾é€‚ç”¨äº {file_path} çš„å¤„ç†å™¨..."
            processor = None
            for p in self.file_processors:
                if p.can_handle(file_path):
                    processor = p
                    break
                    
            if not processor:
                # If no appropriate processor is found, return an empty document
                if spinner:
                    spinner.text = f"æ²¡æœ‰æ‰¾åˆ°é€‚ç”¨äº {file_path} çš„å¤„ç†å™¨ï¼Œè·³è¿‡å¤„ç†"
                return []
            
            # Extract text content
            if spinner:
                spinner.text = f"æå– {file_path} çš„æ–‡æœ¬å†…å®¹..."
            content = processor.extract_text(file_path)
            if not content.strip():
                if spinner:
                    spinner.text = f"æ–‡ä»¶ {file_path} æ²¡æœ‰æ–‡æœ¬å†…å®¹ï¼Œè·³è¿‡å¤„ç†"
                return []
            
            # Split text
            if spinner:
                spinner.text = f"åˆ†å‰² {file_path} çš„æ–‡æœ¬..."
            chunks = self._split_text(content)
            
            # Create document objects
            if spinner:
                spinner.text = f"ä¸º {file_path} åˆ›å»º {len(chunks)} ä¸ªæ–‡æ¡£å¯¹è±¡..."
            documents = []
            for i, chunk in enumerate(chunks):
                doc = Document(
                    content=chunk,
                    metadata={
                        "file_path": file_path,
                        "file_type": Path(file_path).suffix.lower(),
                        "chunk_index": i,
                        "total_chunks": len(chunks)
                    },
                    md5=current_md5
                )
                documents.append(doc)
            
            # Update MD5 cache
            self.file_md5_cache[file_path] = current_md5
            if spinner:
                spinner.text = f"æ–‡ä»¶ {file_path} å¤„ç†å®Œæˆï¼Œå…±åˆ›å»º {len(documents)} ä¸ªæ–‡æ¡£å¯¹è±¡"
            return documents
            
        except Exception as e:
            if spinner:
                spinner.text = f"å¤„ç†æ–‡ä»¶å¤±è´¥: {file_path}: {str(e)}"
            PrettyOutput.print(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {file_path}: {str(e)}", 
                            output_type=OutputType.ERROR)
            return []

    def _should_ignore_path(self, path: str, ignored_paths: List[str]) -> bool:
        """
        æ£€æŸ¥è·¯å¾„æ˜¯å¦åº”è¯¥è¢«å¿½ç•¥
        
        Args:
            path: æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„
            ignored_paths: å¿½ç•¥æ¨¡å¼åˆ—è¡¨
            
        Returns:
            bool: å¦‚æœè·¯å¾„åº”è¯¥è¢«å¿½ç•¥åˆ™è¿”å›True
        """
        import fnmatch
        import os
        
        # è·å–ç›¸å¯¹è·¯å¾„
        rel_path = path
        if os.path.isabs(path):
            try:
                rel_path = os.path.relpath(path, self.root_dir)
            except ValueError:
                # å¦‚æœä¸èƒ½è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä½¿ç”¨åŸå§‹è·¯å¾„
                pass
                
        path_parts = rel_path.split(os.sep)
        
        # æ£€æŸ¥è·¯å¾„çš„æ¯ä¸€éƒ¨åˆ†æ˜¯å¦åŒ¹é…ä»»æ„å¿½ç•¥æ¨¡å¼
        for part in path_parts:
            for pattern in ignored_paths:
                if fnmatch.fnmatch(part, pattern):
                    return True
                    
        # æ£€æŸ¥å®Œæ•´è·¯å¾„æ˜¯å¦åŒ¹é…ä»»æ„å¿½ç•¥æ¨¡å¼
        for pattern in ignored_paths:
            if fnmatch.fnmatch(rel_path, pattern):
                return True
                
        return False
        
    def _is_git_repo(self) -> bool:
        """
        æ£€æŸ¥å½“å‰ç›®å½•æ˜¯å¦ä¸ºGitä»“åº“
        
        Returns:
            bool: å¦‚æœæ˜¯Gitä»“åº“åˆ™è¿”å›True
        """
        import subprocess
        
        try:
            result = subprocess.run(
                ["git", "rev-parse", "--is-inside-work-tree"],
                cwd=self.root_dir,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                check=False
            )
            return result.returncode == 0 and result.stdout.strip() == "true"
        except Exception:
            return False
    
    def _get_git_managed_files(self) -> List[str]:
        """
        è·å–Gitä»“åº“ä¸­è¢«ç®¡ç†çš„æ–‡ä»¶åˆ—è¡¨
        
        Returns:
            List[str]: è¢«Gitç®¡ç†çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
        """
        import subprocess
        
        try:
            # è·å–gitç´¢å¼•ä¸­çš„æ–‡ä»¶
            result = subprocess.run(
                ["git", "ls-files"],
                cwd=self.root_dir,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                check=False
            )
            
            if result.returncode != 0:
                return []
                
            git_files = [line.strip() for line in result.stdout.splitlines() if line.strip()]
            
            # æ·»åŠ æœªæš‚å­˜ä½†å·²è·Ÿè¸ªçš„ä¿®æ”¹æ–‡ä»¶
            result = subprocess.run(
                ["git", "ls-files", "--modified"],
                cwd=self.root_dir,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                check=False
            )
            
            if result.returncode == 0:
                modified_files = [line.strip() for line in result.stdout.splitlines() if line.strip()]
                git_files.extend([f for f in modified_files if f not in git_files])
            
            # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            return [os.path.join(self.root_dir, file) for file in git_files]
            
        except Exception as e:
            PrettyOutput.print(f"è·å–Gitç®¡ç†çš„æ–‡ä»¶å¤±è´¥: {str(e)}", output_type=OutputType.WARNING)
            return []

    def build_index(self, dir: str):
        try:
            """Build document index with optimized processing"""
            # Get all files
            with yaspin(text="è·å–æ‰€æœ‰æ–‡ä»¶...", color="cyan") as spinner:
                all_files = []
                
                # è·å–éœ€è¦å¿½ç•¥çš„è·¯å¾„åˆ—è¡¨
                ignored_paths = get_rag_ignored_paths()
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºGitä»“åº“
                is_git_repo = self._is_git_repo()
                if is_git_repo:
                    git_files = self._get_git_managed_files()
                    # è¿‡æ»¤æ‰è¢«å¿½ç•¥çš„æ–‡ä»¶
                    for file_path in git_files:
                        if self._should_ignore_path(file_path, ignored_paths):
                            continue
                            
                        if os.path.getsize(file_path) > 100 * 1024 * 1024:  # 100MB
                            PrettyOutput.print(f"è·³è¿‡å¤§æ–‡ä»¶: {file_path}", 
                                            output_type=OutputType.WARNING)
                            continue
                        all_files.append(file_path)
                else:
                    # éGitä»“åº“ï¼Œä½¿ç”¨å¸¸è§„æ–‡ä»¶éå†
                    for root, _, files in os.walk(dir):
                        # æ£€æŸ¥ç›®å½•æ˜¯å¦åŒ¹é…å¿½ç•¥æ¨¡å¼
                        if self._should_ignore_path(root, ignored_paths):
                            continue
                            
                        for file in files:
                            file_path = os.path.join(root, file)
                            
                            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åŒ¹é…å¿½ç•¥æ¨¡å¼
                            if self._should_ignore_path(file_path, ignored_paths):
                                continue
                                
                            if os.path.getsize(file_path) > 100 * 1024 * 1024:  # 100MB
                                PrettyOutput.print(f"è·³è¿‡å¤§æ–‡ä»¶: {file_path}", 
                                                output_type=OutputType.WARNING)
                                continue
                            all_files.append(file_path)
                            
                spinner.text = f"è·å–æ‰€æœ‰æ–‡ä»¶å®Œæˆï¼Œå…± {len(all_files)} ä¸ªæ–‡ä»¶"
                spinner.ok("âœ…")

            # Clean up cache for deleted files
            with yaspin(text="æ¸…ç†ç¼“å­˜...", color="cyan") as spinner:
                deleted_files = set(self.file_md5_cache.keys()) - set(all_files)
                deleted_count = len(deleted_files)
                
                if deleted_count > 0:
                    spinner.write(f"ğŸ—‘ï¸ åˆ é™¤ä¸å­˜åœ¨æ–‡ä»¶çš„ç¼“å­˜: {deleted_count} ä¸ª")
                    
                for file_path in deleted_files:
                    # Remove from MD5 cache
                    del self.file_md5_cache[file_path]
                    # Remove related documents
                    self.documents = [doc for doc in self.documents if doc.metadata['file_path'] != file_path]
                    # Delete cache files
                    self._delete_file_cache(file_path, None)  # Pass None as spinner to not show individual deletions
                    
                spinner.text = f"æ¸…ç†ç¼“å­˜å®Œæˆï¼Œå…±åˆ é™¤ {deleted_count} ä¸ªä¸å­˜åœ¨æ–‡ä»¶çš„ç¼“å­˜"
                spinner.ok("âœ…")

            # Check file changes
            with yaspin(text="æ£€æŸ¥æ–‡ä»¶å˜åŒ–...", color="cyan") as spinner:
                files_to_process = []
                unchanged_files = []
                new_files_count = 0
                modified_files_count = 0
                
                for file_path in all_files:
                    current_md5 = get_file_md5(file_path)
                    if current_md5:  # Only process files that can successfully calculate MD5
                        if file_path in self.file_md5_cache and self.file_md5_cache[file_path] == current_md5:
                            # Fileæœªå˜åŒ–ï¼Œè®°å½•ä½†ä¸é‡æ–°å¤„ç†
                            unchanged_files.append(file_path)
                        else:
                            # New file or modified file
                            files_to_process.append(file_path)
                            
                            # å¦‚æœæ˜¯ä¿®æ”¹çš„æ–‡ä»¶ï¼Œåˆ é™¤æ—§ç¼“å­˜
                            if file_path in self.file_md5_cache:
                                modified_files_count += 1
                                # åˆ é™¤æ—§ç¼“å­˜
                                self._delete_file_cache(file_path, spinner)
                                # ä»æ–‡æ¡£åˆ—è¡¨ä¸­ç§»é™¤
                                self.documents = [doc for doc in self.documents if doc.metadata['file_path'] != file_path]
                            else:
                                new_files_count += 1
                
                # è¾“å‡ºæ±‡æ€»ä¿¡æ¯
                if unchanged_files:
                    spinner.write(f"ğŸ“š å·²ç¼“å­˜æ–‡ä»¶: {len(unchanged_files)} ä¸ª")
                if new_files_count > 0:
                    spinner.write(f"ğŸ†• æ–°å¢æ–‡ä»¶: {new_files_count} ä¸ª")
                if modified_files_count > 0:
                    spinner.write(f"ğŸ“ ä¿®æ”¹æ–‡ä»¶: {modified_files_count} ä¸ª")
                    
                spinner.text = f"æ£€æŸ¥æ–‡ä»¶å˜åŒ–å®Œæˆï¼Œå…± {len(files_to_process)} ä¸ªæ–‡ä»¶éœ€è¦å¤„ç†"
                spinner.ok("âœ…")

            # Keep documents for unchanged files
            unchanged_documents = [doc for doc in self.documents 
                                if doc.metadata['file_path'] in unchanged_files]

            # Process files one by one with optimized vectorization
            if files_to_process:
                new_documents = []
                new_vectors = []
                success_count = 0
                skipped_count = 0
                failed_count = 0
                
                with yaspin(text=f"å¤„ç†æ–‡ä»¶ä¸­ (0/{len(files_to_process)})...", color="cyan") as spinner:
                    for index, file_path in enumerate(files_to_process):
                        spinner.text = f"å¤„ç†æ–‡ä»¶ä¸­ ({index+1}/{len(files_to_process)}): {file_path}"
                        try:
                            # Process single file
                            file_docs = self._process_file(file_path, spinner)
                            if file_docs:
                                # Vectorize documents from this file
                                spinner.text = f"å¤„ç†æ–‡ä»¶ä¸­ ({index+1}/{len(files_to_process)}): ä¸º {file_path} ç”Ÿæˆå‘é‡åµŒå…¥..."
                                texts_to_vectorize = [
                                    f"File:{doc.metadata['file_path']} Content:{doc.content}"
                                    for doc in file_docs
                                ]
                                
                                file_vectors = get_embedding_batch(self.embedding_model, f"({index+1}/{len(files_to_process)}){file_path}", texts_to_vectorize, spinner)
                                
                                # Save cache for this file
                                spinner.text = f"å¤„ç†æ–‡ä»¶ä¸­ ({index+1}/{len(files_to_process)}): ä¿å­˜ {file_path} çš„ç¼“å­˜..."
                                self._save_cache(file_path, file_docs, file_vectors, spinner)
                                
                                # Accumulate documents and vectors
                                new_documents.extend(file_docs)
                                new_vectors.append(file_vectors)
                                success_count += 1
                            else:
                                # æ–‡ä»¶è·³è¿‡å¤„ç†
                                skipped_count += 1
                                
                        except Exception as e:
                            spinner.write(f"âŒ å¤„ç†å¤±è´¥: {file_path}: {str(e)}")
                            failed_count += 1
                    
                    # è¾“å‡ºå¤„ç†ç»Ÿè®¡
                    spinner.text = f"æ–‡ä»¶å¤„ç†å®Œæˆ: æˆåŠŸ {success_count} ä¸ª, è·³è¿‡ {skipped_count} ä¸ª, å¤±è´¥ {failed_count} ä¸ª"
                    spinner.ok("âœ…")
                    
                # Update documents list
                self.documents.extend(new_documents)

                # Build final index
                if new_vectors:
                    with yaspin(text="æ„å»ºæœ€ç»ˆç´¢å¼•...", color="cyan") as spinner:
                        spinner.text = "åˆå¹¶æ–°å‘é‡..."
                        all_new_vectors = np.vstack(new_vectors)
                        
                        unchanged_vector_count = 0
                        if self.flat_index is not None:
                            # Get vectors for unchanged documents
                            spinner.text = "è·å–æœªå˜åŒ–æ–‡æ¡£çš„å‘é‡..."
                            unchanged_vectors = self._get_unchanged_vectors(unchanged_documents, spinner)
                            if unchanged_vectors is not None:
                                unchanged_vector_count = unchanged_vectors.shape[0]
                                spinner.text = f"åˆå¹¶æ–°æ—§å‘é‡ï¼ˆæ–°ï¼š{all_new_vectors.shape[0]}ï¼Œæ—§ï¼š{unchanged_vector_count}ï¼‰..."
                                final_vectors = np.vstack([unchanged_vectors, all_new_vectors])
                            else:
                                spinner.text = f"ä»…ä½¿ç”¨æ–°å‘é‡ï¼ˆ{all_new_vectors.shape[0]}ï¼‰..."
                                final_vectors = all_new_vectors
                        else:
                            spinner.text = f"ä»…ä½¿ç”¨æ–°å‘é‡ï¼ˆ{all_new_vectors.shape[0]}ï¼‰..."
                            final_vectors = all_new_vectors

                        # Build index
                        spinner.text = f"æ„å»ºç´¢å¼•ï¼ˆå‘é‡æ•°é‡ï¼š{final_vectors.shape[0]}ï¼‰..."
                        self._build_index(final_vectors, spinner)
                        spinner.text = f"ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(self.documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ"
                        spinner.ok("âœ…")

                # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
                PrettyOutput.print(
                    f"ğŸ“Š ç´¢å¼•ç»Ÿè®¡:\n"
                    f"  â€¢ æ€»æ–‡æ¡£æ•°: {len(self.documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ\n"
                    f"  â€¢ å·²ç¼“å­˜æ–‡ä»¶: {len(unchanged_files)} ä¸ª\n"
                    f"  â€¢ å¤„ç†æ–‡ä»¶: {len(files_to_process)} ä¸ª\n"
                    f"    - æˆåŠŸ: {success_count} ä¸ª\n"
                    f"    - è·³è¿‡: {skipped_count} ä¸ª\n"
                    f"    - å¤±è´¥: {failed_count} ä¸ª", 
                    OutputType.SUCCESS
                )
        except Exception as e:
            PrettyOutput.print(f"ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}", 
                            output_type=OutputType.ERROR)

    def _get_unchanged_vectors(self, unchanged_documents: List[Document], spinner=None) -> Optional[np.ndarray]:
        """Get vectors for unchanged documents from existing index"""
        try:
            if not unchanged_documents:
                if spinner:
                    spinner.text = "æ²¡æœ‰æœªå˜åŒ–çš„æ–‡æ¡£"
                return None

            if spinner:
                spinner.text = f"åŠ è½½ {len(unchanged_documents)} ä¸ªæœªå˜åŒ–æ–‡æ¡£çš„å‘é‡..."
            
            # æŒ‰æ–‡ä»¶åˆ†ç»„å¤„ç†
            unchanged_files = set(doc.metadata['file_path'] for doc in unchanged_documents)
            unchanged_vectors = []
            
            for file_path in unchanged_files:
                if spinner:
                    spinner.text = f"åŠ è½½ {file_path} çš„å‘é‡..."
                
                # è·å–è¯¥æ–‡ä»¶æ‰€æœ‰æ–‡æ¡£çš„chunkç´¢å¼•
                doc_indices = [(i, doc.metadata['chunk_index']) 
                              for i, doc in enumerate(unchanged_documents) 
                              if doc.metadata['file_path'] == file_path]
                
                if not doc_indices:
                    continue
                
                # åŠ è½½è¯¥æ–‡ä»¶çš„å‘é‡
                vec_cache_path = self._get_cache_path(file_path, "vec")
                if os.path.exists(vec_cache_path):
                    try:
                        with lzma.open(vec_cache_path, 'rb') as f:
                            vec_cache_data = pickle.load(f)
                            file_vectors = vec_cache_data["vectors"]
                        
                        # æŒ‰ç…§chunk_indexåŠ è½½å¯¹åº”çš„å‘é‡
                        for _, chunk_idx in doc_indices:
                            if chunk_idx < file_vectors.shape[0]:
                                unchanged_vectors.append(file_vectors[chunk_idx].reshape(1, -1))
                            
                        if spinner:
                            spinner.text = f"æˆåŠŸåŠ è½½ {file_path} çš„å‘é‡"
                    except Exception as e:
                        if spinner:
                            spinner.text = f"åŠ è½½ {file_path} å‘é‡å¤±è´¥: {str(e)}"
                else:
                    if spinner:
                        spinner.text = f"æœªæ‰¾åˆ° {file_path} çš„å‘é‡ç¼“å­˜"
                        
                    # ä»flat_indexé‡å»ºå‘é‡
                    if self.flat_index is not None:
                        if spinner:
                            spinner.text = f"ä»ç´¢å¼•é‡å»º {file_path} çš„å‘é‡..."
                        
                        for doc_idx, chunk_idx in doc_indices:
                            idx = next((i for i, d in enumerate(self.documents) 
                                     if d.metadata['file_path'] == file_path and 
                                     d.metadata['chunk_index'] == chunk_idx), None)
                            
                            if idx is not None:
                                vector = np.zeros((1, self.vector_dim), dtype=np.float32) # type: ignore
                                self.flat_index.reconstruct(idx, vector.ravel())
                                unchanged_vectors.append(vector)

            if not unchanged_vectors:
                if spinner:
                    spinner.text = "æœªèƒ½åŠ è½½ä»»ä½•æœªå˜åŒ–æ–‡æ¡£çš„å‘é‡"
                return None
                
            if spinner:
                spinner.text = f"æœªå˜åŒ–æ–‡æ¡£å‘é‡åŠ è½½å®Œæˆï¼Œå…± {len(unchanged_vectors)} ä¸ª"
                
            return np.vstack(unchanged_vectors)
            
        except Exception as e:
            if spinner:
                spinner.text = f"è·å–ä¸å˜å‘é‡å¤±è´¥: {str(e)}"
            PrettyOutput.print(f"è·å–ä¸å˜å‘é‡å¤±è´¥: {str(e)}", OutputType.ERROR)
            return None

    def search(self, query: str, top_k: int = 30) -> List[Tuple[Document, float]]:
        """Search documents with context window"""
        if not self.index:
            self.build_index(self.root_dir)
            
        # å¦‚æœç´¢å¼•å»ºç«‹å¤±è´¥æˆ–æ–‡æ¡£åˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ
        if not self.index or len(self.documents) == 0:
            PrettyOutput.print("ç´¢å¼•æœªå»ºç«‹æˆ–æ–‡æ¡£åˆ—è¡¨ä¸ºç©º", OutputType.WARNING)
            return []
            
        # Get query vector
        with yaspin(text="è·å–æŸ¥è¯¢å‘é‡...", color="cyan") as spinner:
            query_vector = get_embedding(self.embedding_model, query)
            query_vector = query_vector.reshape(1, -1)
            spinner.text = "æŸ¥è¯¢å‘é‡è·å–å®Œæˆ"
            spinner.ok("âœ…")
        
        # Search with more candidates
        with yaspin(text="æœç´¢...", color="cyan") as spinner:
            initial_k = min(top_k * 4, len(self.documents))
            if initial_k == 0:
                spinner.text = "æ–‡æ¡£ä¸ºç©ºï¼Œæœç´¢ç»ˆæ­¢"
                spinner.fail("âŒ")
                return []
                
            distances, indices = self.index.search(query_vector, initial_k) # type: ignore
            spinner.text = "æœç´¢å®Œæˆ"
            spinner.ok("âœ…")
        
        # Process results with context window
        with yaspin(text="å¤„ç†ç»“æœ...", color="cyan") as spinner:
            results = []
            seen_files = set()
            
            # æ£€æŸ¥ç´¢å¼•æ•°ç»„æ˜¯å¦ä¸ºç©º
            if indices.size == 0 or indices[0].size == 0:
                spinner.text = "æœç´¢ç»“æœä¸ºç©º"
                spinner.fail("âŒ")
                return []
                
            for idx, dist in zip(indices[0], distances[0]):
                if idx != -1 and idx < len(self.documents):  # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                    doc = self.documents[idx]
                    similarity = 1.0 / (1.0 + float(dist))
                    if similarity > 0.3:
                        file_path = doc.metadata['file_path']
                        if file_path not in seen_files:
                            seen_files.add(file_path)
                            
                            # Get full context from original document
                            original_doc = next((d for d in self.documents 
                                            if d.metadata['file_path'] == file_path), None)
                            if original_doc:
                                window_docs = []  # Add this line to initialize the list
                                full_content = original_doc.content
                                # Find all chunks from this file
                                file_chunks = [d for d in self.documents 
                                            if d.metadata['file_path'] == file_path]
                                # Add all related chunks
                                for chunk_doc in file_chunks:
                                    window_docs.append((chunk_doc, similarity * 0.9))
                            
                            results.extend(window_docs)
                            if len(results) >= top_k * (2 * self.context_window + 1):
                                break
            spinner.text = "å¤„ç†ç»“æœå®Œæˆ"
            spinner.ok("âœ…")
        
        # Sort by similarity and deduplicate
        with yaspin(text="æ’åº...", color="cyan") as spinner:
            if not results:
                spinner.text = "æ— æœ‰æ•ˆç»“æœ"
                spinner.fail("âŒ")
                return []
                
            results.sort(key=lambda x: x[1], reverse=True)
            seen = set()
            final_results = []
            for doc, score in results:
                key = (doc.metadata['file_path'], doc.metadata['chunk_index'])
                if key not in seen:
                    seen.add(key)
                    final_results.append((doc, score))
                    if len(final_results) >= top_k:
                        break
            spinner.text = "æ’åºå®Œæˆ"
            spinner.ok("âœ…")
                    
        return final_results

    def query(self, query: str) -> List[Document]:
        """Query related documents
        
        Args:
            query: Query text
            
        Returns:
            List[Document]: Related documents
        """
        results = self.search(query)
        return [doc for doc, _ in results]

    def ask(self, question: str) -> Optional[str]:
        """Ask questions about documents with enhanced context building"""
        try:
            results = self.search(question)
            if not results:
                return None
            
            prompt = f"""
# ğŸ¤– è§’è‰²å®šä¹‰
æ‚¨æ˜¯ä¸€ä½æ–‡æ¡£åˆ†æä¸“å®¶ï¼Œèƒ½å¤ŸåŸºäºæä¾›çš„æ–‡æ¡£æä¾›å‡†ç¡®ä¸”å…¨é¢çš„å›ç­”ã€‚

# ğŸ¯ æ ¸å¿ƒèŒè´£
- å…¨é¢åˆ†ææ–‡æ¡£ç‰‡æ®µ
- å‡†ç¡®å›ç­”é—®é¢˜
- å¼•ç”¨æºæ–‡æ¡£
- è¯†åˆ«ç¼ºå¤±ä¿¡æ¯
- ä¿æŒä¸“ä¸šè¯­æ°”

# ğŸ“‹ å›ç­”è¦æ±‚
## å†…å®¹è´¨é‡
- ä¸¥æ ¼åŸºäºæä¾›çš„æ–‡æ¡£ä½œç­”
- å…·ä½“ä¸”ç²¾ç¡®
- åœ¨æœ‰å¸®åŠ©æ—¶å¼•ç”¨ç›¸å…³å†…å®¹
- æŒ‡å‡ºä»»ä½•ä¿¡æ¯ç¼ºå£
- ä½¿ç”¨ä¸“ä¸šè¯­è¨€

## å›ç­”ç»“æ„
1. ç›´æ¥å›ç­”
   - æ¸…æ™°ç®€æ´çš„å›åº”
   - åŸºäºæ–‡æ¡£è¯æ®
   - ä¸“ä¸šæœ¯è¯­

2. æ”¯æŒç»†èŠ‚
   - ç›¸å…³æ–‡æ¡£å¼•ç”¨
   - æ–‡ä»¶å‚è€ƒ
   - ä¸Šä¸‹æ–‡è§£é‡Š

3. ä¿¡æ¯ç¼ºå£ï¼ˆå¦‚æœ‰ï¼‰
   - ç¼ºå¤±ä¿¡æ¯
   - éœ€è¦çš„é¢å¤–ä¸Šä¸‹æ–‡
   - æ½œåœ¨é™åˆ¶

# ğŸ” åˆ†æä¸Šä¸‹æ–‡
é—®é¢˜: {question}

ç›¸å…³æ–‡æ¡£ï¼ˆæŒ‰ç›¸å…³æ€§æ’åºï¼‰ï¼š
"""

            # Add context with length control
            with yaspin(text="æ·»åŠ ä¸Šä¸‹æ–‡...", color="cyan") as spinner:
                available_count = self.max_token_count - get_context_token_count(prompt) - 1000
                current_count = 0
                
                for doc, score in results:
                    doc_content = f"""
    ## æ–‡æ¡£ç‰‡æ®µ [ç›¸å…³åº¦: {score:.3f}]
    æ¥æº: {doc.metadata['file_path']}
    ```
    {doc.content}
    ```
    ---
    """
                    if current_count + get_context_token_count(doc_content) > available_count:
                        PrettyOutput.print(
                            "ç”±äºä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼Œéƒ¨åˆ†å†…å®¹è¢«çœç•¥",
                            output_type=OutputType.WARNING
                        )
                        break
                        
                    prompt += doc_content
                    current_count += get_context_token_count(doc_content)

                prompt += """
    # â— é‡è¦è§„åˆ™
    1. ä»…ä½¿ç”¨æä¾›çš„æ–‡æ¡£
    2. ä¿æŒç²¾ç¡®å’Œå‡†ç¡®
    3. åœ¨ç›¸å…³æ—¶å¼•ç”¨æ¥æº
    4. æŒ‡å‡ºç¼ºå¤±çš„ä¿¡æ¯
    5. ä¿æŒä¸“ä¸šè¯­æ°”
    6. ä½¿ç”¨ç”¨æˆ·çš„è¯­è¨€å›ç­”
    """
                spinner.text = "æ·»åŠ ä¸Šä¸‹æ–‡å®Œæˆ"
                spinner.ok("âœ…")

            with yaspin(text="æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...", color="cyan") as spinner:
                model = PlatformRegistry.get_global_platform_registry().get_normal_platform()
                response = model.chat_until_success(prompt)
                spinner.text = "ç­”æ¡ˆç”Ÿæˆå®Œæˆ"
                spinner.ok("âœ…")
                return response
            
        except Exception as e:
            PrettyOutput.print(f"å›ç­”å¤±è´¥ï¼š{str(e)}", OutputType.ERROR)
            return None

    def is_index_built(self) -> bool:
        """Check if the index is built and valid
        
        Returns:
            bool: True if index is built and valid
        """
        return self.index is not None and len(self.documents) > 0

    def _delete_file_cache(self, file_path: str, spinner=None):
        """Delete cache files for a specific file
        
        Args:
            file_path: Path to the original file
            spinner: Optional spinner for progress information. If None, runs silently.
        """
        try:
            # Delete document cache
            doc_cache_path = self._get_cache_path(file_path, "doc")
            if os.path.exists(doc_cache_path):
                os.remove(doc_cache_path)
                if spinner is not None:
                    spinner.write(f"ğŸ—‘ï¸ åˆ é™¤æ–‡æ¡£ç¼“å­˜: {file_path}")
                    
            # Delete vector cache
            vec_cache_path = self._get_cache_path(file_path, "vec")
            if os.path.exists(vec_cache_path):
                os.remove(vec_cache_path)
                if spinner is not None:
                    spinner.write(f"ğŸ—‘ï¸ åˆ é™¤å‘é‡ç¼“å­˜: {file_path}")
                    
        except Exception as e:
            if spinner is not None:
                spinner.write(f"âŒ åˆ é™¤ç¼“å­˜å¤±è´¥: {file_path}: {str(e)}")
            PrettyOutput.print(f"åˆ é™¤ç¼“å­˜å¤±è´¥: {file_path}: {str(e)}", output_type=OutputType.ERROR)

def main():
    """Main function"""
    import argparse
    import sys
    
    # Set standard output encoding to UTF-8
    if sys.stdout.encoding != 'utf-8':
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')
    
    parser = argparse.ArgumentParser(description='Document retrieval and analysis tool')
    parser.add_argument('--dir', type=str, help='Directory to process')
    parser.add_argument('--build', action='store_true', help='Build document index')
    parser.add_argument('--search', type=str, help='Search document content')
    parser.add_argument('--ask', type=str, help='Ask about documents')
    args = parser.parse_args()

    try:
        current_dir = os.getcwd()
        rag = RAGTool(current_dir)

        if not args.dir:
            args.dir = current_dir

        if args.dir and args.build:
            rag.build_index(args.dir)
            return 0

        if args.search or args.ask:

            if args.search:
                results = rag.query(args.search)
                if not results:
                    PrettyOutput.print("æœªæ‰¾åˆ°ç›¸å…³å†…å®¹", output_type=OutputType.WARNING)
                    return 1
                    
                for doc in results:
                    output = f"""æ–‡ä»¶: {doc.metadata['file_path']}\n"""
                    output += f"""ç‰‡æ®µ {doc.metadata['chunk_index'] + 1}/{doc.metadata['total_chunks']}\n"""
                    output += f"""å†…å®¹:\n{doc.content}\n"""
                    PrettyOutput.print(output, output_type=OutputType.INFO, lang="markdown")
                return 0

            if args.ask:
                # Call ask method
                response = rag.ask(args.ask)
                if not response:
                    PrettyOutput.print("è·å–ç­”æ¡ˆå¤±è´¥", output_type=OutputType.WARNING)
                    return 1
                    
                # Display answer
                output = f"""{response}"""
                PrettyOutput.print(output, output_type=OutputType.INFO)
                return 0

        PrettyOutput.print("è¯·æŒ‡å®šæ“ä½œå‚æ•°ã€‚ä½¿ç”¨ -h æŸ¥çœ‹å¸®åŠ©ã€‚", output_type=OutputType.WARNING)
        return 1

    except Exception as e:
        PrettyOutput.print(f"æ‰§è¡Œå¤±è´¥: {str(e)}", output_type=OutputType.ERROR)
        return 1

if __name__ == "__main__":
    main()

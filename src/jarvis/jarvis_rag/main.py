import os
import re
import numpy as np
import faiss
import torch  # Add torch import
from typing import List, Tuple, Optional, Dict
import pickle
from dataclasses import dataclass
from pathlib import Path
import contextlib  # æ·»åŠ  contextlib å¯¼å…¥

from yaspin import yaspin
from jarvis.jarvis_platform.registry import PlatformRegistry
import lzma  # æ·»åŠ  lzma å¯¼å…¥
from threading import Lock
import hashlib

from jarvis.jarvis_utils.config import get_max_paragraph_length, get_max_token_count, get_min_paragraph_length, get_thread_count, get_rag_ignored_paths
from jarvis.jarvis_utils.embedding import get_context_token_count, get_embedding, get_embedding_batch, load_embedding_model
from jarvis.jarvis_utils.output import OutputType, PrettyOutput
from jarvis.jarvis_utils.utils import  ct, get_file_md5, init_env, init_gpu_config, ot

from .file_processors import TextFileProcessor, PDFProcessor, DocxProcessor, PPTProcessor, ExcelProcessor

@dataclass
class Document:
    """Document class, for storing document content and metadata"""
    content: str  # Document content
    metadata: Dict  # Metadata (file path, position, etc.)
    md5: str = ""  # File MD5 value, for incremental update detection



class RAGTool:
    def __init__(self, root_dir: str):
        """Initialize RAG tool
        
        Args:
            root_dir: Project root directory
        """
        with yaspin(text="åˆå§‹åŒ–ç¯å¢ƒ...", color="cyan") as spinner:
            init_env()
            self.root_dir = root_dir
            os.chdir(self.root_dir)
            spinner.text = "ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ"
            spinner.ok("âœ…")
        
        # Initialize configuration
        with yaspin(text="åˆå§‹åŒ–é…ç½®...", color="cyan") as spinner:
            self.min_paragraph_length = get_min_paragraph_length()  # Minimum paragraph length
            self.max_paragraph_length = get_max_paragraph_length()  # Maximum paragraph length
            self.context_window = 5  # Fixed context window size
            self.max_token_count = int(get_max_token_count() * 0.8)
            spinner.text = "é…ç½®åˆå§‹åŒ–å®Œæˆ"
            spinner.ok("âœ…")
        
        # Initialize data directory
        with yaspin(text="åˆå§‹åŒ–æ•°æ®ç›®å½•...", color="cyan") as spinner:
            self.data_dir = os.path.join(self.root_dir, ".jarvis/rag")
            if not os.path.exists(self.data_dir):
                os.makedirs(self.data_dir)
            spinner.text = "æ•°æ®ç›®å½•åˆå§‹åŒ–å®Œæˆ"
            spinner.ok("âœ…")
            
        # Initialize embedding model
        with yaspin(text="åˆå§‹åŒ–æ¨¡å‹...", color="cyan") as spinner:
            try:
                self.embedding_model = load_embedding_model()
                self.vector_dim = self.embedding_model.get_sentence_embedding_dimension()
                spinner.text = "æ¨¡å‹åŠ è½½å®Œæˆ"
                spinner.ok("âœ…")
            except Exception as e:
                spinner.text = "æ¨¡å‹åŠ è½½å¤±è´¥"
                spinner.fail("âŒ")
                raise

        with yaspin(text="åˆå§‹åŒ–ç¼“å­˜ç›®å½•...", color="cyan") as spinner:
            self.cache_dir = os.path.join(self.data_dir, "cache")
            if not os.path.exists(self.cache_dir):
                os.makedirs(self.cache_dir)
                
            self.documents: List[Document] = []
            self.index = None
            self.flat_index = None
            self.file_md5_cache = {}
            spinner.text = "ç¼“å­˜ç›®å½•åˆå§‹åŒ–å®Œæˆ"
            spinner.ok("âœ…")
        
        # åŠ è½½ç¼“å­˜ç´¢å¼•
        self._load_cache_index()

        # Register file processors
        with yaspin(text="åˆå§‹åŒ–æ–‡ä»¶å¤„ç†å™¨...", color="cyan") as spinner:
            self.file_processors = [
                TextFileProcessor(),
                PDFProcessor(),
                DocxProcessor(),
                PPTProcessor(),
                ExcelProcessor()
            ]
            spinner.text = "æ–‡ä»¶å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ"
            spinner.ok("âœ…")


        # Add thread related configuration
        with yaspin(text="åˆå§‹åŒ–çº¿ç¨‹é…ç½®...", color="cyan") as spinner:
            self.thread_count = get_thread_count()
            self.vector_lock = Lock()  # Protect vector list concurrency
            spinner.text = "çº¿ç¨‹é…ç½®åˆå§‹åŒ–å®Œæˆ"
            spinner.ok("âœ…")

        # åˆå§‹åŒ– GPU å†…å­˜é…ç½®
        with yaspin(text="åˆå§‹åŒ– GPU å†…å­˜é…ç½®...", color="cyan") as spinner:
            with spinner.hidden():
                self.gpu_config = init_gpu_config()
            spinner.text = "GPU å†…å­˜é…ç½®åˆå§‹åŒ–å®Œæˆ"
            spinner.ok("âœ…")


    def _get_cache_path(self, file_path: str, cache_type: str = "doc") -> str:
        """Get cache file path for a document
        
        Args:
            file_path: Original file path
            cache_type: Type of cache ("doc" for documents, "vec" for vectors)
            
        Returns:
            str: Cache file path
        """
        # ä½¿ç”¨æ–‡ä»¶è·¯å¾„çš„å“ˆå¸Œä½œä¸ºç¼“å­˜æ–‡ä»¶å
        file_hash = hashlib.md5(file_path.encode()).hexdigest()
        
        # ç¡®ä¿ä¸åŒç±»å‹çš„ç¼“å­˜æœ‰ä¸åŒçš„ç›®å½•
        if cache_type == "doc":
            cache_subdir = os.path.join(self.cache_dir, "documents")
        elif cache_type == "vec":
            cache_subdir = os.path.join(self.cache_dir, "vectors")
        else:
            cache_subdir = self.cache_dir
            
        # ç¡®ä¿å­ç›®å½•å­˜åœ¨
        if not os.path.exists(cache_subdir):
            os.makedirs(cache_subdir)
            
        return os.path.join(cache_subdir, f"{file_hash}.cache")

    def _load_cache_index(self):
        """Load cache index"""
        index_path = os.path.join(self.data_dir, "index.pkl")
        if os.path.exists(index_path):
            try:
                with yaspin(text="åŠ è½½ç¼“å­˜ç´¢å¼•...", color="cyan") as spinner:
                    with lzma.open(index_path, 'rb') as f:
                        cache_data = pickle.load(f)
                        self.file_md5_cache = cache_data.get("file_md5_cache", {})
                    spinner.text = "ç¼“å­˜ç´¢å¼•åŠ è½½å®Œæˆ"
                    spinner.ok("âœ…")
                        
                # ä»å„ä¸ªç¼“å­˜æ–‡ä»¶åŠ è½½æ–‡æ¡£
                with yaspin(text="åŠ è½½ç¼“å­˜æ–‡ä»¶...", color="cyan") as spinner:
                    for file_path in self.file_md5_cache:
                        doc_cache_path = self._get_cache_path(file_path, "doc")
                        if os.path.exists(doc_cache_path):
                            try:
                                with lzma.open(doc_cache_path, 'rb') as f:
                                    doc_cache_data = pickle.load(f)
                                    self.documents.extend(doc_cache_data["documents"])
                                spinner.text = f"åŠ è½½æ–‡æ¡£ç¼“å­˜: {file_path}"
                            except Exception as e:
                                spinner.write(f"âŒ åŠ è½½æ–‡æ¡£ç¼“å­˜å¤±è´¥: {file_path}: {str(e)}")
                    spinner.text = "æ–‡æ¡£ç¼“å­˜åŠ è½½å®Œæˆ"
                    spinner.ok("âœ…")
                
                # é‡å»ºå‘é‡ç´¢å¼•
                if self.documents:
                    with yaspin(text="é‡å»ºå‘é‡ç´¢å¼•...", color="cyan") as spinner:
                        vectors = []
                        
                        # æŒ‰ç…§æ–‡æ¡£åˆ—è¡¨é¡ºåºåŠ è½½å‘é‡
                        processed_files = set()
                        for doc in self.documents:
                            file_path = doc.metadata['file_path']
                            
                            # é¿å…é‡å¤å¤„ç†åŒä¸€ä¸ªæ–‡ä»¶
                            if file_path in processed_files:
                                continue
                                
                            processed_files.add(file_path)
                            vec_cache_path = self._get_cache_path(file_path, "vec")
                            
                            if os.path.exists(vec_cache_path):
                                try:
                                    # åŠ è½½è¯¥æ–‡ä»¶çš„å‘é‡ç¼“å­˜
                                    with lzma.open(vec_cache_path, 'rb') as f:
                                        vec_cache_data = pickle.load(f)
                                        file_vectors = vec_cache_data["vectors"]
                                    
                                    # æŒ‰ç…§æ–‡æ¡£çš„chunk_indexæ£€ç´¢å¯¹åº”å‘é‡
                                    doc_indices = [d.metadata['chunk_index'] for d in self.documents 
                                                if d.metadata['file_path'] == file_path]
                                    
                                    # æ£€æŸ¥å‘é‡æ•°é‡ä¸æ–‡æ¡£å—æ•°é‡æ˜¯å¦åŒ¹é…
                                    if len(doc_indices) <= file_vectors.shape[0]:
                                        for idx in doc_indices:
                                            if idx < file_vectors.shape[0]:
                                                vectors.append(file_vectors[idx].reshape(1, -1))
                                    else:
                                        spinner.write(f"âš ï¸ å‘é‡ç¼“å­˜ä¸åŒ¹é…: {file_path}")
                                        
                                    spinner.text = f"åŠ è½½å‘é‡ç¼“å­˜: {file_path}"
                                except Exception as e:
                                    spinner.write(f"âŒ åŠ è½½å‘é‡ç¼“å­˜å¤±è´¥: {file_path}: {str(e)}")
                            else:
                                spinner.write(f"âš ï¸ ç¼ºå°‘å‘é‡ç¼“å­˜: {file_path}")
                        
                        if vectors:
                            vectors = np.vstack(vectors)
                            self._build_index(vectors, spinner)
                        spinner.text = f"å‘é‡ç´¢å¼•é‡å»ºå®Œæˆï¼ŒåŠ è½½ {len(self.documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ"
                        spinner.ok("âœ…")
                                
            except Exception as e:
                PrettyOutput.print(f"åŠ è½½ç¼“å­˜ç´¢å¼•å¤±è´¥: {str(e)}", 
                                output_type=OutputType.WARNING)
                self.documents = []
                self.index = None
                self.flat_index = None
                self.file_md5_cache = {}

    def _save_cache(self, file_path: str, documents: List[Document], vectors: np.ndarray, spinner=None):
        """Save cache for a single file
        
        Args:
            file_path: File path
            documents: List of documents
            vectors: Document vectors
            spinner: Optional spinner for progress display
        """
        try:
            # ä¿å­˜æ–‡æ¡£ç¼“å­˜
            if spinner:
                spinner.text = f"ä¿å­˜ {file_path} çš„æ–‡æ¡£ç¼“å­˜..."
            doc_cache_path = self._get_cache_path(file_path, "doc")
            doc_cache_data = {
                "documents": documents
            }
            with lzma.open(doc_cache_path, 'wb') as f:
                pickle.dump(doc_cache_data, f)
                
            # ä¿å­˜å‘é‡ç¼“å­˜
            if spinner:
                spinner.text = f"ä¿å­˜ {file_path} çš„å‘é‡ç¼“å­˜..."
            vec_cache_path = self._get_cache_path(file_path, "vec")
            vec_cache_data = {
                "vectors": vectors
            }
            with lzma.open(vec_cache_path, 'wb') as f:
                pickle.dump(vec_cache_data, f)
                
            # æ›´æ–°å¹¶ä¿å­˜ç´¢å¼•
            if spinner:
                spinner.text = f"æ›´æ–° {file_path} çš„ç´¢å¼•ç¼“å­˜..."
            index_path = os.path.join(self.data_dir, "index.pkl")
            index_data = {
                "file_md5_cache": self.file_md5_cache
            }
            with lzma.open(index_path, 'wb') as f:
                pickle.dump(index_data, f)
            
            if spinner:
                spinner.text = f"{file_path} çš„ç¼“å­˜ä¿å­˜å®Œæˆ"
                            
        except Exception as e:
            if spinner:
                spinner.text = f"ä¿å­˜ {file_path} çš„ç¼“å­˜å¤±è´¥: {str(e)}"
            PrettyOutput.print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {str(e)}", output_type=OutputType.ERROR)

    def _build_index(self, vectors: np.ndarray, spinner=None):
        """Build FAISS index"""
        if vectors.shape[0] == 0:
            if spinner:
                spinner.text = "å‘é‡ä¸ºç©ºï¼Œè·³è¿‡ç´¢å¼•æ„å»º"
            self.index = None
            self.flat_index = None
            return
            
        # Create a flat index to store original vectors, for reconstruction
        if spinner:
            spinner.text = "åˆ›å»ºå¹³é¢ç´¢å¼•ç”¨äºå‘é‡é‡å»º..."
        self.flat_index = faiss.IndexFlatIP(self.vector_dim)
        self.flat_index.add(vectors) # type: ignore
        
        # Create an IVF index for fast search
        if spinner:
            spinner.text = "åˆ›å»ºIVFç´¢å¼•ç”¨äºå¿«é€Ÿæœç´¢..."
        # ä¿®æ”¹èšç±»ä¸­å¿ƒçš„è®¡ç®—æ–¹å¼ï¼Œå°æ•°æ®é‡æ—¶ä½¿ç”¨æ›´å°‘çš„èšç±»ä¸­å¿ƒ
        # é¿å…"WARNING clustering X points to Y centroids: please provide at least Z training points"è­¦å‘Š
        num_vectors = vectors.shape[0]
        if num_vectors < 100:
            # å¯¹äºå°äº100ä¸ªå‘é‡çš„æƒ…å†µï¼Œä½¿ç”¨æ›´å°‘çš„èšç±»ä¸­å¿ƒ
            nlist = 1  # åªç”¨1ä¸ªèšç±»ä¸­å¿ƒ
        elif num_vectors < 1000:
            # å¯¹äº100-1000ä¸ªå‘é‡çš„æƒ…å†µï¼Œä½¿ç”¨è¾ƒå°‘çš„èšç±»ä¸­å¿ƒ
            nlist = max(1, int(num_vectors / 100))  # æ¯100ä¸ªå‘é‡ä¸€ä¸ªèšç±»ä¸­å¿ƒ
        else:
            # åŸå§‹é€»è¾‘ï¼šæ¯1000ä¸ªå‘é‡ä¸€ä¸ªèšç±»ä¸­å¿ƒï¼Œæœ€å°‘4ä¸ª
            nlist = max(4, int(num_vectors / 1000))
            
        quantizer = faiss.IndexFlatIP(self.vector_dim)
        self.index = faiss.IndexIVFFlat(quantizer, self.vector_dim, nlist, faiss.METRIC_INNER_PRODUCT)
        
        # Train and add vectors
        if spinner:
            spinner.text = f"è®­ç»ƒç´¢å¼•ï¼ˆ{vectors.shape[0]}ä¸ªå‘é‡ï¼Œ{nlist}ä¸ªèšç±»ä¸­å¿ƒï¼‰..."
        self.index.train(vectors) # type: ignore
        
        if spinner:
            spinner.text = "æ·»åŠ å‘é‡åˆ°ç´¢å¼•..."
        self.index.add(vectors) # type: ignore
        
        # Set the number of clusters to probe during search
        if spinner:
            spinner.text = "è®¾ç½®æœç´¢å‚æ•°..."
        self.index.nprobe = min(nlist, 10)
        
        if spinner:
            spinner.text = f"ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {vectors.shape[0]} ä¸ªå‘é‡"

    def _split_text(self, text: str) -> List[str]:
        """Use a more intelligent splitting strategy"""
        # Add overlapping blocks to maintain context consistency
        overlap_size = min(200, self.max_paragraph_length // 4)
        
        paragraphs = []
        current_chunk = []
        current_length = 0
        
        # First split by sentence
        sentences = []
        current_sentence = []
        sentence_ends = {'ã€‚', 'ï¼', 'ï¼Ÿ', 'â€¦', '.', '!', '?'}
        
        for char in text:
            current_sentence.append(char)
            if char in sentence_ends:
                sentence = ''.join(current_sentence)
                if sentence.strip():
                    sentences.append(sentence)
                current_sentence = []
        
        if current_sentence:
            sentence = ''.join(current_sentence)
            if sentence.strip():
                sentences.append(sentence)
        
        # Build overlapping blocks based on sentences
        for sentence in sentences:
            if current_length + len(sentence) > self.max_paragraph_length:
                if current_chunk:
                    chunk_text = ' '.join(current_chunk)
                    if len(chunk_text) >= self.min_paragraph_length:
                        paragraphs.append(chunk_text)
                        
                    # Keep some content as overlap
                    overlap_text = ' '.join(current_chunk[-2:])  # Keep the last two sentences
                    current_chunk = []
                    if overlap_text:
                        current_chunk.append(overlap_text)
                        current_length = len(overlap_text)
                    else:
                        current_length = 0
                        
            current_chunk.append(sentence)
            current_length += len(sentence)
        
        # Process the last chunk
        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            if len(chunk_text) >= self.min_paragraph_length:
                paragraphs.append(chunk_text)
        
        return paragraphs


    def _process_file(self, file_path: str, spinner=None) -> List[Document]:
        """Process a single file"""
        try:
            # Calculate file MD5
            if spinner:
                spinner.text = f"è®¡ç®—æ–‡ä»¶ {file_path} çš„MD5..."
            current_md5 = get_file_md5(file_path)
            if not current_md5:
                if spinner:
                    spinner.text = f"æ–‡ä»¶ {file_path} è®¡ç®—MD5å¤±è´¥"
                return []

            # Check if the file needs to be reprocessed
            if file_path in self.file_md5_cache and self.file_md5_cache[file_path] == current_md5:
                if spinner:
                    spinner.text = f"æ–‡ä»¶ {file_path} æœªå‘ç”Ÿå˜åŒ–ï¼Œè·³è¿‡å¤„ç†"
                return []

            # Find the appropriate processor
            if spinner:
                spinner.text = f"æŸ¥æ‰¾é€‚ç”¨äº {file_path} çš„å¤„ç†å™¨..."
            processor = None
            for p in self.file_processors:
                if p.can_handle(file_path):
                    processor = p
                    break
                    
            if not processor:
                # If no appropriate processor is found, return an empty document
                if spinner:
                    spinner.text = f"æ²¡æœ‰æ‰¾åˆ°é€‚ç”¨äº {file_path} çš„å¤„ç†å™¨ï¼Œè·³è¿‡å¤„ç†"
                return []
            
            # Extract text content
            if spinner:
                spinner.text = f"æå– {file_path} çš„æ–‡æœ¬å†…å®¹..."
            content = processor.extract_text(file_path)
            if not content.strip():
                if spinner:
                    spinner.text = f"æ–‡ä»¶ {file_path} æ²¡æœ‰æ–‡æœ¬å†…å®¹ï¼Œè·³è¿‡å¤„ç†"
                return []
            
            # Split text
            if spinner:
                spinner.text = f"åˆ†å‰² {file_path} çš„æ–‡æœ¬..."
            chunks = self._split_text(content)
            
            # Create document objects
            if spinner:
                spinner.text = f"ä¸º {file_path} åˆ›å»º {len(chunks)} ä¸ªæ–‡æ¡£å¯¹è±¡..."
            documents = []
            for i, chunk in enumerate(chunks):
                doc = Document(
                    content=chunk,
                    metadata={
                        "file_path": file_path,
                        "file_type": Path(file_path).suffix.lower(),
                        "chunk_index": i,
                        "total_chunks": len(chunks)
                    },
                    md5=current_md5
                )
                documents.append(doc)
            
            # Update MD5 cache
            self.file_md5_cache[file_path] = current_md5
            if spinner:
                spinner.text = f"æ–‡ä»¶ {file_path} å¤„ç†å®Œæˆï¼Œå…±åˆ›å»º {len(documents)} ä¸ªæ–‡æ¡£å¯¹è±¡"
            return documents
            
        except Exception as e:
            if spinner:
                spinner.text = f"å¤„ç†æ–‡ä»¶å¤±è´¥: {file_path}: {str(e)}"
            PrettyOutput.print(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {file_path}: {str(e)}", 
                            output_type=OutputType.ERROR)
            return []

    def _should_ignore_path(self, path: str, ignored_paths: List[str]) -> bool:
        """
        æ£€æŸ¥è·¯å¾„æ˜¯å¦åº”è¯¥è¢«å¿½ç•¥
        
        Args:
            path: æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„
            ignored_paths: å¿½ç•¥æ¨¡å¼åˆ—è¡¨
            
        Returns:
            bool: å¦‚æœè·¯å¾„åº”è¯¥è¢«å¿½ç•¥åˆ™è¿”å›True
        """
        import fnmatch
        import os
        
        # è·å–ç›¸å¯¹è·¯å¾„
        rel_path = path
        if os.path.isabs(path):
            try:
                rel_path = os.path.relpath(path, self.root_dir)
            except ValueError:
                # å¦‚æœä¸èƒ½è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä½¿ç”¨åŸå§‹è·¯å¾„
                pass
                
        path_parts = rel_path.split(os.sep)
        
        # æ£€æŸ¥è·¯å¾„çš„æ¯ä¸€éƒ¨åˆ†æ˜¯å¦åŒ¹é…ä»»æ„å¿½ç•¥æ¨¡å¼
        for part in path_parts:
            for pattern in ignored_paths:
                if fnmatch.fnmatch(part, pattern):
                    return True
                    
        # æ£€æŸ¥å®Œæ•´è·¯å¾„æ˜¯å¦åŒ¹é…ä»»æ„å¿½ç•¥æ¨¡å¼
        for pattern in ignored_paths:
            if fnmatch.fnmatch(rel_path, pattern):
                return True
                
        return False
        
    def _is_git_repo(self) -> bool:
        """
        æ£€æŸ¥å½“å‰ç›®å½•æ˜¯å¦ä¸ºGitä»“åº“
        
        Returns:
            bool: å¦‚æœæ˜¯Gitä»“åº“åˆ™è¿”å›True
        """
        import subprocess
        
        try:
            result = subprocess.run(
                ["git", "rev-parse", "--is-inside-work-tree"],
                cwd=self.root_dir,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                check=False
            )
            return result.returncode == 0 and result.stdout.strip() == "true"
        except Exception:
            return False
    
    def _get_git_managed_files(self) -> List[str]:
        """
        è·å–Gitä»“åº“ä¸­è¢«ç®¡ç†çš„æ–‡ä»¶åˆ—è¡¨
        
        Returns:
            List[str]: è¢«Gitç®¡ç†çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
        """
        import subprocess
        
        try:
            # è·å–gitç´¢å¼•ä¸­çš„æ–‡ä»¶
            result = subprocess.run(
                ["git", "ls-files"],
                cwd=self.root_dir,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                check=False
            )
            
            if result.returncode != 0:
                return []
                
            git_files = [line.strip() for line in result.stdout.splitlines() if line.strip()]
            
            # æ·»åŠ æœªæš‚å­˜ä½†å·²è·Ÿè¸ªçš„ä¿®æ”¹æ–‡ä»¶
            result = subprocess.run(
                ["git", "ls-files", "--modified"],
                cwd=self.root_dir,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                check=False
            )
            
            if result.returncode == 0:
                modified_files = [line.strip() for line in result.stdout.splitlines() if line.strip()]
                git_files.extend([f for f in modified_files if f not in git_files])
            
            # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            return [os.path.join(self.root_dir, file) for file in git_files]
            
        except Exception as e:
            PrettyOutput.print(f"è·å–Gitç®¡ç†çš„æ–‡ä»¶å¤±è´¥: {str(e)}", output_type=OutputType.WARNING)
            return []

    def build_index(self, dir: str):
        try:
            """Build document index with optimized processing"""
            # Get all files
            with yaspin(text="è·å–æ‰€æœ‰æ–‡ä»¶...", color="cyan") as spinner:
                all_files = []
                
                # è·å–éœ€è¦å¿½ç•¥çš„è·¯å¾„åˆ—è¡¨
                ignored_paths = get_rag_ignored_paths()
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºGitä»“åº“
                is_git_repo = self._is_git_repo()
                if is_git_repo:
                    git_files = self._get_git_managed_files()
                    # è¿‡æ»¤æ‰è¢«å¿½ç•¥çš„æ–‡ä»¶
                    for file_path in git_files:
                        if self._should_ignore_path(file_path, ignored_paths):
                            continue
                            
                        if os.path.getsize(file_path) > 100 * 1024 * 1024:  # 100MB
                            PrettyOutput.print(f"è·³è¿‡å¤§æ–‡ä»¶: {file_path}", 
                                            output_type=OutputType.WARNING)
                            continue
                        all_files.append(file_path)
                else:
                    # éGitä»“åº“ï¼Œä½¿ç”¨å¸¸è§„æ–‡ä»¶éå†
                    for root, _, files in os.walk(dir):
                        # æ£€æŸ¥ç›®å½•æ˜¯å¦åŒ¹é…å¿½ç•¥æ¨¡å¼
                        if self._should_ignore_path(root, ignored_paths):
                            continue
                            
                        for file in files:
                            file_path = os.path.join(root, file)
                            
                            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åŒ¹é…å¿½ç•¥æ¨¡å¼
                            if self._should_ignore_path(file_path, ignored_paths):
                                continue
                                
                            if os.path.getsize(file_path) > 100 * 1024 * 1024:  # 100MB
                                PrettyOutput.print(f"è·³è¿‡å¤§æ–‡ä»¶: {file_path}", 
                                                output_type=OutputType.WARNING)
                                continue
                            all_files.append(file_path)
                            
                spinner.text = f"è·å–æ‰€æœ‰æ–‡ä»¶å®Œæˆï¼Œå…± {len(all_files)} ä¸ªæ–‡ä»¶"
                spinner.ok("âœ…")

            # Clean up cache for deleted files
            with yaspin(text="æ¸…ç†ç¼“å­˜...", color="cyan") as spinner:
                deleted_files = set(self.file_md5_cache.keys()) - set(all_files)
                deleted_count = len(deleted_files)
                
                if deleted_count > 0:
                    spinner.write(f"ğŸ—‘ï¸ åˆ é™¤ä¸å­˜åœ¨æ–‡ä»¶çš„ç¼“å­˜: {deleted_count} ä¸ª")
                    
                for file_path in deleted_files:
                    # Remove from MD5 cache
                    del self.file_md5_cache[file_path]
                    # Remove related documents
                    self.documents = [doc for doc in self.documents if doc.metadata['file_path'] != file_path]
                    # Delete cache files
                    self._delete_file_cache(file_path, None)  # Pass None as spinner to not show individual deletions
                    
                spinner.text = f"æ¸…ç†ç¼“å­˜å®Œæˆï¼Œå…±åˆ é™¤ {deleted_count} ä¸ªä¸å­˜åœ¨æ–‡ä»¶çš„ç¼“å­˜"
                spinner.ok("âœ…")

            # Check file changes
            with yaspin(text="æ£€æŸ¥æ–‡ä»¶å˜åŒ–...", color="cyan") as spinner:
                files_to_process = []
                unchanged_files = []
                new_files_count = 0
                modified_files_count = 0
                
                for file_path in all_files:
                    current_md5 = get_file_md5(file_path)
                    if current_md5:  # Only process files that can successfully calculate MD5
                        if file_path in self.file_md5_cache and self.file_md5_cache[file_path] == current_md5:
                            # Fileæœªå˜åŒ–ï¼Œè®°å½•ä½†ä¸é‡æ–°å¤„ç†
                            unchanged_files.append(file_path)
                        else:
                            # New file or modified file
                            files_to_process.append(file_path)
                            
                            # å¦‚æœæ˜¯ä¿®æ”¹çš„æ–‡ä»¶ï¼Œåˆ é™¤æ—§ç¼“å­˜
                            if file_path in self.file_md5_cache:
                                modified_files_count += 1
                                # åˆ é™¤æ—§ç¼“å­˜
                                self._delete_file_cache(file_path, spinner)
                                # ä»æ–‡æ¡£åˆ—è¡¨ä¸­ç§»é™¤
                                self.documents = [doc for doc in self.documents if doc.metadata['file_path'] != file_path]
                            else:
                                new_files_count += 1
                
                # è¾“å‡ºæ±‡æ€»ä¿¡æ¯
                if unchanged_files:
                    spinner.write(f"ğŸ“š å·²ç¼“å­˜æ–‡ä»¶: {len(unchanged_files)} ä¸ª")
                if new_files_count > 0:
                    spinner.write(f"ğŸ†• æ–°å¢æ–‡ä»¶: {new_files_count} ä¸ª")
                if modified_files_count > 0:
                    spinner.write(f"ğŸ“ ä¿®æ”¹æ–‡ä»¶: {modified_files_count} ä¸ª")
                    
                spinner.text = f"æ£€æŸ¥æ–‡ä»¶å˜åŒ–å®Œæˆï¼Œå…± {len(files_to_process)} ä¸ªæ–‡ä»¶éœ€è¦å¤„ç†"
                spinner.ok("âœ…")

            # Keep documents for unchanged files
            unchanged_documents = [doc for doc in self.documents 
                                if doc.metadata['file_path'] in unchanged_files]

            # Process files one by one with optimized vectorization
            if files_to_process:
                new_documents = []
                new_vectors = []
                success_count = 0
                skipped_count = 0
                failed_count = 0
                
                with yaspin(text=f"å¤„ç†æ–‡ä»¶ä¸­ (0/{len(files_to_process)})...", color="cyan") as spinner:
                    for index, file_path in enumerate(files_to_process):
                        spinner.text = f"å¤„ç†æ–‡ä»¶ä¸­ ({index+1}/{len(files_to_process)}): {file_path}"
                        try:
                            # Process single file
                            file_docs = self._process_file(file_path, spinner)
                            if file_docs:
                                # Vectorize documents from this file
                                spinner.text = f"å¤„ç†æ–‡ä»¶ä¸­ ({index+1}/{len(files_to_process)}): ä¸º {file_path} ç”Ÿæˆå‘é‡åµŒå…¥..."
                                texts_to_vectorize = [
                                    f"File:{doc.metadata['file_path']} Content:{doc.content}"
                                    for doc in file_docs
                                ]
                                
                                file_vectors = get_embedding_batch(self.embedding_model, f"({index+1}/{len(files_to_process)}){file_path}", texts_to_vectorize, spinner)
                                
                                # Save cache for this file
                                spinner.text = f"å¤„ç†æ–‡ä»¶ä¸­ ({index+1}/{len(files_to_process)}): ä¿å­˜ {file_path} çš„ç¼“å­˜..."
                                self._save_cache(file_path, file_docs, file_vectors, spinner)
                                
                                # Accumulate documents and vectors
                                new_documents.extend(file_docs)
                                new_vectors.append(file_vectors)
                                success_count += 1
                            else:
                                # æ–‡ä»¶è·³è¿‡å¤„ç†
                                skipped_count += 1
                                
                        except Exception as e:
                            spinner.write(f"âŒ å¤„ç†å¤±è´¥: {file_path}: {str(e)}")
                            failed_count += 1
                    
                    # è¾“å‡ºå¤„ç†ç»Ÿè®¡
                    spinner.text = f"æ–‡ä»¶å¤„ç†å®Œæˆ: æˆåŠŸ {success_count} ä¸ª, è·³è¿‡ {skipped_count} ä¸ª, å¤±è´¥ {failed_count} ä¸ª"
                    spinner.ok("âœ…")
                    
                # Update documents list
                self.documents.extend(new_documents)

                # Build final index
                if new_vectors:
                    with yaspin(text="æ„å»ºæœ€ç»ˆç´¢å¼•...", color="cyan") as spinner:
                        spinner.text = "åˆå¹¶æ–°å‘é‡..."
                        all_new_vectors = np.vstack(new_vectors)
                        
                        unchanged_vector_count = 0
                        if self.flat_index is not None:
                            # Get vectors for unchanged documents
                            spinner.text = "è·å–æœªå˜åŒ–æ–‡æ¡£çš„å‘é‡..."
                            unchanged_vectors = self._get_unchanged_vectors(unchanged_documents, spinner)
                            if unchanged_vectors is not None:
                                unchanged_vector_count = unchanged_vectors.shape[0]
                                spinner.text = f"åˆå¹¶æ–°æ—§å‘é‡ï¼ˆæ–°ï¼š{all_new_vectors.shape[0]}ï¼Œæ—§ï¼š{unchanged_vector_count}ï¼‰..."
                                final_vectors = np.vstack([unchanged_vectors, all_new_vectors])
                            else:
                                spinner.text = f"ä»…ä½¿ç”¨æ–°å‘é‡ï¼ˆ{all_new_vectors.shape[0]}ï¼‰..."
                                final_vectors = all_new_vectors
                        else:
                            spinner.text = f"ä»…ä½¿ç”¨æ–°å‘é‡ï¼ˆ{all_new_vectors.shape[0]}ï¼‰..."
                            final_vectors = all_new_vectors

                        # Build index
                        spinner.text = f"æ„å»ºç´¢å¼•ï¼ˆå‘é‡æ•°é‡ï¼š{final_vectors.shape[0]}ï¼‰..."
                        self._build_index(final_vectors, spinner)
                        spinner.text = f"ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(self.documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ"
                        spinner.ok("âœ…")

                # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
                PrettyOutput.print(
                    f"ğŸ“Š ç´¢å¼•ç»Ÿè®¡:\n"
                    f"  â€¢ æ€»æ–‡æ¡£æ•°: {len(self.documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ\n"
                    f"  â€¢ å·²ç¼“å­˜æ–‡ä»¶: {len(unchanged_files)} ä¸ª\n"
                    f"  â€¢ å¤„ç†æ–‡ä»¶: {len(files_to_process)} ä¸ª\n"
                    f"    - æˆåŠŸ: {success_count} ä¸ª\n"
                    f"    - è·³è¿‡: {skipped_count} ä¸ª\n"
                    f"    - å¤±è´¥: {failed_count} ä¸ª", 
                    OutputType.SUCCESS
                )
        except Exception as e:
            PrettyOutput.print(f"ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}", 
                            output_type=OutputType.ERROR)

    def _get_unchanged_vectors(self, unchanged_documents: List[Document], spinner=None) -> Optional[np.ndarray]:
        """Get vectors for unchanged documents from existing index"""
        try:
            if not unchanged_documents:
                if spinner:
                    spinner.text = "æ²¡æœ‰æœªå˜åŒ–çš„æ–‡æ¡£"
                return None

            if spinner:
                spinner.text = f"åŠ è½½ {len(unchanged_documents)} ä¸ªæœªå˜åŒ–æ–‡æ¡£çš„å‘é‡..."
            
            # æŒ‰æ–‡ä»¶åˆ†ç»„å¤„ç†
            unchanged_files = set(doc.metadata['file_path'] for doc in unchanged_documents)
            unchanged_vectors = []
            
            for file_path in unchanged_files:
                if spinner:
                    spinner.text = f"åŠ è½½ {file_path} çš„å‘é‡..."
                
                # è·å–è¯¥æ–‡ä»¶æ‰€æœ‰æ–‡æ¡£çš„chunkç´¢å¼•
                doc_indices = [(i, doc.metadata['chunk_index']) 
                              for i, doc in enumerate(unchanged_documents) 
                              if doc.metadata['file_path'] == file_path]
                
                if not doc_indices:
                    continue
                
                # åŠ è½½è¯¥æ–‡ä»¶çš„å‘é‡
                vec_cache_path = self._get_cache_path(file_path, "vec")
                if os.path.exists(vec_cache_path):
                    try:
                        with lzma.open(vec_cache_path, 'rb') as f:
                            vec_cache_data = pickle.load(f)
                            file_vectors = vec_cache_data["vectors"]
                        
                        # æŒ‰ç…§chunk_indexåŠ è½½å¯¹åº”çš„å‘é‡
                        for _, chunk_idx in doc_indices:
                            if chunk_idx < file_vectors.shape[0]:
                                unchanged_vectors.append(file_vectors[chunk_idx].reshape(1, -1))
                            
                        if spinner:
                            spinner.text = f"æˆåŠŸåŠ è½½ {file_path} çš„å‘é‡"
                    except Exception as e:
                        if spinner:
                            spinner.text = f"åŠ è½½ {file_path} å‘é‡å¤±è´¥: {str(e)}"
                else:
                    if spinner:
                        spinner.text = f"æœªæ‰¾åˆ° {file_path} çš„å‘é‡ç¼“å­˜"
                        
                    # ä»flat_indexé‡å»ºå‘é‡
                    if self.flat_index is not None:
                        if spinner:
                            spinner.text = f"ä»ç´¢å¼•é‡å»º {file_path} çš„å‘é‡..."
                        
                        for doc_idx, chunk_idx in doc_indices:
                            idx = next((i for i, d in enumerate(self.documents) 
                                     if d.metadata['file_path'] == file_path and 
                                     d.metadata['chunk_index'] == chunk_idx), None)
                            
                            if idx is not None:
                                vector = np.zeros((1, self.vector_dim), dtype=np.float32) # type: ignore
                                self.flat_index.reconstruct(idx, vector.ravel())
                                unchanged_vectors.append(vector)

            if not unchanged_vectors:
                if spinner:
                    spinner.text = "æœªèƒ½åŠ è½½ä»»ä½•æœªå˜åŒ–æ–‡æ¡£çš„å‘é‡"
                return None
                
            if spinner:
                spinner.text = f"æœªå˜åŒ–æ–‡æ¡£å‘é‡åŠ è½½å®Œæˆï¼Œå…± {len(unchanged_vectors)} ä¸ª"
                
            return np.vstack(unchanged_vectors)
            
        except Exception as e:
            if spinner:
                spinner.text = f"è·å–ä¸å˜å‘é‡å¤±è´¥: {str(e)}"
            PrettyOutput.print(f"è·å–ä¸å˜å‘é‡å¤±è´¥: {str(e)}", OutputType.ERROR)
            return None

    def _perform_keyword_search(self, query: str, limit: int = 30) -> List[Tuple[int, float]]:
        """æ‰§è¡ŒåŸºäºå…³é”®è¯çš„æ–‡æœ¬æœç´¢
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            List[Tuple[int, float]]: æ–‡æ¡£ç´¢å¼•å’Œå¾—åˆ†çš„åˆ—è¡¨
        """
        # ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆå…³é”®è¯
        keywords = self._generate_keywords_with_llm(query)
        
        # å¦‚æœå¤§æ¨¡å‹ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°ç®€å•çš„å…³é”®è¯æå–
        if not keywords:
            # ç®€å•çš„å…³é”®è¯é¢„å¤„ç†
            keywords = query.lower().split()
            # ç§»é™¤åœç”¨è¯å’Œè¿‡çŸ­çš„è¯
            stop_words = {'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'åœ¨', 'æœ‰', 'ä¸', 'å¯¹', 'ä¸º', 'a', 'an', 'the', 'and', 'is', 'in', 'of', 'to', 'with'}
            keywords = [k for k in keywords if k not in stop_words and len(k) > 1]
        
        if not keywords:
            return []
        
        # ä½¿ç”¨TF-IDFæ€æƒ³çš„ç®€å•å®ç°
        doc_scores = []
        
        # è®¡ç®—IDFï¼ˆé€†æ–‡æ¡£é¢‘ç‡ï¼‰
        doc_count = len(self.documents)
        keyword_doc_count = {}
        
        for keyword in keywords:
            count = 0
            for doc in self.documents:
                if keyword in doc.content.lower():
                    count += 1
            keyword_doc_count[keyword] = max(1, count)  # é¿å…é™¤é›¶é”™è¯¯
        
        # è®¡ç®—æ¯ä¸ªå…³é”®è¯çš„IDFå€¼
        keyword_idf = {
            keyword: np.log(doc_count / count) 
            for keyword, count in keyword_doc_count.items()
        }
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£è®¡ç®—å¾—åˆ†
        for i, doc in enumerate(self.documents):
            doc_content = doc.content.lower()
            score = 0
            
            # è®¡ç®—æ¯ä¸ªå…³é”®è¯çš„TFï¼ˆè¯é¢‘ï¼‰
            for keyword in keywords:
                # ç®€å•çš„TFï¼šå…³é”®è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°
                tf = doc_content.count(keyword)
                # TF-IDFå¾—åˆ†
                if tf > 0:
                    score += tf * keyword_idf[keyword]
            
            # æ·»åŠ é¢å¤–æƒé‡ï¼šæ ‡é¢˜åŒ¹é…ã€å®Œæ•´çŸ­è¯­åŒ¹é…ç­‰
            if query.lower() in doc_content:
                score *= 2.0  # å®Œæ•´æŸ¥è¯¢åŒ¹é…åŠ å€å¾—åˆ†
                
            # æ–‡ä»¶è·¯å¾„åŒ¹é…ä¹ŸåŠ åˆ†
            file_path = doc.metadata['file_path'].lower()
            for keyword in keywords:
                if keyword in file_path:
                    score += 0.5 * keyword_idf.get(keyword, 1.0)
            
            if score > 0:
                # å½’ä¸€åŒ–å¾—åˆ†ï¼ˆ0-1èŒƒå›´ï¼‰
                doc_scores.append((i, score))
        
        # æ’åºå¹¶é™åˆ¶ç»“æœæ•°é‡
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        # å½’ä¸€åŒ–åˆ†æ•°åˆ°0-1ä¹‹é—´
        if doc_scores:
            max_score = max(score for _, score in doc_scores)
            if max_score > 0:
                doc_scores = [(idx, score/max_score) for idx, score in doc_scores]
        
        return doc_scores[:limit]

    def _generate_keywords_with_llm(self, query: str) -> List[str]:
        """
        ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ä»æŸ¥è¯¢ä¸­æå–å…³é”®è¯
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            List[str]: æå–çš„å…³é”®è¯åˆ—è¡¨
        """
        try:
            from jarvis.jarvis_utils.output import PrettyOutput, OutputType
            from jarvis.jarvis_platform.registry import PlatformRegistry
            
            # è·å–å¹³å°æ³¨å†Œè¡¨å’Œæ¨¡å‹
            registry = PlatformRegistry.get_global_platform_registry()
            model = registry.get_normal_platform()

            # æ„å»ºå…³é”®è¯æå–æç¤ºè¯
            prompt = f"""
            è¯·åˆ†æä»¥ä¸‹æŸ¥è¯¢ï¼Œæå–ç”¨äºæ–‡æ¡£æ£€ç´¢çš„å…³é”®è¯ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
            
            1. è¯†åˆ«æ ¸å¿ƒæ¦‚å¿µã€ä¸»é¢˜å’Œå®ä½“ï¼ŒåŒ…æ‹¬:
               - æŠ€æœ¯æœ¯è¯­å’Œä¸“ä¸šåè¯
               - ä»£ç ç›¸å…³çš„å‡½æ•°åã€ç±»åã€å˜é‡åå’Œåº“å
               - é‡è¦çš„ä¸šåŠ¡é¢†åŸŸè¯æ±‡
               - æ“ä½œå’ŒåŠ¨ä½œç›¸å…³çš„è¯æ±‡
            
            2. ä¼˜å…ˆæå–ä¸ä»¥ä¸‹åœºæ™¯ç›¸å…³çš„å…³é”®è¯:
               - ä»£ç æœç´¢: ç¼–ç¨‹è¯­è¨€ã€æ¡†æ¶ã€APIã€ç‰¹å®šåŠŸèƒ½
               - æ–‡æ¡£æ£€ç´¢: ä¸»é¢˜ã€æ ‡é¢˜è¯æ±‡ã€ä¸“ä¸šåè¯
               - é”™è¯¯æ’æŸ¥: é”™è¯¯ä¿¡æ¯ã€å¼‚å¸¸åç§°ã€é—®é¢˜ç—‡çŠ¶
            
            3. åŒæ—¶åŒ…å«:
               - ä¸­è‹±æ–‡å…³é”®è¯ (å°¤å…¶æ˜¯æŠ€æœ¯é¢†åŸŸå¸¸ç”¨è‹±æ–‡æœ¯è¯­)
               - å®Œæ•´çš„ä¸“ä¸šæœ¯è¯­å’Œç¼©å†™å½¢å¼
               - å¯èƒ½çš„åŒä¹‰è¯æˆ–ç›¸å…³æ¦‚å¿µ
            
            4. å…³é”®è¯åº”å½“ç²¾å‡†ã€å…·ä½“ï¼Œæ•°é‡æ§åˆ¶åœ¨3-10ä¸ªä¹‹é—´ã€‚
            
            è¾“å‡ºæ ¼å¼ï¼š
            {ot("KEYWORD")}
            å…³é”®è¯1
            å…³é”®è¯2
            ...
            {ct("KEYWORD")}
            
            æŸ¥è¯¢æ–‡æœ¬:
            {query}

            ä»…è¿”å›æå–çš„å…³é”®è¯ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚
            """
            
            # è°ƒç”¨å¤§æ¨¡å‹è·å–å“åº”
            response = model.chat_until_success(prompt)
            
            if response:
                # æ¸…ç†å“åº”ï¼Œæå–å…³é”®è¯
                sm = re.search(ot('KEYWORD') + r"(.*?)" + ct('KEYWORD'), response, re.DOTALL)
                if sm:
                    extracted_keywords = sm[1]
                
                    if extracted_keywords:
                        # è®°å½•æ£€æµ‹åˆ°çš„å…³é”®è¯
                        ret = extracted_keywords.strip().splitlines()
                        return ret
                
            # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            return []
            
        except Exception as e:
            from jarvis.jarvis_utils.output import PrettyOutput, OutputType
            PrettyOutput.print(f"ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆå…³é”®è¯å¤±è´¥: {str(e)}", OutputType.WARNING)
            return []

    def _hybrid_search(self, query: str, top_k: int = 30) -> List[Tuple[int, float]]:
        """æ··åˆæœç´¢æ–¹æ³•ï¼Œç»¼åˆå‘é‡ç›¸ä¼¼åº¦å’Œå…³é”®è¯åŒ¹é…
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            List[Tuple[int, float]]: æ–‡æ¡£ç´¢å¼•å’Œå¾—åˆ†çš„åˆ—è¡¨
        """
        # è·å–å‘é‡æœç´¢ç»“æœ
        query_vector = get_embedding(self.embedding_model, query)
        query_vector = query_vector.reshape(1, -1)
        
        # è¿›è¡Œå‘é‡æœç´¢
        vector_limit = min(top_k * 3, len(self.documents))
        if self.index and vector_limit > 0:
            distances, indices = self.index.search(query_vector, vector_limit) # type: ignore
            vector_results = [(int(idx), 1.0 / (1.0 + float(dist))) 
                             for idx, dist in zip(indices[0], distances[0])
                             if idx != -1 and idx < len(self.documents)]
        else:
            vector_results = []
        
        # è¿›è¡Œå…³é”®è¯æœç´¢
        keyword_results = self._perform_keyword_search(query, top_k * 2)
        
        # åˆå¹¶ç»“æœé›†
        combined_results = {}
        
        # åŠ å…¥å‘é‡ç»“æœï¼Œæƒé‡ä¸º0.7
        for idx, score in vector_results:
            combined_results[idx] = score * 0.7
        
        # åŠ å…¥å…³é”®è¯ç»“æœï¼Œæƒé‡ä¸º0.3ï¼Œå¦‚æœæ–‡æ¡£å·²å­˜åœ¨åˆ™å–åŠ æƒå¹³å‡
        for idx, score in keyword_results:
            if idx in combined_results:
                # å·²æœ‰å‘é‡å¾—åˆ†ï¼Œå–åŠ æƒå¹³å‡
                combined_results[idx] = combined_results[idx] + score * 0.3
            else:
                # æ–°æ–‡æ¡£ï¼Œç›´æ¥æ·»åŠ å…³é”®è¯å¾—åˆ†ï¼ˆæƒé‡ç¨ä½ï¼‰
                combined_results[idx] = score * 0.3
        
        # è½¬æ¢æˆåˆ—è¡¨å¹¶æ’åº
        result_list = [(idx, score) for idx, score in combined_results.items()]
        result_list.sort(key=lambda x: x[1], reverse=True)
        
        return result_list[:top_k]

    def _rerank_candidates(self, query: str, initial_indices: List[int], spinner=None) -> Tuple[List[int], List[Tuple[int, float]]]:
        """å¯¹å€™é€‰æ–‡æ¡£è¿›è¡Œé‡æ’åºä»¥æé«˜å‡†ç¡®åº¦
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            initial_indices: åˆå§‹å€™é€‰æ–‡æ¡£ç´¢å¼•
            spinner: ç”¨äºæ˜¾ç¤ºè¿›åº¦çš„spinnerå¯¹è±¡(å¯é€‰)
            
        Returns:
            Tuple[List[int], List[Tuple[int, float]]]: é‡æ’åºåçš„æ–‡æ¡£ç´¢å¼•åŠå¯¹åº”è¯„åˆ†
        """
        try:
            # è·å–é‡æ’åºæ¨¡å‹
            from jarvis.jarvis_utils.embedding import load_rerank_model
            
            with spinner.hidden() if spinner else contextlib.nullcontext():
                rerank_model, rerank_tokenizer = load_rerank_model()
            
            # å‡†å¤‡é‡æ’åºçš„æ–‡æ¡£
            rerank_candidates = []
            rerank_indices = []
            
            # æ”¶é›†æœ‰æ•ˆçš„æ–‡æ¡£ç”¨äºé‡æ’åº
            for i, idx in enumerate(initial_indices):
                if idx < len(self.documents):  # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                    doc = self.documents[idx]
                    # è·å–æ–‡æ¡£å†…å®¹ï¼Œæ·»åŠ æ–‡ä»¶è·¯å¾„ä½œä¸ºä¸Šä¸‹æ–‡å¢å¼º
                    doc_text = f"æ–‡ä»¶: {doc.metadata['file_path']}\n{doc.content}"
                    rerank_candidates.append(doc_text)
                    rerank_indices.append((i, idx))
            
            # å¦‚æœæœ‰å€™é€‰æ–‡æ¡£ï¼Œè¿›è¡Œé‡æ’åº
            if rerank_candidates:
                if spinner:
                    spinner.text = f"é‡æ’åº {len(rerank_candidates)} ä¸ªå€™é€‰æ–‡æ¡£..."
                
                # åˆ†æ‰¹é‡æ’åºä»¥é¿å…å†…å­˜æº¢å‡º
                batch_size = 50
                all_scores = []
                
                # å…ˆå°è¯•ä½¿ç”¨GPUè¿›è¡Œé‡æ’åº
                use_gpu = torch.cuda.is_available()
                gpu_failed = False
                
                try:
                    for i in range(0, len(rerank_candidates), batch_size):
                        batch = rerank_candidates[i:i+batch_size]
                        # å‡†å¤‡é‡æ’åºæ¨¡å‹çš„è¾“å…¥
                        inputs = []
                        for doc in batch:
                            inputs.append((query, doc))
                            
                        model_inputs = rerank_tokenizer.batch_encode_plus( # type: ignore
                            inputs,
                            padding=True,
                            truncation=True,
                            return_tensors="pt",
                            max_length=512
                        )
                        
                        # å°†å¼ é‡ç§»åˆ°é€‚å½“çš„è®¾å¤‡ä¸Š
                        if use_gpu:
                            try:
                                model_inputs = {k: v.cuda() for k, v in model_inputs.items()}
                            except Exception as gpu_error:
                                if spinner:
                                    spinner.text = f"GPUåŠ è½½å¤±è´¥({str(gpu_error)})ï¼Œåˆ‡æ¢åˆ°CPU..."
                                use_gpu = False
                                gpu_failed = True
                                # é‡æ–°å¼€å§‹æœ¬æ‰¹æ¬¡ï¼Œä½¿ç”¨CPU
                                raise RuntimeError("GPUåŠ è½½å¤±è´¥ï¼Œåˆ‡æ¢åˆ°CPU") from gpu_error
                        
                        # ä½¿ç”¨å½“å‰è®¾å¤‡è®¡ç®—é‡æ’åºå¾—åˆ†
                        with torch.no_grad():
                            outputs = rerank_model(**model_inputs) # type: ignore
                            scores = outputs.logits
                            scores = scores.detach().cpu().numpy()
                            all_scores.extend(scores.squeeze().tolist())
                            
                        if spinner and i + batch_size < len(rerank_candidates):
                            spinner.text = f"é‡æ’åºè¿›åº¦: {i + batch_size}/{len(rerank_candidates)} ({use_gpu and 'GPU' or 'CPU'})"
                
                except Exception as e:
                    # å¦‚æœä½¿ç”¨GPUå¤±è´¥ï¼Œå°è¯•åˆ‡æ¢åˆ°CPUé‡æ–°å¤„ç†æ•´ä¸ªä»»åŠ¡
                    if use_gpu or gpu_failed:
                        if spinner:
                            spinner.text = f"GPUé‡æ’åºå¤±è´¥ï¼Œåˆ‡æ¢åˆ°CPUé‡è¯•..."
                        
                        # é‡ç½®å¾—åˆ†å’Œä½¿ç”¨CPU
                        all_scores = []
                        use_gpu = False
                        
                        # ä½¿ç”¨CPUé‡æ–°å¤„ç†æ‰€æœ‰æ‰¹æ¬¡
                        for i in range(0, len(rerank_candidates), batch_size):
                            batch = rerank_candidates[i:i+batch_size]
                            
                            inputs = []
                            for doc in batch:
                                inputs.append((query, doc))
                                
                            model_inputs = rerank_tokenizer.batch_encode_plus( # type: ignore
                                inputs,
                                padding=True,
                                truncation=True,
                                return_tensors="pt",
                                max_length=512
                            )
                            
                            # ç¡®ä¿åœ¨CPUä¸Šå¤„ç†
                            with torch.no_grad():
                                outputs = rerank_model(**model_inputs) # type: ignore
                                scores = outputs.logits
                                scores = scores.detach().cpu().numpy()
                                all_scores.extend(scores.squeeze().tolist())
                                
                            if spinner and i + batch_size < len(rerank_candidates):
                                spinner.text = f"CPUé‡æ’åºè¿›åº¦: {i + batch_size}/{len(rerank_candidates)}"
                    else:
                        # å¦‚æœCPUä¹Ÿå¤±è´¥ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
                        raise
                
                # å°†é‡æ’åºå¾—åˆ†ä¸å€™é€‰æ–‡æ¡£å’ŒåŸå§‹ç´¢å¼•å…³è”
                reranked_results = []
                for (orig_i, idx), score in zip(rerank_indices, all_scores):
                    reranked_results.append((idx, score))
                
                # æŒ‰é‡æ’åºå¾—åˆ†æ’åºï¼ˆé™åºï¼‰
                reranked_results.sort(key=lambda x: x[1], reverse=True)
                
                # æå–æ’åºåçš„æ–‡æ¡£ç´¢å¼•
                reranked_indices = [idx for idx, _ in reranked_results]
                
                if spinner:
                    spinner.text = f"é‡æ’åºå®Œæˆ (ä½¿ç”¨{'GPU' if use_gpu else 'CPU'})"
                
                # è¿”å›é‡æ’åºç»“æœåŠè¯„åˆ†
                return reranked_indices, reranked_results
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆå€™é€‰ï¼Œè¿”å›åŸå§‹ç´¢å¼•
                if spinner:
                    spinner.text = "è·³è¿‡é‡æ’åºï¼ˆæ— æœ‰æ•ˆå€™é€‰ï¼‰"
                
                # è¿”å›åŸå§‹ç»“æœï¼Œæ²¡æœ‰è¯„åˆ†
                return initial_indices, []
        
        except Exception as e:
            # å¦‚æœé‡æ’åºå¤±è´¥ï¼Œè¿”å›åŸå§‹ç´¢å¼•
            if spinner:
                spinner.text = f"é‡æ’åºå¤±è´¥ï¼ˆ{str(e)}ï¼‰ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ"
            
            # è¿”å›åŸå§‹ç´¢å¼•ï¼Œæ²¡æœ‰è¯„åˆ†
            return initial_indices, []

    def search(self, query: str, top_k: int = 30) -> List[Tuple[Document, float]]:
        """Search documents with context window"""
        if not self.is_index_built():
            PrettyOutput.print("ç´¢å¼•æœªå»ºç«‹ï¼Œè‡ªåŠ¨å»ºç«‹ç´¢å¼•ä¸­...", OutputType.INFO)
            self.build_index(self.root_dir)
            
        # å¦‚æœç´¢å¼•å»ºç«‹å¤±è´¥æˆ–æ–‡æ¡£åˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ
        if not self.is_index_built():
            PrettyOutput.print("ç´¢å¼•å»ºç«‹å¤±è´¥æˆ–æ–‡æ¡£åˆ—è¡¨ä¸ºç©º", OutputType.WARNING)
            return []
            
        # ä½¿ç”¨æ··åˆæœç´¢è·å–å€™é€‰æ–‡æ¡£
        with yaspin(text="æ‰§è¡Œæ··åˆæœç´¢...", color="cyan") as spinner:
            # è·å–åˆå§‹å€™é€‰ç»“æœ
            search_results = self._hybrid_search(query, top_k * 2)
            
            if not search_results:
                spinner.text = "æœç´¢ç»“æœä¸ºç©º"
                spinner.fail("âŒ")
                return []
                
            # å‡†å¤‡é‡æ’åº
            initial_indices = [idx for idx, _ in search_results]
            spinner.text = f"æ£€ç´¢å®Œæˆï¼Œè·å– {len(initial_indices)} ä¸ªå€™é€‰æ–‡æ¡£"
            spinner.ok("âœ…")
        
        # Apply reranking for better accuracy
        with yaspin(text="é‡æ’åºä»¥æé«˜å‡†ç¡®åº¦...", color="cyan") as spinner:
            # è°ƒç”¨é‡æ’åºå‡½æ•°
            indices_list, reranked_results = self._rerank_candidates(query, initial_indices, spinner)
            
            if reranked_results:  # å¦‚æœé‡æ’åºæˆåŠŸ
                spinner.text = "é‡æ’åºå®Œæˆ"
            else:  # ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ
                indices_list = [idx for idx, _ in search_results if idx < len(self.documents)]
            
            spinner.ok("âœ…")
        
        # Process results with context window
        with yaspin(text="å¤„ç†ç»“æœ...", color="cyan") as spinner:
            results = []
            seen_files = set()
            
            # æ£€æŸ¥ç´¢å¼•åˆ—è¡¨æ˜¯å¦ä¸ºç©º
            if not indices_list:
                spinner.text = "æœç´¢ç»“æœä¸ºç©º"
                spinner.fail("âŒ")
                return []
                
            for idx in indices_list:
                if idx < len(self.documents):  # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                    doc = self.documents[idx]
                    
                    # ä½¿ç”¨é‡æ’åºå¾—åˆ†æˆ–åŸºäºåŸå§‹ç›¸ä¼¼åº¦çš„å¾—åˆ†
                    similarity = next((score for i, score in reranked_results if i == idx), 0.5) if reranked_results else 0.5
                    
                    file_path = doc.metadata['file_path']
                    if file_path not in seen_files:
                        seen_files.add(file_path)
                        
                        # Get full context from original document
                        original_doc = next((d for d in self.documents 
                                        if d.metadata['file_path'] == file_path), None)
                        if original_doc:
                            window_docs = []  # Add this line to initialize the list
                            # Find all chunks from this file
                            file_chunks = [d for d in self.documents 
                                        if d.metadata['file_path'] == file_path]
                            # Add all related chunks
                            for chunk_doc in file_chunks:
                                window_docs.append((chunk_doc, similarity * 0.9))
                        
                        results.extend(window_docs)
                        if len(results) >= top_k * (2 * self.context_window + 1):
                            break
            spinner.text = "å¤„ç†ç»“æœå®Œæˆ"
            spinner.ok("âœ…")
        
        # Sort by similarity and deduplicate
        with yaspin(text="æ’åº...", color="cyan") as spinner:
            if not results:
                spinner.text = "æ— æœ‰æ•ˆç»“æœ"
                spinner.fail("âŒ")
                return []
                
            results.sort(key=lambda x: x[1], reverse=True)
            seen = set()
            final_results = []
            for doc, score in results:
                key = (doc.metadata['file_path'], doc.metadata['chunk_index'])
                if key not in seen:
                    seen.add(key)
                    final_results.append((doc, score))
                    if len(final_results) >= top_k:
                        break
            spinner.text = "æ’åºå®Œæˆ"
            spinner.ok("âœ…")
                    
        return final_results

    def query(self, query: str) -> List[Document]:
        """Query related documents
        
        Args:
            query: Query text
            
        Returns:
            List[Document]: Related documents
        """
        results = self.search(query)
        return [doc for doc, _ in results]

    def ask(self, question: str) -> Optional[str]:
        """Ask questions about documents with enhanced context building"""
        try:
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å»ºç«‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™è‡ªåŠ¨å»ºç«‹
            if not self.is_index_built():
                PrettyOutput.print("ç´¢å¼•æœªå»ºç«‹ï¼Œè‡ªåŠ¨å»ºç«‹ç´¢å¼•ä¸­...", OutputType.INFO)
                self.build_index(self.root_dir)
                
                # å¦‚æœå»ºç«‹ç´¢å¼•åä»æœªæˆåŠŸï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
                if not self.is_index_built():
                    PrettyOutput.print("æ— æ³•å»ºç«‹ç´¢å¼•ï¼Œè¯·æ£€æŸ¥æ–‡æ¡£å’Œé…ç½®", OutputType.ERROR)
                    return "æ— æ³•å»ºç«‹ç´¢å¼•ï¼Œè¯·æ£€æŸ¥æ–‡æ¡£å’Œé…ç½®ã€‚å¯èƒ½çš„åŸå› ï¼šæ–‡æ¡£ç›®å½•ä¸ºç©ºã€æƒé™ä¸è¶³æˆ–æ ¼å¼ä¸æ”¯æŒã€‚"
            
            # å¢å¼ºæŸ¥è¯¢é¢„å¤„ç† - æå–å…³é”®è¯å’Œè¯­ä¹‰ä¿¡æ¯
            enhanced_query = self._enhance_query(question)
            
            # ä½¿ç”¨å¢å¼ºçš„æŸ¥è¯¢è¿›è¡Œæœç´¢
            results = self.search(enhanced_query)
            if not results:
                return "æœªæ‰¾åˆ°ä¸é—®é¢˜ç›¸å…³çš„æ–‡æ¡£ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°é—®é¢˜æˆ–ç¡®è®¤é—®é¢˜ç›¸å…³å†…å®¹å·²åŒ…å«åœ¨ç´¢å¼•ä¸­ã€‚"
            
            prompt = f"""
# ğŸ¤– è§’è‰²å®šä¹‰
æ‚¨æ˜¯ä¸€ä½æ–‡æ¡£åˆ†æä¸“å®¶ï¼Œèƒ½å¤ŸåŸºäºæä¾›çš„æ–‡æ¡£æä¾›å‡†ç¡®ä¸”å…¨é¢çš„å›ç­”ã€‚

# ğŸ¯ æ ¸å¿ƒèŒè´£
- å…¨é¢åˆ†ææ–‡æ¡£ç‰‡æ®µ
- å‡†ç¡®å›ç­”é—®é¢˜
- å¼•ç”¨æºæ–‡æ¡£
- è¯†åˆ«ç¼ºå¤±ä¿¡æ¯
- ä¿æŒä¸“ä¸šè¯­æ°”

# ğŸ“‹ å›ç­”è¦æ±‚
## å†…å®¹è´¨é‡
- ä¸¥æ ¼åŸºäºæä¾›çš„æ–‡æ¡£ä½œç­”
- å…·ä½“ä¸”ç²¾ç¡®
- åœ¨æœ‰å¸®åŠ©æ—¶å¼•ç”¨ç›¸å…³å†…å®¹
- æŒ‡å‡ºä»»ä½•ä¿¡æ¯ç¼ºå£
- ä½¿ç”¨ä¸“ä¸šè¯­è¨€

## å›ç­”ç»“æ„
1. ç›´æ¥å›ç­”
   - æ¸…æ™°ç®€æ´çš„å›åº”
   - åŸºäºæ–‡æ¡£è¯æ®
   - ä¸“ä¸šæœ¯è¯­

2. æ”¯æŒç»†èŠ‚
   - ç›¸å…³æ–‡æ¡£å¼•ç”¨
   - æ–‡ä»¶å‚è€ƒ
   - ä¸Šä¸‹æ–‡è§£é‡Š

3. ä¿¡æ¯ç¼ºå£ï¼ˆå¦‚æœ‰ï¼‰
   - ç¼ºå¤±ä¿¡æ¯
   - éœ€è¦çš„é¢å¤–ä¸Šä¸‹æ–‡
   - æ½œåœ¨é™åˆ¶

# ğŸ” åˆ†æä¸Šä¸‹æ–‡
é—®é¢˜: {question}

ç›¸å…³æ–‡æ¡£ï¼ˆæŒ‰ç›¸å…³æ€§æ’åºï¼‰ï¼š
"""

            # Add context with length control and deduplication
            with yaspin(text="æ·»åŠ ä¸Šä¸‹æ–‡...", color="cyan") as spinner:
                available_count = self.max_token_count - get_context_token_count(prompt) - 1000
                current_count = 0
                
                # ä¿å­˜å·²æ·»åŠ çš„å†…å®¹æŒ‡çº¹ï¼Œé¿å…é‡å¤
                added_content_hashes = set()
                
                # åˆ†ç»„æ–‡æ¡£ï¼ŒæŒ‰æ–‡ä»¶è·¯å¾„æ•´ç†
                file_groups = {}
                for doc, score in results:
                    file_path = doc.metadata['file_path']
                    if file_path not in file_groups:
                        file_groups[file_path] = []
                    file_groups[file_path].append((doc, score))
                
                # æŒ‰æ–‡ä»¶æ·»åŠ æ–‡æ¡£ç‰‡æ®µ
                for file_path, docs in file_groups.items():
                    # æŒ‰ç›¸å…³æ€§æ’åº
                    docs.sort(key=lambda x: x[1], reverse=True)
                    
                    # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
                    file_header = f"\n## æ–‡ä»¶: {file_path}\n"
                    if current_count + get_context_token_count(file_header) > available_count:
                        break
                    
                    prompt += file_header
                    current_count += get_context_token_count(file_header)
                    
                    # æ·»åŠ æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ
                    added_count = 0
                    for doc, score in docs:
                        # è®¡ç®—å†…å®¹æŒ‡çº¹ä»¥é¿å…é‡å¤
                        content_hash = hash(doc.content)
                        if content_hash in added_content_hashes:
                            continue
                            
                        # å¦‚æœå†…å®¹ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ï¼Œè·³è¿‡
                        if score < 0.2:
                            continue
                            
                        # æ ¼å¼åŒ–æ–‡æ¡£ç‰‡æ®µ
                        doc_content = f"""
### ç‰‡æ®µ {doc.metadata['chunk_index'] + 1}/{doc.metadata['total_chunks']} [ç›¸å…³åº¦: {score:.2f}]
```
{doc.content}
```
"""
                        if current_count + get_context_token_count(doc_content) > available_count:
                            break
                            
                        prompt += doc_content
                        current_count += get_context_token_count(doc_content)
                        added_content_hashes.add(content_hash)
                        added_count += 1
                        
                        # æ¯ä¸ªæ–‡ä»¶æœ€å¤šæ·»åŠ 3ä¸ªæœ€ç›¸å…³çš„ç‰‡æ®µ
                        if added_count >= 3:
                            break
                
                if current_count >= available_count:
                    PrettyOutput.print(
                        "ç”±äºä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼Œéƒ¨åˆ†å†…å®¹è¢«çœç•¥",
                        output_type=OutputType.WARNING
                    )

                prompt += """
# â— é‡è¦è§„åˆ™
1. ä»…ä½¿ç”¨æä¾›çš„æ–‡æ¡£
2. ä¿æŒç²¾ç¡®å’Œå‡†ç¡®
3. åœ¨ç›¸å…³æ—¶å¼•ç”¨æ¥æº
4. æŒ‡å‡ºç¼ºå¤±çš„ä¿¡æ¯
5. ä¿æŒä¸“ä¸šè¯­æ°”
6. ä½¿ç”¨ç”¨æˆ·çš„è¯­è¨€å›ç­”
"""
                spinner.text = "æ·»åŠ ä¸Šä¸‹æ–‡å®Œæˆ"
                spinner.ok("âœ…")

            with yaspin(text="æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...", color="cyan") as spinner:
                model = PlatformRegistry.get_global_platform_registry().get_normal_platform()
                response = model.chat_until_success(prompt)
                spinner.text = "ç­”æ¡ˆç”Ÿæˆå®Œæˆ"
                spinner.ok("âœ…")
                return response
            
        except Exception as e:
            PrettyOutput.print(f"å›ç­”å¤±è´¥ï¼š{str(e)}", OutputType.ERROR)
            return None
            
    def _enhance_query(self, query: str) -> str:
        """å¢å¼ºæŸ¥è¯¢ä»¥æé«˜æ£€ç´¢è´¨é‡
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            str: å¢å¼ºåçš„æŸ¥è¯¢
        """
        # ç®€å•çš„æŸ¥è¯¢é¢„å¤„ç†
        query = query.strip()
        
        # å¦‚æœæŸ¥è¯¢å¤ªçŸ­ï¼Œè¿”å›åŸå§‹æŸ¥è¯¢
        if len(query) < 10:
            return query
            
        try:
            # å°è¯•ä½¿ç”¨å¤§æ¨¡å‹å¢å¼ºæŸ¥è¯¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            model = PlatformRegistry.get_global_platform_registry().get_normal_platform()
            enhance_prompt = f"""è¯·åˆ†æä»¥ä¸‹æŸ¥è¯¢ï¼Œæå–å…³é”®æ¦‚å¿µã€å…³é”®è¯å’Œä¸»é¢˜ã€‚
            
æŸ¥è¯¢ï¼š"{query}"

è¾“å‡ºæ ¼å¼ï¼šå¯¹åŸå§‹æŸ¥è¯¢çš„æ”¹å†™ç‰ˆæœ¬ï¼Œä¸“æ³¨äºæå–å…³é”®ä¿¡æ¯ï¼Œä¿ç•™åŸå§‹è¯­ä¹‰ï¼Œä»¥æé«˜æ£€ç´¢ç›¸å…³åº¦ã€‚
ä»…è¾“å‡ºæ”¹å†™åçš„æŸ¥è¯¢æ–‡æœ¬ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚
åªå¯¹ä¿¡æ¯è¿›è¡Œæœ€å°å¿…è¦çš„å¢å¼ºï¼Œä¸è¦è¿‡åº¦æ·»åŠ ä¸åŸå§‹æŸ¥è¯¢æ— å…³çš„å†…å®¹ã€‚
"""
            
            enhanced_query = model.chat_until_success(enhance_prompt)
            # æ¸…ç†å¢å¼ºçš„æŸ¥è¯¢ç»“æœ
            enhanced_query = enhanced_query.strip().strip('"')
            
            # å¦‚æœå¢å¼ºæŸ¥è¯¢æœ‰æ•ˆä¸”ä¸æ˜¯å®Œå…¨ç›¸åŒçš„ï¼Œä½¿ç”¨å®ƒ
            if enhanced_query and len(enhanced_query) >= len(query) / 2 and enhanced_query != query:
                return enhanced_query
                
        except Exception:
            # å¦‚æœå¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
            pass
            
        return query

    def is_index_built(self) -> bool:
        """Check if the index is built and valid
        
        Returns:
            bool: True if index is built and valid
        """
        return self.index is not None and len(self.documents) > 0

    def _delete_file_cache(self, file_path: str, spinner=None):
        """Delete cache files for a specific file
        
        Args:
            file_path: Path to the original file
            spinner: Optional spinner for progress information. If None, runs silently.
        """
        try:
            # Delete document cache
            doc_cache_path = self._get_cache_path(file_path, "doc")
            if os.path.exists(doc_cache_path):
                os.remove(doc_cache_path)
                if spinner is not None:
                    spinner.write(f"ğŸ—‘ï¸ åˆ é™¤æ–‡æ¡£ç¼“å­˜: {file_path}")
                    
            # Delete vector cache
            vec_cache_path = self._get_cache_path(file_path, "vec")
            if os.path.exists(vec_cache_path):
                os.remove(vec_cache_path)
                if spinner is not None:
                    spinner.write(f"ğŸ—‘ï¸ åˆ é™¤å‘é‡ç¼“å­˜: {file_path}")
                    
        except Exception as e:
            if spinner is not None:
                spinner.write(f"âŒ åˆ é™¤ç¼“å­˜å¤±è´¥: {file_path}: {str(e)}")
            PrettyOutput.print(f"åˆ é™¤ç¼“å­˜å¤±è´¥: {file_path}: {str(e)}", output_type=OutputType.ERROR)

def main():
    """Main function"""
    import argparse
    import sys
    
    # Set standard output encoding to UTF-8
    if sys.stdout.encoding != 'utf-8':
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')
    
    parser = argparse.ArgumentParser(description='Document retrieval and analysis tool')
    parser.add_argument('--dir', type=str, help='Directory to process')
    parser.add_argument('--build', action='store_true', help='Build document index')
    parser.add_argument('--search', type=str, help='Search document content')
    parser.add_argument('--ask', type=str, help='Ask about documents')
    args = parser.parse_args()

    try:
        current_dir = os.getcwd()
        rag = RAGTool(current_dir)

        if not args.dir:
            args.dir = current_dir

        if args.dir and args.build:
            rag.build_index(args.dir)
            return 0

        if args.search or args.ask:
            # å½“éœ€è¦æœç´¢æˆ–æé—®æ—¶ï¼Œè‡ªåŠ¨æ£€æŸ¥å¹¶å»ºç«‹ç´¢å¼•
            if not rag.is_index_built():
                PrettyOutput.print(f"ç´¢å¼•æœªå»ºç«‹ï¼Œè‡ªåŠ¨ä¸ºç›®å½• '{args.dir}' å»ºç«‹ç´¢å¼•...", OutputType.INFO)
                rag.build_index(args.dir)
                
                if not rag.is_index_built():
                    PrettyOutput.print("ç´¢å¼•å»ºç«‹å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›®å½•å’Œæ–‡ä»¶æ ¼å¼", OutputType.ERROR)
                    return 1

            if args.search:
                results = rag.query(args.search)
                if not results:
                    PrettyOutput.print("æœªæ‰¾åˆ°ç›¸å…³å†…å®¹", output_type=OutputType.WARNING)
                    return 1
                    
                for doc in results:
                    output = f"""æ–‡ä»¶: {doc.metadata['file_path']}\n"""
                    output += f"""ç‰‡æ®µ {doc.metadata['chunk_index'] + 1}/{doc.metadata['total_chunks']}\n"""
                    output += f"""å†…å®¹:\n{doc.content}\n"""
                    PrettyOutput.print(output, output_type=OutputType.INFO, lang="markdown")
                return 0

            if args.ask:
                # Call ask method
                response = rag.ask(args.ask)
                if not response:
                    PrettyOutput.print("è·å–ç­”æ¡ˆå¤±è´¥", output_type=OutputType.WARNING)
                    return 1
                    
                # Display answer
                output = f"""{response}"""
                PrettyOutput.print(output, output_type=OutputType.INFO)
                return 0

        PrettyOutput.print("è¯·æŒ‡å®šæ“ä½œå‚æ•°ã€‚ä½¿ç”¨ -h æŸ¥çœ‹å¸®åŠ©ã€‚", output_type=OutputType.WARNING)
        return 1

    except Exception as e:
        PrettyOutput.print(f"æ‰§è¡Œå¤±è´¥: {str(e)}", output_type=OutputType.ERROR)
        return 1

if __name__ == "__main__":
    main()

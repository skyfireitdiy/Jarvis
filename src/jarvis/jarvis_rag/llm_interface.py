from abc import ABC, abstractmethod
import os
import os
from abc import ABC, abstractmethod

from jarvis.jarvis_agent import Agent as JarvisAgent
from jarvis.jarvis_platform.base import BasePlatform
from jarvis.jarvis_platform.registry import PlatformRegistry


class LLMInterface(ABC):
    """
    Abstract Base Class for Large Language Model interfaces.

    This class defines the standard interface for interacting with a remote LLM.
    Any LLM provider (OpenAI, Anthropic, etc.) should be implemented as a
    subclass of this interface.
    """

    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> str:
        """
        Generates a response from the LLM based on a given prompt.

        Args:
            prompt: The input prompt to send to the LLM.
            **kwargs: Additional keyword arguments for the LLM API call
                      (e.g., temperature, max_tokens).

        Returns:
            The text response generated by the LLM.
        """
        pass


class OpenAI_LLM(LLMInterface):
    """
    An implementation of the LLMInterface for OpenAI's models (GPT series).

    This class uses the 'openai' Python client to interact with the API.
    It requires the OPENAI_API_KEY environment variable to be set.
    """

    def __init__(self, model_name: str = "gpt-4o"):
        """
        Initializes the OpenAI LLM client.

        Args:
            model_name: The name of the OpenAI model to use (e.g., "gpt-4o", "gpt-3.5-turbo").
        """
        try:
            from openai import OpenAI
        except ImportError:
            raise ImportError(
                "OpenAI client not found. Please install it with 'pip install openai'."
            )

        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY environment variable not set.")

        self.client = OpenAI(api_key=self.api_key)
        self.model_name = model_name
        print(f"ğŸš€ å·²åˆå§‹åŒ– OpenAI LLMï¼Œæ¨¡å‹: {self.model_name}")

    def generate(self, prompt: str, **kwargs) -> str:
        """
        Sends a prompt to the OpenAI API and returns the response.

        Args:
            prompt: The user's prompt.
            **kwargs: Supports standard OpenAI API parameters like 'temperature',
                      'max_tokens', etc.

        Returns:
            The content of the first choice from the chat completion.
        """
        # Set default parameters if not provided
        params = {
            "temperature": 0.7,
            "max_tokens": 1500,
            **kwargs,
        }

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt},
                ],
                **params,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"âŒ è°ƒç”¨ OpenAI API æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return "é”™è¯¯: æ— æ³•ä»LLMè·å–å“åº”ã€‚"


class ToolAgent_LLM(LLMInterface):
    """
    An implementation of the LLMInterface that uses a tool-wielding JarvisAgent
    to generate the final response.
    """

    def __init__(self):
        """
        Initializes the Tool-Agent LLM wrapper.
        """
        print("ğŸ¤– å·²åˆå§‹åŒ–å·¥å…· Agent ä½œä¸ºæœ€ç»ˆåº”ç­”è€…ã€‚")
        self.allowed_tools = ["read_code", "execute_script"]
        # A generic system prompt for the agent
        self.system_prompt = "You are a helpful assistant. Please answer the user's question based on the provided context. You can use tools to find more information if needed."
        self.summary_prompt = """
<report>
è¯·ä¸ºæœ¬æ¬¡é—®ç­”ä»»åŠ¡ç”Ÿæˆä¸€ä¸ªæ€»ç»“æŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š

1. **åŸå§‹é—®é¢˜**: é‡è¿°ç”¨æˆ·æœ€å¼€å§‹æå‡ºçš„é—®é¢˜ã€‚
2. **å…³é”®ä¿¡æ¯æ¥æº**: æ€»ç»“ä½ æ˜¯åŸºäºå“ªäº›å…³é”®ä¿¡æ¯æˆ–æ–‡ä»¶å¾—å‡ºçš„ç»“è®ºã€‚
3. **æœ€ç»ˆç­”æ¡ˆ**: ç»™å‡ºæœ€ç»ˆçš„ã€ç²¾ç‚¼çš„å›ç­”ã€‚
</report>
"""

    def generate(self, prompt: str, **kwargs) -> str:
        """
        Runs the JarvisAgent with a restricted toolset to generate an answer.

        Args:
            prompt: The full prompt, including context, to be sent to the agent.
            **kwargs: Ignored, kept for interface compatibility.

        Returns:
            The final answer generated by the agent.
        """
        try:
            # Initialize the agent with specific settings for RAG context
            agent = JarvisAgent(
                system_prompt=self.system_prompt,
                use_tools=self.allowed_tools,
                auto_complete=True,
                use_methodology=False,
                use_analysis=False,
                need_summary=True,
                summary_prompt=self.summary_prompt,
            )

            # The agent's run method expects the 'user_input' parameter
            final_answer = agent.run(user_input=prompt)
            return str(final_answer)

        except Exception as e:
            print(f"âŒ Agent åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return "é”™è¯¯: Agent æœªèƒ½æˆåŠŸç”Ÿæˆå›ç­”ã€‚"


class JarvisPlatform_LLM(LLMInterface):
    """
    An implementation of the LLMInterface for the project's internal platform.

    This class uses the PlatformRegistry to get the configured "normal" model.
    """

    def __init__(self):
        """
        Initializes the Jarvis Platform LLM client.
        """
        try:
            self.registry = PlatformRegistry.get_global_platform_registry()
            self.platform: BasePlatform = self.registry.get_normal_platform()
            self.platform.set_suppress_output(
                False
            )  # Ensure no console output from the model
            print(f"ğŸš€ å·²åˆå§‹åŒ– Jarvis å¹³å° LLMï¼Œæ¨¡å‹: {self.platform.name()}")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ– Jarvis å¹³å° LLM å¤±è´¥: {e}")
            raise

    def generate(self, prompt: str, **kwargs) -> str:
        """
        Sends a prompt to the local platform model and returns the response.

        Args:
            prompt: The user's prompt.
            **kwargs: Ignored, kept for interface compatibility.

        Returns:
            The response generated by the platform model.
        """
        try:
            # Use the robust chat_until_success method
            return self.platform.chat_until_success(prompt)
        except Exception as e:
            print(f"âŒ è°ƒç”¨ Jarvis å¹³å°æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return "é”™è¯¯: æ— æ³•ä»æœ¬åœ°LLMè·å–å“åº”ã€‚"

import os
from typing import List, Literal, Optional, cast

from langchain.docstore.document import Document

from .embedding_manager import EmbeddingManager
from .llm_interface import JarvisPlatform_LLM, LLMInterface, ToolAgent_LLM
from .query_rewriter import QueryRewriter
from .reranker import Reranker
from .retriever import ChromaRetriever
from jarvis.jarvis_utils.config import (
    get_rag_embedding_mode,
    get_rag_vector_db_path,
    get_rag_embedding_cache_path,
    get_rag_embedding_models,
)


class JarvisRAGPipeline:
    """
    The main orchestrator for the RAG pipeline.

    This class integrates the embedding manager, retriever, and LLM to provide
    a complete pipeline for adding documents and querying them.
    """

    def __init__(
        self,
        llm: Optional[LLMInterface] = None,
        embedding_mode: Optional[Literal["performance", "accuracy"]] = None,
        db_path: Optional[str] = None,
        collection_name: str = "jarvis_rag_collection",
    ):
        """
        Initializes the RAG pipeline.

        Args:
            llm: An instance of a class implementing LLMInterface.
                 If None, defaults to OpenAI_LLM().
            embedding_mode: The mode for the local embedding model. If None, uses config value.
            db_path: Path to the persistent vector database. If None, uses config value.
            collection_name: Name of the collection in the vector database.
        """
        # Determine the embedding model to isolate data paths
        _embedding_mode = embedding_mode or get_rag_embedding_mode()
        embedding_models = get_rag_embedding_models()
        model_name = embedding_models[_embedding_mode]["model_name"]
        sanitized_model_name = model_name.replace("/", "_").replace("\\", "_")

        # If a specific db_path is given, use it. Otherwise, create a model-specific path.
        _final_db_path = (
            str(db_path)
            if db_path
            else os.path.join(get_rag_vector_db_path(), sanitized_model_name)
        )
        # Always create a model-specific cache path.
        _final_cache_path = os.path.join(
            get_rag_embedding_cache_path(), sanitized_model_name
        )

        self.embedding_manager = EmbeddingManager(
            mode=cast(Literal["performance", "accuracy"], _embedding_mode),
            cache_dir=_final_cache_path,
        )
        self.retriever = ChromaRetriever(
            embedding_manager=self.embedding_manager,
            db_path=_final_db_path,
            collection_name=collection_name,
        )
        # Default to the ToolAgent_LLM unless a specific LLM is provided
        self.llm = llm if llm is not None else ToolAgent_LLM()
        self.reranker = Reranker()
        # Use a standard LLM for the query rewriting task, not the agent
        self.query_rewriter = QueryRewriter(JarvisPlatform_LLM())

        print("âœ… JarvisRAGPipeline åˆå§‹åŒ–æˆåŠŸã€‚")

    def add_documents(self, documents: List[Document]):
        """
        Adds documents to the vector knowledge base.

        Args:
            documents: A list of LangChain Document objects to add.
        """
        self.retriever.add_documents(documents)

    def _create_prompt(
        self, query: str, context_docs: List[Document], source_files: List[str]
    ) -> str:
        """Creates the final prompt for the LLM or Agent."""
        context = "\n\n".join([doc.page_content for doc in context_docs])
        sources_text = "\n".join([f"- {source}" for source in source_files])

        prompt_template = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“å®¶åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»“åˆä¸‹é¢æä¾›çš„å‚è€ƒä¿¡æ¯æ¥å›ç­”ã€‚

        **é‡è¦**: æä¾›çš„ä¸Šä¸‹æ–‡å’Œæ–‡ä»¶åˆ—è¡¨**ä»…ä¾›å‚è€ƒ**ï¼Œå¯èƒ½ä¸å®Œæ•´æˆ–å·²è¿‡æ—¶ã€‚åœ¨å›ç­”å‰ï¼Œä½ åº”è¯¥**ä¼˜å…ˆä½¿ç”¨å·¥å…·ï¼ˆå¦‚ read_codeï¼‰æ¥è·å–æœ€æ–°ã€æœ€å‡†ç¡®çš„ä¿¡æ¯**ã€‚

        å‚è€ƒæ–‡ä»¶åˆ—è¡¨:
        ---
        {sources_text}
        ---

        å‚è€ƒä¸Šä¸‹æ–‡:
        ---
        {context}
        ---

        é—®é¢˜: {query}

        å›ç­”:
        """
        return prompt_template.strip()

    def query(self, query_text: str, n_results: int = 5) -> str:
        """
        Performs a query against the knowledge base using a multi-query
        retrieval and reranking pipeline.

        Args:
            query_text: The user's original question.
            n_results: The number of final relevant chunks to retrieve.

        Returns:
            The answer generated by the LLM.
        """
        # 1. Rewrite the original query into multiple queries
        rewritten_queries = self.query_rewriter.rewrite(query_text)

        # 2. Retrieve initial candidates for each rewritten query
        all_candidate_docs = []
        for q in rewritten_queries:
            print(f"ğŸ” æ­£åœ¨ä¸ºæŸ¥è¯¢å˜ä½“ '{q}' è¿›è¡Œæ··åˆæ£€ç´¢...")
            candidates = self.retriever.retrieve(q, n_results=n_results * 2)
            all_candidate_docs.extend(candidates)

        # De-duplicate the candidate documents
        unique_docs_dict = {doc.page_content: doc for doc in all_candidate_docs}
        unique_candidate_docs = list(unique_docs_dict.values())

        if not unique_candidate_docs:
            return "æˆ‘åœ¨æä¾›çš„æ–‡æ¡£ä¸­æ‰¾ä¸åˆ°ä»»ä½•ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"

        # 3. Rerank the unified candidate pool against the *original* query
        print(
            f"ğŸ” æ­£åœ¨å¯¹ {len(unique_candidate_docs)} ä¸ªå€™é€‰æ–‡æ¡£è¿›è¡Œé‡æ’ï¼ˆåŸºäºåŸå§‹é—®é¢˜ï¼‰..."
        )
        retrieved_docs = self.reranker.rerank(
            query_text, unique_candidate_docs, top_n=n_results
        )

        if not retrieved_docs:
            return "æˆ‘åœ¨æä¾›çš„æ–‡æ¡£ä¸­æ‰¾ä¸åˆ°ä»»ä½•ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"

        # Print the sources of the final retrieved documents
        sources = sorted(
            list(
                {
                    doc.metadata["source"]
                    for doc in retrieved_docs
                    if "source" in doc.metadata
                }
            )
        )
        if sources:
            print(f"ğŸ“š æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å›ç­”:")
            for source in sources:
                print(f"  - {source}")

        # 4. Create the final prompt and generate the answer
        # We use the original query_text for the final prompt to the LLM
        prompt = self._create_prompt(query_text, retrieved_docs, sources)

        print("ğŸ¤– æ­£åœ¨ä»LLMç”Ÿæˆç­”æ¡ˆ...")
        answer = self.llm.generate(prompt)

        return answer

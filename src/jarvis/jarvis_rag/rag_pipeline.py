import os
from typing import List, Literal, Optional, cast

from langchain.docstore.document import Document

from .embedding_manager import EmbeddingManager
from .llm_interface import LLMInterface, JarvisPlatform_LLM
from .retriever import ChromaRetriever
from jarvis.jarvis_utils.config import (
    get_rag_embedding_mode,
    get_rag_vector_db_path,
    get_rag_embedding_cache_path,
    get_rag_embedding_models,
)


class JarvisRAGPipeline:
    """
    The main orchestrator for the RAG pipeline.

    This class integrates the embedding manager, retriever, and LLM to provide
    a complete pipeline for adding documents and querying them.
    """

    def __init__(
        self,
        llm: Optional[LLMInterface] = None,
        embedding_mode: Optional[Literal["performance", "accuracy"]] = None,
        db_path: Optional[str] = None,
        collection_name: str = "jarvis_rag_collection",
    ):
        """
        Initializes the RAG pipeline.

        Args:
            llm: An instance of a class implementing LLMInterface.
                 If None, defaults to OpenAI_LLM().
            embedding_mode: The mode for the local embedding model. If None, uses config value.
            db_path: Path to the persistent vector database. If None, uses config value.
            collection_name: Name of the collection in the vector database.
        """
        # Determine the embedding model to isolate data paths
        _embedding_mode = embedding_mode or get_rag_embedding_mode()
        embedding_models = get_rag_embedding_models()
        model_name = embedding_models[_embedding_mode]["model_name"]
        sanitized_model_name = model_name.replace("/", "_").replace("\\", "_")

        # If a specific db_path is given, use it. Otherwise, create a model-specific path.
        _final_db_path = (
            str(db_path)
            if db_path
            else os.path.join(get_rag_vector_db_path(), sanitized_model_name)
        )
        # Always create a model-specific cache path.
        _final_cache_path = os.path.join(
            get_rag_embedding_cache_path(), sanitized_model_name
        )

        self.embedding_manager = EmbeddingManager(
            mode=cast(Literal["performance", "accuracy"], _embedding_mode),
            cache_dir=_final_cache_path,
        )
        self.retriever = ChromaRetriever(
            embedding_manager=self.embedding_manager,
            db_path=_final_db_path,
            collection_name=collection_name,
        )
        # Default to the local Jarvis Platform LLM
        self.llm = llm if llm is not None else JarvisPlatform_LLM()

        print("âœ… JarvisRAGPipeline åˆå§‹åŒ–æˆåŠŸã€‚")

    def add_documents(self, documents: List[Document]):
        """
        Adds documents to the vector knowledge base.

        Args:
            documents: A list of LangChain Document objects to add.
        """
        self.retriever.add_documents(documents)

    def _create_prompt(self, query: str, context_docs: List[Document]) -> str:
        """Creates the final prompt for the LLM."""
        context = "\n\n".join([doc.page_content for doc in context_docs])

        prompt_template = f"""
        ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚è¯·ä»…æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚
        å¦‚æœä¸Šä¸‹æ–‡ä¸­ä¸åŒ…å«ç­”æ¡ˆï¼Œè¯·è¯´æ˜ä½ æ— æ³•æ ¹æ®ç»™å®šä¿¡æ¯å›ç­”ã€‚ä¸è¦ä½¿ç”¨ä»»ä½•å…ˆéªŒçŸ¥è¯†ã€‚

        ä¸Šä¸‹æ–‡:
        ---
        {context}
        ---

        é—®é¢˜: {query}

        å›ç­”:
        """
        return prompt_template.strip()

    def query(self, query_text: str, n_results: int = 5) -> str:
        """
        Performs a query against the knowledge base.

        Args:
            query_text: The user's question.
            n_results: The number of relevant chunks to retrieve.

        Returns:
            The answer generated by the LLM.
        """
        print(f"ğŸ” æ­£åœ¨ä¸ºæŸ¥è¯¢æ£€ç´¢ä¸Šä¸‹æ–‡: '{query_text}'")
        retrieved_docs = self.retriever.retrieve(query_text, n_results=n_results)

        if not retrieved_docs:
            return "æˆ‘åœ¨æä¾›çš„æ–‡æ¡£ä¸­æ‰¾ä¸åˆ°ä»»ä½•ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"

        # Print the sources of the retrieved documents
        sources = sorted(
            list(
                {
                    doc.metadata["source"]
                    for doc in retrieved_docs
                    if "source" in doc.metadata
                }
            )
        )
        if sources:
            print(f"ğŸ“š æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å›ç­”:")
            for source in sources:
                print(f"  - {source}")

        prompt = self._create_prompt(query_text, retrieved_docs)

        print("ğŸ¤– æ­£åœ¨ä»LLMç”Ÿæˆç­”æ¡ˆ...")
        answer = self.llm.generate(prompt)

        return answer

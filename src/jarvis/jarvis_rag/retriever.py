import os
import pickle
from typing import Any, Dict, List, cast

import chromadb
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from rank_bm25 import BM25Okapi  # type: ignore

from .embedding_manager import EmbeddingManager


class ChromaRetriever:
    """
    ä¸€ä¸ªæ£€ç´¢å™¨ç±»ï¼Œå®ƒç»“åˆäº†å¯†é›†å‘é‡æœç´¢ï¼ˆChromaDBï¼‰å’Œç¨€ç–å…³é”®å­—æœç´¢ï¼ˆBM25ï¼‰
    ä»¥å®ç°æ··åˆæ£€ç´¢ã€‚
    """

    def __init__(
        self,
        embedding_manager: EmbeddingManager,
        db_path: str,
        collection_name: str = "jarvis_rag_collection",
    ):
        """
        åˆå§‹åŒ–ChromaRetrieverã€‚

        å‚æ•°:
            embedding_manager: EmbeddingManagerçš„å®ä¾‹ã€‚
            db_path: ChromaDBæŒä¹…åŒ–å­˜å‚¨çš„æ–‡ä»¶è·¯å¾„ã€‚
            collection_name: ChromaDBä¸­é›†åˆçš„åç§°ã€‚
        """
        self.embedding_manager = embedding_manager
        self.db_path = db_path
        self.collection_name = collection_name

        # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯
        self.client = chromadb.PersistentClient(path=self.db_path)
        self.collection = self.client.get_or_create_collection(
            name=self.collection_name
        )
        print(f"âœ… ChromaDB å®¢æˆ·ç«¯å·²åœ¨ '{db_path}' åˆå§‹åŒ–ï¼Œé›†åˆä¸º '{collection_name}'ã€‚")

        # BM25ç´¢å¼•è®¾ç½®
        self.bm25_index_path = os.path.join(self.db_path, f"{collection_name}_bm25.pkl")
        self._load_or_initialize_bm25()

    def _load_or_initialize_bm25(self):
        """ä»ç£ç›˜åŠ è½½BM25ç´¢å¼•æˆ–åˆå§‹åŒ–ä¸€ä¸ªæ–°ç´¢å¼•ã€‚"""
        if os.path.exists(self.bm25_index_path):
            print("ğŸ” æ­£åœ¨åŠ è½½ç°æœ‰çš„ BM25 ç´¢å¼•...")
            with open(self.bm25_index_path, "rb") as f:
                data = pickle.load(f)
                self.bm25_corpus = data["corpus"]
                self.bm25_index = BM25Okapi(self.bm25_corpus)
            print("âœ… BM25 ç´¢å¼•åŠ è½½æˆåŠŸã€‚")
        else:
            print("âš ï¸ æœªæ‰¾åˆ° BM25 ç´¢å¼•ï¼Œå°†åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„ã€‚")
            self.bm25_corpus = []
            self.bm25_index = None

    def _save_bm25_index(self):
        """å°†BM25ç´¢å¼•ä¿å­˜åˆ°ç£ç›˜ã€‚"""
        if self.bm25_index:
            print("ğŸ’¾ æ­£åœ¨ä¿å­˜ BM25 ç´¢å¼•...")
            with open(self.bm25_index_path, "wb") as f:
                pickle.dump({"corpus": self.bm25_corpus, "index": self.bm25_index}, f)
            print("âœ… BM25 ç´¢å¼•ä¿å­˜æˆåŠŸã€‚")

    def add_documents(
        self, documents: List[Document], chunk_size=1000, chunk_overlap=100
    ):
        """
        å°†æ–‡æ¡£æ‹†åˆ†ã€åµŒå…¥ï¼Œå¹¶æ·»åŠ åˆ°ChromaDBå’ŒBM25ç´¢å¼•ä¸­ã€‚
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap
        )
        chunks = text_splitter.split_documents(documents)

        print(f"ğŸ“„ å·²å°† {len(documents)} ä¸ªæ–‡æ¡£æ‹†åˆ†ä¸º {len(chunks)} ä¸ªå—ã€‚")

        if not chunks:
            return

        # æå–å†…å®¹ã€å…ƒæ•°æ®å¹¶ç”ŸæˆID
        chunk_texts = [chunk.page_content for chunk in chunks]
        metadatas = [chunk.metadata for chunk in chunks]
        start_id = self.collection.count()
        ids = [f"doc_{i}" for i in range(start_id, start_id + len(chunks))]

        # æ·»åŠ åˆ°ChromaDB
        embeddings = self.embedding_manager.embed_documents(chunk_texts)
        self.collection.add(
            ids=ids,
            embeddings=cast(Any, embeddings),
            documents=chunk_texts,
            metadatas=cast(Any, metadatas),
        )
        print(f"âœ… æˆåŠŸå°† {len(chunks)} ä¸ªå—æ·»åŠ åˆ° ChromaDB é›†åˆä¸­ã€‚")

        # æ›´æ–°å¹¶ä¿å­˜BM25ç´¢å¼•
        tokenized_chunks = [doc.split() for doc in chunk_texts]
        self.bm25_corpus.extend(tokenized_chunks)
        self.bm25_index = BM25Okapi(self.bm25_corpus)
        self._save_bm25_index()

    def retrieve(
        self, query: str, n_results: int = 5, use_bm25: bool = True
    ) -> List[Document]:
        """
        ä½¿ç”¨å‘é‡æœç´¢å’ŒBM25æ‰§è¡Œæ··åˆæ£€ç´¢ï¼Œç„¶åä½¿ç”¨å€’æ•°æ’åºèåˆï¼ˆRRFï¼‰
        å¯¹ç»“æœè¿›è¡Œèåˆã€‚
        """
        # 1. å‘é‡æœç´¢ (ChromaDB)
        query_embedding = self.embedding_manager.embed_query(query)
        vector_results = self.collection.query(
            query_embeddings=cast(Any, [query_embedding]),
            n_results=n_results * 2,  # æ£€ç´¢æ›´å¤šç»“æœç”¨äºèåˆ
        )

        # 2. å…³é”®å­—æœç´¢ (BM25)
        bm25_docs = []
        if self.bm25_index and use_bm25:
            tokenized_query = query.split()
            doc_scores = self.bm25_index.get_scores(tokenized_query)

            # ä»Chromaè·å–æ‰€æœ‰æ–‡æ¡£ä»¥åŒ¹é…BM25åˆ†æ•°
            all_docs_in_collection = self.collection.get()
            all_documents = all_docs_in_collection.get("documents")
            all_metadatas = all_docs_in_collection.get("metadatas")

            bm25_results_with_docs = []
            if all_documents and all_metadatas:
                # åˆ›å»ºä»ç´¢å¼•åˆ°æ–‡æ¡£çš„æ˜ å°„
                bm25_results_with_docs = [
                    (
                        all_documents[i],
                        all_metadatas[i],
                        score,
                    )
                    for i, score in enumerate(doc_scores)
                    if score > 0
                ]

            # æŒ‰åˆ†æ•°æ’åºå¹¶å–æœ€é«˜ç»“æœ
            bm25_results_with_docs.sort(key=lambda x: x[2], reverse=True)  # type: ignore

            for doc_text, metadata, _ in bm25_results_with_docs[: n_results * 2]:
                bm25_docs.append(Document(page_content=doc_text, metadata=metadata))

        # 3. å€’æ•°æ’åºèåˆ (RRF)
        fused_scores: Dict[str, float] = {}
        k = 60  # RRFæ’åå¸¸æ•°

        # å¤„ç†å‘é‡ç»“æœ
        if vector_results and vector_results["ids"] and vector_results["documents"]:
            vec_ids = vector_results["ids"][0]
            vec_texts = vector_results["documents"][0]

            for rank, doc_id in enumerate(vec_ids):
                fused_scores[doc_id] = fused_scores.get(doc_id, 0) + 1 / (k + rank)

            # ä¸ºBM25èåˆåˆ›å»ºä»æ–‡æ¡£æ–‡æœ¬åˆ°å…¶IDçš„æ˜ å°„
            doc_text_to_id = {text: doc_id for text, doc_id in zip(vec_texts, vec_ids)}

            for rank, doc in enumerate(bm25_docs):
                bm25_doc_id = doc_text_to_id.get(doc.page_content)
                if bm25_doc_id:
                    fused_scores[bm25_doc_id] = fused_scores.get(bm25_doc_id, 0) + 1 / (
                        k + rank
                    )

        # å¯¹èåˆç»“æœè¿›è¡Œæ’åº
        sorted_fused_results = sorted(
            fused_scores.items(), key=lambda x: x[1], reverse=True
        )

        # æ ¹æ®èåˆæ’åä»ChromaDBè·å–æœ€ç»ˆæ–‡æ¡£
        final_doc_ids = [item[0] for item in sorted_fused_results[:n_results]]

        if not final_doc_ids:
            return []

        final_docs_data = self.collection.get(ids=final_doc_ids)

        retrieved_docs = []
        if final_docs_data:
            final_documents = final_docs_data.get("documents")
            final_metadatas = final_docs_data.get("metadatas")

            if final_documents and final_metadatas:
                for doc_text, metadata in zip(final_documents, final_metadatas):
                    if doc_text is not None and metadata is not None:
                        retrieved_docs.append(
                            Document(
                                page_content=cast(str, doc_text), metadata=metadata
                            )
                        )

        return retrieved_docs

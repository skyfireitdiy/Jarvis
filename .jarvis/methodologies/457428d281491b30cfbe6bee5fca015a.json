{
  "problem_type": "为数字孪生组件集成LLM",
  "content": "# 为数字孪生组件集成LLM\n\n## 规则简介\n\n本方法论指导为Jarvis数字孪生系统的智能增强组件添加LLM推理能力。通过标准化的集成模式，确保组件能够使用真正的LLM进行智能推理，同时保留规则/模式匹配作为降级方案。\n\n## 你必须遵守的原则\n\n### 保持兼容性\n\n**要求说明：**\n\n- **必须**：保留原有的规则/模式匹配逻辑，不破坏现有功能\n- **必须**：LLM推理失败时必须优雅降级到规则模式\n- **禁止**：删除或修改现有的RuleBasedInferrer和PatternBasedInferrer类\n- **禁止**：让LLM调用阻塞主流程\n\n### 确保隔离性\n\n**要求说明：**\n\n- **必须**：每个组件使用独立的LLM实例（通过registry.get_cheap_platform()创建）\n- **必须**：使用complete()方法进行无状态调用，避免上下文污染\n- **禁止**：让多个组件共享同一个LLM实例\n\n### 提供可观测性\n\n**要求说明：**\n\n- **必须**：添加过程打印，让用户感知到LLM在工作\n- **必须**：显示当前使用的推理模式（LLM/规则/降级）\n- **建议**：打印推理结果数量，便于验证\n\n## 你必须执行的操作\n\n### 阶段1：分析现有代码结构\n\n1. **读取目标组件代码**：使用read_code工具查看组件的完整实现\n2. **识别继承结构**：\n   - 如果组件已继承HybridEngine基类，使用模式A\n   - 如果组件是独立类，使用模式B\n3. **定位修改点**：\n   - `__init__`方法：添加llm_client参数\n   - 主推理方法：添加LLM优先逻辑\n   - 辅助方法：实现_llm_infer方法\n\n### 阶段2：实现LLM集成\n\n**模式A：继承HybridEngine的组件**\n- 基类已提供_llm_infer方法和fallback机制\n- 重点实现_llm_infer方法中的prompt设计\n- 参考组件：EmotionRecognizer、AmbiguityResolver\n\n**模式B：独立组件（如NeedInferrer）**\n\n1. **修改__init__方法**：\n   ```python\n   def __init__(\n       self,\n       ...现有参数,\n       llm_client: Optional[Any] = None,\n   ) -> None:\n       ...现有初始化代码\n       self._llm_client = llm_client\n```\n\n2. **实现_llm_infer方法**：\n   ```python\n   def _llm_infer_xxx(self, input_data: str) -> List[tuple[str, float, str, List[str]]]:\n       \"\"\"使用LLM推理\"\"\"\n       if not self._llm_client:\n               return []\n      \n       try:\n           prompt = f\"\"\"你是...专家。请分析...\n          \n           输入：{input_data}\n          \n           请返回JSON格式...\n           \"\"\"\n          \n           response = self._llm_client.complete(prompt)\n          \n           # 解析JSON（使用正则提取）\n           import json, re\n           json_match = re.search(r'\\[.*\\]', response, re.DOTALL)\n           if not json_match:\n               return []\n          \n           data = json.loads(json_match.group(0))\n           results = []\n           for item in data:\n               # 构建结果元组\n               results.append((content, confidence, reasoning, chain))\n       return results\n       except Exception:\n           return []  # 触发降级\n```\n\n3. **修改主推理方法**：\n   ```python\n   def infer_xxx(self, context, input_data) -> List[InferenceResult]:\n       all_inferences = []\n       inference_mode = \"规则\"\n      \n       # LLM推理（优先）\n       if self._llm_client is not None:\n           llm_results = self._llm_infer_xxx(input_data)\n           if llm_results:\n               all_inferences.extend(llm_results)\n               inference_mode = \"LLM\"\n           else:\n               inference_mode = \"规则(降级)\"\n      \n       # 规则推理（降级或补充）\n       if not all_inferences or self._strategy == HYBRID:\n           rule_results = self._rule_inferrer.infer_xxx(input_data)\n           all_inferences.extend(rule_results)\n      \n       # 构建结果...\n      \n       # 过程打印\n       print(f\"📚 需求推理: {len(results)}个结果 (模式: {inference_mode})\")\n      \n       return results\n```\n\n### 阶段3：设计LLM Prompt\n\n1. **明确角色定位**：\"你是一个[领域]专家\"\n2. **提供清晰输入**：明确展示需要分析的内容\n3. **要求JSON返回**：提供字段说明和示例\n4. **限制输出格式**：\"请仅返回JSON数组，不要包含其他解释文字\"\n\n**Prompt模板示例**：\n```\n你是一个需求分析专家。请分析用户的显式需求，推理出可能的隐式需求。\n\n显式需求：{explicit_need}\n\n请返回JSON格式的推理结果，每个结果包含：\n- implicit_need: 隐式需求描述\n- confidence: 置信度（0-1之间的浮点数）\n- reasoning: 推理依据\n- inference_chain: 推理步骤列表\n\n返回格式示例：\n[\n  {\n    \"implicit_need\": \"编写单元测试\",\n    \"confidence\": 0.9,\n    \"reasoning\": \"功能实现通常需要配套测试\",\n    \"inference_chain\": [\"识别到功能开发需求\", \"根据最佳实践推断需要测试\", \"得出结论\"]\n  }\n]\n\n请仅返回JSON数组，不要包含其他解释文字。\n```\n\n### 阶段4：验证功能\n\n1. **语法检查**：确保代码通过ruff检查\n2. **异常处理**：确保LLM失败时能正确降级\n3. **过程打印**：验证能正确显示推理模式\n4. **运行测试**：确保所有相关测试通过\n\n## 实践指导\n\n### LLM调用模式\n\n- **无状态调用**：始终使用`complete()`方法，不要使用`chat()`\n- **JSON解析**：使用正则提取`re.search(r'\\[.*\\]', response, re.DOTALL)`\n- **异常处理**：使用`except Exception:`捕获所有异常，返回空列表\n\n### 过程打印格式规范\n\n不同功能使用不同的emoji前缀：\n- 情绪识别：🎭 情绪识别: {emotion_type} (置信度: {confidence})\n- 歧义检测：🔍 歧义检测: {ambiguity_type}\n- 主动服务：💡 主动服务: 触发 {count} 个服务\n- 需求推理：📚 需求推理: {count}个结果 (模式: {mode})\n- 持续学习：📚 持续学习: 知识+{k}, 技能+{s}, 经验+{e}\n\n### 常见错误\n\n1. **F841错误**：不要使用未捕获的异常变量`e`，使用`except Exception:`代替`except Exception as e:`\n2. **上下文污染**：不要使用chat()方法，会累积上下文影响其他调用\n3. **阻塞主流程**：LLM调用应该有超时机制，避免长时间阻塞\n4. **缺少降级**：LLM失败时必须有规则模式作为后备方案\n\n### 模式选择建议\n\n- **复杂组件**：建议使用HybridEngine基类，代码更简洁\n- **简单组件**：可以直接集成LLM，保持代码灵活性\n- **已有组件**：遵循现有架构，不要强行改变继承关系\n\n## 检查清单\n\n完成LLM集成后，验证以下内容：\n\n- [ ] __init__方法添加了llm_client参数（可选，默认None）\n- [ ] 实现了_llm_infer_xxx方法进行LLM推理\n- [ ] 主推理方法优先使用LLM，失败时降级到规则模式\n- [ ] 添加了过程打印，显示推理模式和结果数量\n- [ ] 使用complete()方法进行无状态调用\n- [ ] JSON解析使用正则提取，处理多余文本\n- [ ] 异常处理完善，返回空列表触发降级\n- [ ] 代码通过ruff静态检查（无F841等错误）\n- [ ] 保留原有规则/模式匹配逻辑未破坏\n- [ ] 相关测试全部通过",
  "scope": "project"
}
